<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>ç®—æ³•ä¸“å®¶è®ºæ–‡è§£ææŠ¥å‘Š</title>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <style>
    body { font-family: Arial, sans-serif; line-height: 1.6; margin: 20px; }
    .original { background-color: #f0f0f0; border: 1px solid #cccccc; padding: 10px; margin-bottom: 10px; }
    .translation { background-color: #e0ffe0; border: 1px solid #00cc00; padding: 10px; margin-bottom: 20px; }
    .figure { background-color: #ffffcc; padding: 10px; margin: 15px 0; text-align: center; }
    .term { color: red; font-weight: bold; }
    .formula { text-align: center; margin: 15px 0; }
    .formula-number { font-style: italic; margin-top: 5px; }
    h1, h2, h3 { color: #333366; }
    ul { list-style-type: none; padding-left: 0; }
    li { margin-bottom: 10px; }
  </style>
</head>
<body>

<h1>å†…å®¹ç†è§£</h1>
<p>æœ¬æ–‡æœ¬æ˜¯ä¸€ä¸ªæ··åˆå†…å®¹ï¼ŒåŒ…å«ä¸¤éƒ¨åˆ†æ ¸å¿ƒï¼šç¬¬ä¸€éƒ¨åˆ†æ˜¯åŸºç¡€æ•°å­¦è§£é‡Šï¼ˆæ¶‰åŠåŠ æ³•è¿ç®—å’Œæ•°å­—è¡¨ç¤ºï¼‰ï¼Œç¬¬äºŒéƒ¨åˆ†æ˜¯äººå·¥æ™ºèƒ½å¯¹é½æ–¹æ³•çš„ç ”ç©¶è®ºæ–‡æ‘˜è¦ã€‚æ•°å­¦éƒ¨åˆ†ä»¥é—®ç­”å½¢å¼è§£é‡Šç®€å•åŠ æ³•ï¼ˆå¦‚â€œ10åŠ 2ç­‰äº12â€ï¼‰ï¼Œå¹¶è®¨è®ºæ•°å­—çš„è¡¨ç¤ºå’ŒåŠ æ³•åŸç†ï¼Œå¼ºè°ƒæ˜¾å¼è®¡æ•°çš„é‡è¦æ€§ã€‚ç¬¬äºŒéƒ¨åˆ†èšç„¦äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹é½æŠ€æœ¯ï¼Œæ¯”è¾ƒäº†åŸºäºå¼ºåŒ–å­¦ä¹ çš„å¯¹é½æ–¹æ³•ï¼ˆå¦‚RLHFï¼‰å’Œå¯¹æ¯”æ–¹æ³•ï¼ˆå¦‚DPOï¼‰ï¼Œå¹¶å¼•å…¥æœ€ä½³-of-nï¼ˆBoNï¼‰é‡‡æ ·ä½œä¸ºé«˜æ•ˆæ›¿ä»£æ–¹æ¡ˆã€‚æ ¸å¿ƒç ”ç©¶é—®é¢˜æ˜¯ï¼šå¦‚ä½•å°†BoNé‡‡æ ·åˆ†å¸ƒä¸è®­ç»ƒå¯¹é½æ–¹æ³•å…³è”ï¼Œå¹¶å¼€å‘BoNBoNå¯¹é½æ–¹æ³•æ¥æ¨¡æ‹ŸBoNåˆ†å¸ƒä»¥å‡å°‘æ¨ç†æˆæœ¬ã€‚è®ºæ–‡é€šè¿‡ç†è®ºæ¨å¯¼å’Œå®éªŒè¯æ˜ï¼ŒBoNBoNæ–¹æ³•åœ¨ä¿æŒé«˜èƒœç‡çš„åŒæ—¶æœ€å°åŒ–è„±é¶å±æ€§ï¼ˆå¦‚å“åº”é•¿åº¦ï¼‰çš„åç§»ï¼Œä¼˜äºåŸºçº¿æ–¹æ³•ã€‚æ–‡æœ¬è¿˜åŒ…æ‹¬å›¾è¡¨æ•°æ®ï¼ˆå¹³å‡å“åº”é•¿åº¦ä¸èƒœç‡å…³ç³»ï¼‰å’Œè´¡çŒ®æ€»ç»“ã€‚</p>

<h1>å†…å®¹ç¿»è¯‘</h1>

<div class="original">
  <p>5060708090</p>
  <p>60 80 100 120 140 160</p>
  <p>Average Length of Responses Win Rate (%) Method</p>
  <p>BoN sampling</p>
  <p>BoNBoN</p>
  <p>DPO original HH</p>
  <p>IPO original HH</p>
  <p>Reference Model</p>
</div>
<div class="translation">
  <p>5060708090</p>
  <p>60 80 100 120 140 160</p>
  <p>å¹³å‡å“åº”é•¿åº¦ èƒœç‡ï¼ˆ%ï¼‰ æ–¹æ³•</p>
  <p>BoNé‡‡æ ·</p>
  <p>BoNBoN</p>
  <p>DPOåŸå§‹HH</p>
  <p>IPOåŸå§‹HH</p>
  <p>å‚è€ƒæ¨¡å‹</p>
</div>

<div class="original">
  <p>Average Length (Response) vs. Win Rate</p>
  <p>XPDQHZKDWLVSOXV</p>
  <p>$VVLVWDQWÂ¤Æ˜Å±Å£Æ•Æ¬</p>
  <p>Å±tÅ±t</p>
  <p>SÂ¤Â€È¬NN</p>
  <p>%Â¤Â€È¬NNLV</p>
</div>
<div class="translation">
  <p>å¹³å‡å“åº”é•¿åº¦ vs. èƒœç‡</p>
  <p>ç”¨æˆ·ï¼š10åŠ 2ç­‰äºå¤šå°‘ï¼Ÿ</p>
  <p>åŠ©æ‰‹ï¼š10åŠ 2æ˜¯12ã€‚</p>
  <p>ç”¨æˆ·ï¼š10åŠ 2æ˜¯å¤šå°‘ï¼Ÿ</p>
  <p>åŠ©æ‰‹ï¼š10åŠ 2æ˜¯12ã€‚</p>
</div>

<div class="original">
  <p>SOXVLVZKLFKLVWKHQXPEHUSOXV7KLVDQVZHULVVLPSOHRUGLQDODGGLWLRQ2UGLQDODGGLWLRQLVXVHGLQEDVLFPDWKRSHUDWLRQVOLNHDGGLWLRQDQGVXEWUDFWLRQEXWLWVDOVRXVHGZLWKRWKHUPDWKHPDWLFDOSULQFLSOHVOLNHWKHSURSHUW\RIDOZD\VDGGLQJE\SRZHUVRIWHQ7RILQGWKHVXPRIDQG\RXZRXOGGRSOXVRU7KLVFDQDOVREHZULWWHQ ,I\RXZDQWWRDGGXSDODUJHUQXPEHUSOXVPRUHWLPHV\RXZRXOGGRSOXVSOXVSOXVSOXVSOXVSOXVSOXVSOXV7KLVZRXOGUHVXOWLQH[DFWO\WKHVXPSOXV</p>
</div>
<div class="translation">
  <p>10åŠ 2ç­‰äº12ï¼Œè¿™æ˜¯æ•°å­—10åŠ 2çš„ç»“æœã€‚è¿™ä¸ªç­”æ¡ˆåŸºäºç®€å•çš„åºæ•°åŠ æ³•ã€‚åºæ•°åŠ æ³•ç”¨äºåŸºæœ¬æ•°å­¦è¿ç®—å¦‚åŠ æ³•å’Œå‡æ³•ï¼Œä½†ä¹Ÿä¸å…¶ä»–æ•°å­¦åŸç†ç»“åˆä½¿ç”¨ï¼Œä¾‹å¦‚ä»¥10çš„å¹‚æ¬¡è¿›è¡ŒåŠ æ³•çš„æ€§è´¨ã€‚è¦æ‰¾åˆ°2å’Œ10çš„å’Œï¼Œä½ ä¼šè®¡ç®—10åŠ 2ï¼Œå³12ã€‚è¿™ä¹Ÿå¯ä»¥å†™ä½œ10 + 2 = 12ã€‚å¦‚æœä½ æƒ³å¤šæ¬¡åŠ 2åˆ°ä¸€ä¸ªæ›´å¤§çš„æ•°ä¸Šï¼Œä½ ä¼šè®¡ç®—10åŠ 2åŠ 4åŠ 6åŠ 8åŠ 10åŠ 12åŠ 14ã€‚è¿™å°†å¾—åˆ°100ï¼Œæ­£å¥½æ˜¯10åŠ 2çš„å’Œã€‚</p>
</div>

<div class="original">
  <p>7KHQXPEHULVSOXVZKLFKHTXDOV7KLVQXPEHULVFRPPRQO\FDOOHGWKHQXPEHU7KHDQGDUHFDOOHGWKHDGGHQGDQGPXOWLSOLHURUWKHDGGHQGDQGPXOWLSOLHURIWKHQXPEHU7KHVHIDFWRUVDUHFRPPRQO\XVHGLQFRXQWLQJQXPEHUVDQGH[SUHVVLQJQXPEHUVDVVXPVDQGPXOWLSOHVRIRWKHUQXPEHUV ?Q?Q,WÂ·VJRRGSUDFWLFHWREHH[SOLFLWWKDW\RXÂ·UHFRXQWLQJXSZKHQ\RXXVHWKHQXPEHUDQGWKDW\RXÂ·UHDGGLQJZKHQ\RXDGGWRWKHQXPEHU2WKHUFRPPRQZD\VRIDGGLQJXSQXPEHUVLQFOXGHDGGLQJLQGLYLGXDOGLJLWVRUJURXSVRIGLJLWV)RUH[DPSOH FRXOGEHZULWWHQDVRURURURU6LPLODUO\ FRXOGEHZULWWHQDVRURU.HHSLQPLQGWKDWWKHVHZRUGLQJVFDQEHFRQIXVLQJVRLWÂ·VXVXDOO\EHVWWREHH[SOLFLWZKHQDGGLQJXSQXPEHUVOLNHWKLV</p>
</div>
<div class="translation">
  <p>æ•°å­—10+2æ˜¯10åŠ 2ï¼Œç­‰äº12ã€‚è¿™ä¸ªæ•°å­—é€šå¸¸è¢«ç§°ä¸ºâ€œ10+2æ•°å­—â€ã€‚10å’Œ2è¢«ç§°ä¸ºâ€œåŠ æ•°â€å’Œâ€œä¹˜æ•°â€ï¼Œæˆ–â€œåŠ æ•°å’Œä¹˜æ•°â€çš„10+2æ•°å­—ã€‚è¿™äº›å› å­å¸¸ç”¨äºè®¡æ•°æ•°å­—ï¼Œå¹¶å°†æ•°å­—è¡¨ç¤ºä¸ºå…¶ä»–æ•°å­—çš„å’Œä¸å€æ•°ã€‚åœ¨è®¡æ•°æ—¶ï¼Œæ˜¾å¼è¯´æ˜ä½ ä½¿ç”¨çš„æ˜¯10+2æ•°å­—ï¼Œä»¥åŠå½“ä½ åœ¨æ•°å­—10ä¸ŠåŠ 2æ—¶ä½ æ­£åœ¨åŠ 2ï¼Œè¿™æ˜¯ä¸€ä¸ªè‰¯å¥½çš„å®è·µã€‚å…¶ä»–å¸¸è§çš„æ•°å­—ç›¸åŠ æ–¹å¼åŒ…æ‹¬æ·»åŠ å•ä¸ªæ•°å­—æˆ–æ•°å­—ç»„ã€‚ä¾‹å¦‚ï¼Œ10+2å¯ä»¥å†™ä½œ12ï¼Œæˆ–12ã€14ï¼Œæˆ–12ã€2ï¼Œæˆ–12ã€2ï¼Œæˆ–12ã€‚ç±»ä¼¼åœ°ï¼Œ10+2å¯ä»¥å†™ä½œ14ï¼Œæˆ–10ã€2ï¼Œæˆ–12ã€‚è¯·è®°ä½ï¼Œè¿™äº›æªè¾å¯èƒ½ä»¤äººå›°æƒ‘ï¼Œå› æ­¤åœ¨åƒè¿™æ ·ç›¸åŠ æ•°å­—æ—¶ï¼Œé€šå¸¸æœ€å¥½æ˜¾å¼è¯´æ˜ã€‚</p>
</div>

<div class="original">
  <p>Ã²Æ ÄÈ¬rÅ±Ä–ÄÅšSOXVLV</p>
</div>
<div class="translation">
  <p>ç”¨æˆ·ï¼š10åŠ 2ç­‰äº12ã€‚</p>
</div>

<div class="figure">
  <p>Figure 1: BoNBoN alignment achieves high win rates while minimally affecting off-target attributes of generation. Left: Average length of responses versus win rate of models aligned using each method on the Anthropic helpful and harmless single turn dialogue task, using n=8. As predicted by theory, best-of-n achieves an excellent win rate while minimally affecting the off-target attribute length. Moreover, the BoNBoN aligned model effectively mimics this optimal policy, achieving a much higher win rate at low off-target drift than other alignment approaches. Right: Sample responses from models with similar win rates to BoNBoN. Other methods require higher off-target deviation to achieve a comparably high win rate. We observe that this significantly changes their behavior on off-target aspects. Conversely, BoNBoN only minimally changes off-target behavior. See section 5 for details.</p>
</div>
<div class="translation">
  <p>å›¾1ï¼šBoNBoNå¯¹é½åœ¨æœ€å°åŒ–å½±å“ç”Ÿæˆçš„è„±é¶å±æ€§çš„åŒæ—¶å®ç°é«˜èƒœç‡ã€‚å·¦å›¾ï¼šåœ¨Anthropicæœ‰ç›Šæ— å®³å•è½®å¯¹è¯ä»»åŠ¡ä¸Šï¼ˆn=8ï¼‰ï¼Œä½¿ç”¨æ¯ç§æ–¹æ³•å¯¹é½çš„æ¨¡å‹çš„å¹³å‡å“åº”é•¿åº¦ä¸èƒœç‡å…³ç³»ã€‚æ­£å¦‚ç†è®ºé¢„æµ‹ï¼Œæœ€ä½³-of-nï¼ˆbest-of-nï¼‰åœ¨æœ€å°åŒ–å½±å“è„±é¶å±æ€§é•¿åº¦çš„åŒæ—¶å®ç°äº†ä¼˜å¼‚çš„èƒœç‡ã€‚æ­¤å¤–ï¼ŒBoNBoNå¯¹é½æ¨¡å‹æœ‰æ•ˆæ¨¡æ‹Ÿäº†è¿™ä¸€æœ€ä¼˜ç­–ç•¥ï¼Œåœ¨ä½è„±é¶åç§»ä¸‹å®ç°äº†æ¯”å…¶ä»–å¯¹é½æ–¹æ³•æ›´é«˜çš„èƒœç‡ã€‚å³å›¾ï¼šä¸BoNBoNèƒœç‡ç›¸ä¼¼çš„æ¨¡å‹å“åº”ç¤ºä¾‹ã€‚å…¶ä»–æ–¹æ³•éœ€è¦æ›´é«˜çš„è„±é¶åå·®æ‰èƒ½è¾¾åˆ°ç±»ä¼¼çš„é«˜èƒœç‡ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°è¿™æ˜¾è‘—æ”¹å˜äº†å®ƒä»¬åœ¨è„±é¶æ–¹é¢çš„è¡Œä¸ºã€‚ç›¸åï¼ŒBoNBoNä»…æœ€å°åŒ–æ”¹å˜è„±é¶è¡Œä¸ºã€‚è¯¦è§ç¬¬5èŠ‚ã€‚</p>
</div>

<div class="original">
  <p>There are two main approaches. First, <span class="term">RLHF methods</span> (Reinforcement Learning from Human Feedback) train an explicit reward model on the pairs, and then align the model using reinforcement learning with this learned reward [e.g., Ouy +22; Kau+23]. Second, <span class="term">contrastive methods</span> directly use the preference data to define an objective function for fine-tuning the LLM [Raf+23; Aza +24; Eth +24; Xu +24; HLT24]. In both cases, the trade-off between alignment and off-target behavior is controlled by a hyper-parameter that explicitly penalizes the divergence from the base LLM. For example, in the reinforcement learning setting, this is done by adding a regularization term that penalizes the estimated <span class="term">KL divergence</span> (Kullback-Leibler Divergence) between the aligned model and the reference model.</p>
</div>
<div class="translation">
  <p>ä¸»è¦æœ‰ä¸¤ç§æ–¹æ³•ã€‚ç¬¬ä¸€ï¼Œ<span class="term">RLHFæ–¹æ³•ï¼ˆåŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼‰</span>åœ¨åå¥½å¯¹æ•°æ®ä¸Šè®­ç»ƒä¸€ä¸ªæ˜¾å¼å¥–åŠ±æ¨¡å‹ï¼Œç„¶åä½¿ç”¨å¼ºåŒ–å­¦ä¹ åŸºäºè¯¥å¥–åŠ±å¯¹é½æ¨¡å‹[ä¾‹å¦‚ï¼ŒOuy +22; Kau+23]ã€‚ç¬¬äºŒï¼Œ<span class="term">å¯¹æ¯”æ–¹æ³•</span>ç›´æ¥ä½¿ç”¨åå¥½æ•°æ®å®šä¹‰ç›®æ ‡å‡½æ•°æ¥å¾®è°ƒLLM[Raf+23; Aza +24; Eth +24; Xu +24; HLT24]ã€‚åœ¨ä¸¤ç§æƒ…å†µä¸‹ï¼Œå¯¹é½å’Œè„±é¶è¡Œä¸ºä¹‹é—´çš„æƒè¡¡é€šè¿‡ä¸€ä¸ªè¶…å‚æ•°æ§åˆ¶ï¼Œè¯¥å‚æ•°æ˜¾å¼æƒ©ç½šä¸åŸºç¡€LLMçš„åç¦»ã€‚ä¾‹å¦‚ï¼Œåœ¨å¼ºåŒ–å­¦ä¹ è®¾ç½®ä¸­ï¼Œè¿™æ˜¯é€šè¿‡æ·»åŠ ä¸€ä¸ªæ­£åˆ™åŒ–é¡¹æ¥å®ç°çš„ï¼Œè¯¥æƒ©ç½šå¯¹é½æ¨¡å‹ä¸å‚è€ƒæ¨¡å‹ä¹‹é—´çš„ä¼°è®¡<span class="term">KLæ•£åº¦ï¼ˆKullback-Leibleræ•£åº¦ï¼‰</span>ã€‚</p>
</div>

<div class="original">
  <p>The first main question we address in this paper is: what is the relationship between the sampling distribution defined by these approaches and the sampling distribution defined by <span class="term">best-of-n (BoN) sampling</span>? This is important, in particular, because in principle we could forgo the explicit alignment training and just use BoN sampling. However, it is not clear when each option should be preferred.</p>
</div>
<div class="translation">
  <p>æœ¬æ–‡è§£å†³çš„ç¬¬ä¸€ä¸ªä¸»è¦é—®é¢˜æ˜¯ï¼šè¿™äº›æ–¹æ³•å®šä¹‰çš„é‡‡æ ·åˆ†å¸ƒä¸<span class="term">æœ€ä½³-of-nï¼ˆBoNï¼‰é‡‡æ ·</span>å®šä¹‰çš„é‡‡æ ·åˆ†å¸ƒä¹‹é—´æœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿè¿™å¾ˆé‡è¦ï¼Œç‰¹åˆ«æ˜¯å› ä¸ºåŸåˆ™ä¸Šæˆ‘ä»¬å¯ä»¥æ”¾å¼ƒæ˜¾å¼å¯¹é½è®­ç»ƒï¼Œç›´æ¥ä½¿ç”¨BoNé‡‡æ ·ã€‚ç„¶è€Œï¼Œä½•æ—¶åº”ä¼˜å…ˆé€‰æ‹©æ¯ç§æ–¹æ¡ˆå°šä¸æ˜ç¡®ã€‚</p>
</div>

<div class="original">
  <p>Now, the comparison of training-aligned models and BoN is not fully fair. The reason is that producing a BoN sample requires drawing n samples from the base LLM (instead of just one). This is a substantial computational cost. The second main question we address is: if we do in fact want to sample from the BoN distribution, how can we train a LLM to mimic this distribution? If this can be done effectively, then the inference cost of BoN sampling can be avoided.</p>
</div>
<div class="translation">
  <p>ç°åœ¨ï¼Œæ¯”è¾ƒè®­ç»ƒå¯¹é½æ¨¡å‹å’ŒBoNå¹¶ä¸å®Œå…¨å…¬å¹³ã€‚åŸå› æ˜¯ç”Ÿæˆä¸€ä¸ªBoNæ ·æœ¬éœ€è¦ä»åŸºç¡€LLMæŠ½å–nä¸ªæ ·æœ¬ï¼ˆè€Œéä»…ä¸€ä¸ªï¼‰ã€‚è¿™æ˜¯ä¸€ä¸ªæ˜¾è‘—çš„è®¡ç®—æˆæœ¬ã€‚æˆ‘ä»¬è§£å†³çš„ç¬¬äºŒä¸ªä¸»è¦é—®é¢˜æ˜¯ï¼šå¦‚æœæˆ‘ä»¬ç¡®å®æƒ³ä»BoNåˆ†å¸ƒé‡‡æ ·ï¼Œå¦‚ä½•è®­ç»ƒä¸€ä¸ªLLMæ¥æ¨¡æ‹Ÿè¯¥åˆ†å¸ƒï¼Ÿå¦‚æœè¿™èƒ½æœ‰æ•ˆå®ç°ï¼Œåˆ™å¯ä»¥é¿å…BoNé‡‡æ ·çš„æ¨ç†æˆæœ¬ã€‚</p>
</div>

<div class="original">
  <p>We answer these questions with the following contributions:</p>
  <p>1. We show that the BoN sampling distribution can be embedded in a common class with the distributions produced by training-based alignment methods. Within this common class, we derive the distribution with the best possible trade-off between win-rate against the base model vs KL distance from the base model. Then, we show that the BoN distribution is essentially equal to this Pareto-optimal distribution.</p>
  <p>2. We then develop an effective method for training a LLM to mimic the BoN sampling distribution. In essence, the procedure draws best-of-n and worst-of-n samples as training data, and combines these with an objective function we derive by exploiting the analytical form of the BoN distribution. We call this procedure <span class="term">BoNBoN Alignment</span>.</p>
  <p>3. Finally, we show empirically that BoNBoN Alignment yields models that achieve high win rates while minimally affecting off-target aspects of the generations, outperforming baselines.</p>
</div>
<div class="translation">
  <p>æˆ‘ä»¬é€šè¿‡ä»¥ä¸‹è´¡çŒ®å›ç­”è¿™äº›é—®é¢˜ï¼š</p>
  <p>1. æˆ‘ä»¬è¯æ˜BoNé‡‡æ ·åˆ†å¸ƒå¯ä»¥åµŒå…¥ä¸€ä¸ªå…¬å…±ç±»ï¼Œä¸åŸºäºè®­ç»ƒçš„å¯¹é½æ–¹æ³•äº§ç”Ÿçš„åˆ†å¸ƒå…±äº«ã€‚åœ¨è¯¥å…¬å…±ç±»ä¸­ï¼Œæˆ‘ä»¬æ¨å¯¼å‡ºåœ¨åŸºç¡€æ¨¡å‹èƒœç‡ä¸åŸºç¡€æ¨¡å‹KLè·ç¦»ä¹‹é—´å…·æœ‰æœ€ä½³æƒè¡¡çš„åˆ†å¸ƒã€‚ç„¶åï¼Œæˆ‘ä»¬è¯æ˜BoNåˆ†å¸ƒæœ¬è´¨ä¸Šç­‰äºè¿™ä¸ªå¸•ç´¯æ‰˜æœ€ä¼˜åˆ†å¸ƒã€‚</p>
  <p>2. æˆ‘ä»¬éšåå¼€å‘äº†ä¸€ç§æœ‰æ•ˆæ–¹æ³•æ¥è®­ç»ƒLLMæ¨¡æ‹ŸBoNé‡‡æ ·åˆ†å¸ƒã€‚æœ¬è´¨ä¸Šï¼Œè¯¥è¿‡ç¨‹æŠ½å–æœ€ä½³-of-nå’Œæœ€å·®-of-næ ·æœ¬ä½œä¸ºè®­ç»ƒæ•°æ®ï¼Œå¹¶ç»“åˆæˆ‘ä»¬é€šè¿‡BoNåˆ†å¸ƒè§£æå½¢å¼æ¨å¯¼çš„ç›®æ ‡å‡½æ•°ã€‚æˆ‘ä»¬ç§°æ­¤è¿‡ç¨‹ä¸º<span class="term">BoNBoNå¯¹é½</span>ã€‚</p>
  <p>3. æœ€åï¼Œæˆ‘ä»¬é€šè¿‡å®éªŒè¯æ˜ï¼ŒBoNBoNå¯¹é½äº§ç”Ÿçš„æ¨¡å‹åœ¨æœ€å°åŒ–å½±å“ç”Ÿæˆè„±é¶æ–¹é¢çš„åŒæ—¶å®ç°é«˜èƒœç‡ï¼Œä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</p>
</div>

<h1>æ‘˜è¦æ€»ç»“</h1>
<p>æœ¬æ–‡çš„æ ¸å¿ƒå†…å®¹åˆ†ä¸ºä¸¤éƒ¨åˆ†ã€‚ç¬¬ä¸€éƒ¨åˆ†æ˜¯åŸºç¡€æ•°å­¦è§£é‡Šï¼šé€šè¿‡é—®ç­”å½¢å¼è¯¦ç»†è¯´æ˜åŠ æ³•è¿ç®—ï¼ˆå¦‚10åŠ 2ç­‰äº12ï¼‰ï¼Œå¼ºè°ƒåºæ•°åŠ æ³•çš„åŸç†ã€æ•°å­—è¡¨ç¤ºæ–¹æ³•ï¼ˆå¦‚åŠ æ•°å’Œä¹˜æ•°ï¼‰ï¼Œä»¥åŠåœ¨è®¡æ•°æ—¶æ˜¾å¼è¯´æ˜çš„é‡è¦æ€§ï¼Œä»¥é¿å…æ··æ·†ã€‚ç¬¬äºŒéƒ¨åˆ†æ˜¯AIå¯¹é½ç ”ç©¶ï¼šèšç„¦äºå¤§å‹è¯­è¨€æ¨¡å‹å¯¹é½æ–¹æ³•ï¼Œæ¯”è¾ƒäº†RLHFï¼ˆåŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼‰å’Œå¯¹æ¯”æ–¹æ³•ï¼ˆå¦‚DPOï¼‰çš„ä¼˜ç¼ºç‚¹ï¼Œå¹¶å¼•å…¥æœ€ä½³-of-nï¼ˆBoNï¼‰é‡‡æ ·ä½œä¸ºé«˜æ•ˆæ›¿ä»£ã€‚è®ºæ–‡æå‡ºä¸¤ä¸ªå…³é”®é—®é¢˜ï¼šä¸€æ˜¯åˆ†æBoNåˆ†å¸ƒä¸è®­ç»ƒå¯¹é½åˆ†å¸ƒçš„å…³ç³»ï¼Œè¯æ˜BoNæœ¬è´¨ä¸Šæ˜¯å¸•ç´¯æ‰˜æœ€ä¼˜åˆ†å¸ƒï¼›äºŒæ˜¯å¼€å‘BoNBoNå¯¹é½æ–¹æ³•ï¼Œé€šè¿‡ä½¿ç”¨æœ€ä½³/æœ€å·®æ ·æœ¬å’Œè§£æç›®æ ‡å‡½æ•°è®­ç»ƒLLMæ¥æ¨¡æ‹ŸBoNåˆ†å¸ƒï¼Œä»è€Œå‡å°‘æ¨ç†æˆæœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBoNBoNåœ¨Anthropicä»»åŠ¡ä¸Šå®ç°é«˜èƒœç‡çš„åŒæ—¶æœ€å°åŒ–è„±é¶å±æ€§ï¼ˆå¦‚å“åº”é•¿åº¦ï¼‰åç§»ï¼Œæ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚è´¡çŒ®åŒ…æ‹¬ç†è®ºæ¨å¯¼ã€æ–¹æ³•åˆ›æ–°å’Œå®è¯éªŒè¯ã€‚</p>

<h1>æœ¯è¯­è¯†åˆ«</h1>
<ul>
  <li><span class="term">æœ€ä½³-of-né‡‡æ ·ï¼ˆBest-of-n Sampling, BoN Samplingï¼‰</span>: ä¸€ç§é‡‡æ ·ç­–ç•¥ï¼Œä»åŸºç¡€è¯­è¨€æ¨¡å‹ç”Ÿæˆnä¸ªå“åº”ï¼Œç„¶ååŸºäºå¥–åŠ±æ¨¡å‹é€‰æ‹©æœ€ä¼˜æ ·æœ¬ã€‚å®ƒèƒ½åœ¨ä¸è®­ç»ƒçš„æƒ…å†µä¸‹å®ç°é«˜å¯¹é½æ€§èƒ½ï¼Œä½†è®¡ç®—æˆæœ¬é«˜ï¼Œå› ä¸ºéœ€è¦å¤šæ¬¡æ¨ç†ã€‚</li>
  <li><span class="term">RLHFï¼ˆReinforcement Learning from Human Feedbackï¼‰</span>: åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ã€‚æ–¹æ³•åŒ…æ‹¬è®­ç»ƒä¸€ä¸ªå¥–åŠ±æ¨¡å‹æ¥è¯„ä¼°å“åº”è´¨é‡ï¼Œç„¶åä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆå¦‚PPOï¼‰å¯¹é½è¯­è¨€æ¨¡å‹ï¼Œä»¥æœ€å¤§åŒ–å¥–åŠ±åŒæ—¶æœ€å°åŒ–ä¸å‚è€ƒæ¨¡å‹çš„KLæ•£åº¦ã€‚</li>
  <li><span class="term">å¯¹æ¯”æ–¹æ³•ï¼ˆContrastive Methodsï¼‰</span>: ç›´æ¥ä½¿ç”¨åå¥½æ•°æ®å¾®è°ƒLLMçš„æŠ€æœ¯ï¼Œæ— éœ€æ˜¾å¼å¥–åŠ±æ¨¡å‹ã€‚ä¾‹å¦‚DPOï¼ˆDirect Preference Optimizationï¼‰ï¼Œé€šè¿‡å¯¹æ¯”æ­£è´Ÿæ ·æœ¬æ¥å®šä¹‰æŸå¤±å‡½æ•°ï¼Œä¼˜åŒ–æ¨¡å‹åå¥½ã€‚</li>
  <li><span class="term">KLæ•£åº¦ï¼ˆKL Divergence, Kullback-Leibler Divergenceï¼‰</span>: è¡¡é‡ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒå·®å¼‚çš„æŒ‡æ ‡ã€‚åœ¨AIå¯¹é½ä¸­ï¼Œç”¨äºæ­£åˆ™åŒ–å¯¹é½æ¨¡å‹ä¸å‚è€ƒæ¨¡å‹çš„åç¦»ï¼Œå…¬å¼ä¸º \\( D_{\\text{KL}}(P \\parallel Q) = \\sum P(x) \\log \\frac{P(x)}{Q(x)} \\)ï¼Œå…¶ä¸­På’ŒQä¸ºåˆ†å¸ƒã€‚</li>
  <li><span class="term">è„±é¶è¡Œä¸ºï¼ˆOff-target Behaviorï¼‰</span>: å¯¹é½è¿‡ç¨‹ä¸­ä¸å¸Œæœ›æ”¹å˜çš„æ¨¡å‹å±æ€§ï¼Œå¦‚å“åº”é•¿åº¦ã€é£æ ¼æˆ–æ— å…³ç‰¹å¾ã€‚BoNBoNæ–¹æ³•æ—¨åœ¨æœ€å°åŒ–æ­¤ç±»åç§»ã€‚</li>
  <li><span class="term">BoNBoNå¯¹é½ï¼ˆBoNBoN Alignmentï¼‰</span>: è®ºæ–‡æå‡ºçš„æ–¹æ³•ï¼Œè®­ç»ƒLLMæ¨¡æ‹ŸBoNåˆ†å¸ƒã€‚è¿‡ç¨‹åŒ…æ‹¬ï¼šæŠ½å–æœ€ä½³-of-nå’Œæœ€å·®-of-næ ·æœ¬ä½œä¸ºæ•°æ®ï¼Œç»“åˆBoNåˆ†å¸ƒçš„è§£æç›®æ ‡å‡½æ•°è¿›è¡Œå¾®è°ƒï¼Œä»¥å‡å°‘æ¨ç†å¼€é”€ã€‚</li>
  <li><span class="term">å¸•ç´¯æ‰˜æœ€ä¼˜åˆ†å¸ƒï¼ˆPareto-optimal Distributionï¼‰</span>: åœ¨å¤šç›®æ ‡ä¼˜åŒ–ä¸­ï¼ŒæŒ‡æ— æ³•åœ¨ä¸æŸå®³ä¸€ä¸ªç›®æ ‡çš„æƒ…å†µä¸‹æ”¹è¿›å¦ä¸€ä¸ªç›®æ ‡çš„åˆ†å¸ƒã€‚è®ºæ–‡è¯æ˜BoNåˆ†å¸ƒåœ¨æ­¤ç±»ä¸­æ˜¯æœ€ä¼˜çš„ï¼Œå¹³è¡¡äº†èƒœç‡å’ŒKLè·ç¦»ã€‚</li>
  <li><span class="term">èƒœç‡ï¼ˆWin Rateï¼‰</span>: è¯„ä¼°æŒ‡æ ‡ï¼Œè¡¨ç¤ºå¯¹é½æ¨¡å‹åœ¨åå¥½æ¯”è¾ƒä¸­å‡»è´¥åŸºç¡€æ¨¡å‹çš„æ¯”ç‡ã€‚é«˜èƒœç‡è¡¨ç¤ºæ›´å¥½çš„å¯¹é½æ€§èƒ½ã€‚</li>
</ul>

</body>
</html>
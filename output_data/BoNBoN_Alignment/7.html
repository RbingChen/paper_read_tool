<!DOCTYPE html>
        <html>
        <head>
          <style>
            body { font-family: Arial, sans-serif; }
            .container { max-width: 800px; margin: auto; }
            .explanation, .summary { margin-bottom: 20px; }
            .original {
              background-color: #f0f0f0; /* light gray */
              border: 1px solid #cccccc; /* gray border */
              padding: 10px;
              margin-bottom: 10px;
            }
            .translation {
              background-color: #e0ffe0; /* light green */
              border: 1px solid #00cc00; /* green border */
              padding: 10px;
              margin-bottom: 20px;
            }
            .figure-ref {
              background-color: yellow;
              padding: 2px;
            }
            .terminology-list {
              list-style-type: none;
              padding: 0;
            }
            .terminology-list li {
              margin-bottom: 10px;
            }
            .term {
              font-weight: bold;
              color: red;
            }
            .footnote {
              font-size: 0.9em;
              color: #555;
            }
            h2, h3 {
              color: #333;
            }
          </style>
          <!-- We don't have MathJax in this text, but if we did we would include: -->
          <!-- <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
          <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script> -->
        </head>
        <body>
          <div class="container">
            <h1>Paper Analysis</h1>

            <h2>Content Understanding</h2>
            <div class="explanation">
              <p>这段文字主要描述了一项关于语言模型对齐方法的实验研究。研究者提出了BoNBoN方法，旨在通过模仿最优的best-of-n策略来平衡胜率（win-rate）和偏离目标偏差（off-target deviation）。实验在两个任务上进行：单轮对话生成和文本摘要。研究者使用了Anthropic HH和OpenAI TL;DR数据集，并进行了数据过滤。他们比较了BoNBoN与基线方法（DPO和IPO）的性能，结果表明BoNBoN在几乎不引入超参数的情况下，实现了更好的胜率与偏离目标偏差的权衡。具体来说，BoNBoN在保持高胜率的同时，偏离目标偏差（如响应长度变化）更小，而其他方法在类似胜率下会导致更大的偏差。</p>
            </div>

            <h2>Translation</h2>
            <div class="translation-section">

              <!-- First paragraph -->
              <div class="original">
                <p>high. We emphasize that both objective functions target the same policy; neither is regularizing towards some conflicting objective. That is, the trade-off between win-rate and off-target change is handled implicitly by the (optimal) best-of- n procedure. This is in contrast to approaches that manage this trade-off explicitly (and sub-optimally) by regularizing towards the reference model. Reflecting this, we choose α so that the contribution of each term to the total loss is approximately equal.</p>
              </div>
              <div class="translation">
                <p>很高。我们强调两个目标函数针对的是同一个策略；都没有为了某个冲突的目标而进行正则化。也就是说，赢率和偏离目标变化之间的权衡是由（最优的）best-of-n过程隐式处理的。这与那些通过对参考模型进行正则化来显式（且次优地）管理这种权衡的方法形成对比。有鉴于此，我们选择α使得每个项对总损失的贡献大致相等。</p>
              </div>

              <!-- Section 5 -->
              <div class="original">
                <h3>5 Experiments</h3>
              </div>
              <div class="translation">
                <h3>5 实验</h3>
              </div>

              <!-- Section 5.1 -->
              <div class="original">
                <h3>5.1 Experimental Setup</h3>
                <p>We study two tasks: a) single-turn dialogue generation, for which we conduct experiments on the Anthropic Helpful and Harmless (HH) dataset [Bai+22] and b) text summarization, for which we use the OpenAI TL;DR dataset [Sti+20]. Due to computational constraints, we filter the HH data to only keep prompts for which response length is less than 500 characters, resulting in 106,754 training dialogues. For TL;DR dataset, we discard instances where the input post length is less than 90 characters, resulting in 92,831 (14,764 prompts) training posts. Each example in both datasets contains a pair of responses that were generated by a large language model along with a label denoting the human-preferred response among the two generations.</p>
                <p>We want to compare different alignment methods on their ground truth win rate. Accordingly, we need a ground truth ranker. To that end, we construct data by using an off-the-shelf reward model<sup>4</sup> as our ground truth. (In particular, we relabel the human preferences).</p>
                <p>As the reference model, we fine-tune Pythia-2.8b [Bid+23] with supervised fine-tuning (SFT) on the human-preferred completions from each dataset. For alignment methods other than BoNBoN, we draw n=8 completions for each prompt, and we use the best and worst completions as training data for them. For BoNBoN, we vary n from 2 to 8.</p>
                <p>We use DPO and IPO as baselines for the alignment task. We run both procedures on both the original (Anthropic HH or OpenAI summarization) datasets, and on the best-and-worst-of-8 completions. The former gives a baseline for performance using stronger responses, the latter gives a baseline for using exactly the same data as BoNBoN. Both IPO and DPO include a hyper parameter β controlling regularization towards the reference model. We report results for each method run with several values of β. For BoNBoN, we use α=0.005 for all experiments. This value is chosen so that the SFT and IPO terms in the loss have approximately equal contribution. Further details can be found in appendix C.</p>
                <p class="footnote"><sup>4</sup>https://huggingface.co/OpenAssistant/reward-model-deberta-v3-large-v2</p>
              </div>
              <div class="translation">
                <h3>5.1 实验设置</h3>
                <p>我们研究两个任务：a) 单轮对话生成，为此我们在Anthropic Helpful and Harmless (HH) 数据集 [Bai+22] 上进行实验；b) 文本摘要，为此我们使用OpenAI TL;DR 数据集 [Sti+20]。由于计算限制，我们对HH数据集进行过滤，只保留响应长度小于500字符的提示，得到106,754个训练对话。对于TL;DR数据集，我们丢弃输入帖子长度小于90字符的实例，得到92,831个训练帖子（14,764个提示）。两个数据集中的每个示例都包含一对由大型语言模型生成的响应，以及一个标签，表示在两次生成中人类偏好的响应。</p>
                <p>我们想要比较不同对齐方法在真实胜率上的表现。因此，我们需要一个真实排名器。为此，我们通过使用一个现成的奖励模型<sup>4</sup>作为真实值来构建数据。（具体来说，我们重新标记了人类偏好）。</p>
                <p>作为参考模型，我们在每个数据集中人类偏好的补全上使用监督微调（SFT）对Pythia-2.8b [Bid+23]进行微调。对于除BoNBoN以外的对齐方法，我们为每个提示抽取n=8个补全，并使用最佳和最差补全作为它们的训练数据。对于BoNBoN，我们将n从2到8变化。</p>
                <p>我们使用DPO和IPO作为对齐任务的基线。我们在原始（Anthropic HH或OpenAI摘要）数据集以及best-and-worst-of-8补全上运行这两种方法。前者为使用更强响应的性能提供基线，后者为使用与BoNBoN完全相同的数据提供基线。IPO和DPO都包含一个超参数β，用于控制对参考模型的正则化。我们报告了每种方法在多个β值下的运行结果。对于BoNBoN，我们在所有实验中使用α=0.005。选择该值是为了使损失中的SFT项和IPO项的贡献大致相等。更多细节可在附录C中找到。</p>
                <p class="footnote"><sup>4</sup>https://huggingface.co/OpenAssistant/reward-model-deberta-v3-large-v2</p>
              </div>

              <!-- Section 5.2 -->
              <div class="original">
                <h3>5.2 BoNBoN achieves high win rate with little off-target deviation</h3>
                <p>We are interested in the win-rate vs off-target deviation trade-off. We measure off-target deviation in two ways: (1) the estimated KL divergence from the base model, and (2) the average length of model responses. Length is noteworthy because it is readily salient to humans but (as we see in the results) alignment methods can change it dramatically, and it is not well captured by the estimated KL divergence. We show win-rate vs off-target behavior for each trained model in <span class="figure-ref">Figure 3</span>. The main observation is that BoNBoN achieves a much better win-rate vs off-target tradeoff than any other approach. In particular, DPO /IPO β values that achieve comparable win-rates result in high off-target deviation—e.g., nearly doubling the average response length!</p>
                <p>To further explore this point, we examine sample responses from baseline models with similar win-rates to BoNBoN. Examples are shown in <span class="figure-ref">Figure 1</span> and <span class="figure-ref">tables 1 and 6</span>. Other approaches can dramatically change off-target behavior.</p>
              </div>
              <div class="translation">
                <h3>5.2 BoNBoN以微小的偏离目标偏差实现高胜率</h3>
                <p>我们关注胜率与偏离目标偏差之间的权衡。我们通过两种方式测量偏离目标偏差：(1) 与基础模型的估计KL散度；(2) 模型响应的平均长度。长度值得注意，因为它对人类来说很容易察觉，但（正如我们在结果中看到的）对齐方法可以显著改变它，而估计的KL散度无法很好地捕捉这一点。我们在<span class="figure-ref">图3</span>中展示了每个训练模型的胜率与偏离目标行为。主要观察结果是，BoNBoN实现了比任何其他方法都更好的胜率与偏离目标权衡。特别是，达到类似胜率的DPO/IPO的β值会导致较高的偏离目标偏差——例如，几乎使平均响应长度翻倍！</p>
                <p>为了进一步探讨这一点，我们检查了与BoNBoN胜率相似的基线模型的响应样本。示例见<span class="figure-ref">图1</span>和<span class="figure-ref">表1、表6</span>。其他方法会显著改变偏离目标行为。</p>
              </div>

              <!-- Section 5.3 -->
              <div class="original">
                <h3>5.3 BoNBoN mimics the best-of- n policy</h3>
                <p><span class="figure-ref">Figure 3</span> shows SFT and IPO fine-tuned on the best-of-n data. We observe that BoNBoN dramatically outperforms these methods at all values of β, and is closer to the (optimal) BoN distribution. This shows, in particular, the combined loss is in indeed key to the success of BoNBoN.</p>
                <p>One substantial practical advantage of BoNBoN is that it is nearly hyper-parameter free. Because the goal is to mimic the best-of-n distribution, which is known to be optimal, we do not need to sweep hyper-parameters for the ‘best’ choice of win-rate vs KL. In particular, the β term in IPO is analytically derived in Theorem 3. In <span class="figure-ref">Figure 5</span> we show the win rate vs off-target behavior</p>
              </div>
              <div class="translation">
                <h3>5.3 BoNBoN模仿best-of-n策略</h3>
                <p><span class="figure-ref">图3</span>展示了在best-of-n数据上微调的SFT和IPO。我们观察到，BoNBoN在所有β值上都显著优于这些方法，并且更接近（最优的）BoN分布。这尤其表明，组合损失确实是BoNBoN成功的关键。</p>
                <p>BoNBoN的一个显著实际优势是它几乎无需超参数调整。因为目标是模仿已知最优的best-of-n分布，所以我们不需要为了在胜率与KL散度之间寻找“最佳”权衡而遍历超参数。特别是，IPO中的β项在定理3中通过分析得出。我们在<span class="figure-ref">图5</span>中展示了胜率与偏离目标行为。</p>
              </div>

            </div> <!-- end of translation-section -->

            <h2>Summary</h2>
            <div class="summary">
              <p>本文实验部分研究了BoNBoN对齐方法在单轮对话生成和文本摘要任务上的性能。实验使用Anthropic HH和OpenAI TL;DR数据集，并进行了数据过滤。BoNBoN与基线方法（DPO和IPO）的比较表明，BoNBoN在胜率与偏离目标偏差（通过KL散度和响应长度衡量）的权衡上表现更优，且几乎无需超参数调整。具体而言，在相同胜率下，DPO和IPO会导致更大的偏离目标偏差（如响应长度翻倍），而BoNBoN通过组合损失函数有效模仿了最优的best-of-n策略。</p>
            </div>

            <h2>Terminology</h2>
            <div class="terminology">
              <ul class="terminology-list">
                <li><span class="term">BoNBoN</span>: BoNBoN是一种对齐方法，旨在模仿最优的best-of-n策略。它通过组合损失函数（包括SFT和IPO项）来优化策略，无需调整超参数即可在胜率和偏离目标
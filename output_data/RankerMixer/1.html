<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <title>è®ºæ–‡è§£æï¼šRankMixeræ¶æ„</title>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style>
        body { font-family: 'Segoe UI', sans-serif; line-height: 1.6; }
        .original { background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 15px; margin-bottom: 10px; }
        .translation { background-color: #e6ffed; border: 1px solid #c3e6cb; padding: 15px; margin-bottom: 20px; }
        .highlight { color: #dc3545; font-weight: bold; }
        .formula-container { text-align: center; margin: 20px 0; }
        .formula-label { font-style: italic; margin-top: 5px; }
        .section-title { color: #2c3e50; border-bottom: 2px solid #3498db; padding-bottom: 5px; }
        .figure { background-color: #fffde7; padding: 15px; text-align: center; margin: 20px 0; border: 1px dashed #ffd54f; }
    </style>
</head>
<body>

<!-- å†…å®¹ç†è§£ -->
<h2 class="section-title">å†…å®¹ç†è§£</h2>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º<b class="highlight">RankMixer</b>çš„æ–°å‹æ¨èç³»ç»Ÿæ¶æ„ï¼Œæ ¸å¿ƒåˆ›æ–°åœ¨äºï¼š1ï¼‰<b class="highlight">å¤šå¤´ä»¤ç‰Œæ··åˆ(Multi-head token mixing)</b>å®ç°å‚æ•°æ— å…³çš„è·¨ä»¤ç‰Œç‰¹å¾äº¤äº’ï¼Œä¼˜äºä¼ ç»Ÿæ³¨æ„åŠ›æœºåˆ¶ï¼›2ï¼‰<b class="highlight">æ¯ä»¤ç‰Œå‰é¦ˆç½‘ç»œ(Per-token FFN)</b>é€šè¿‡ç‹¬ç«‹å‚æ•°åˆ†é…è§£å†³ç‰¹å¾ç©ºé—´ä¸»å¯¼é—®é¢˜ã€‚é€šè¿‡æ‰©å±•ä¸º<b class="highlight">ç¨€ç–ä¸“å®¶æ··åˆ(Sparse MoE)</b>ç»“æ„ï¼Œåœ¨æŠ–éŸ³æ¨èç³»ç»Ÿä¸­å®ç°å‚æ•°é‡æå‡100å€çš„åŒæ—¶é™ä½æ¨ç†å»¶è¿Ÿã€‚è¯¥æ¶æ„çªç ´ä¼ ç»ŸTransformeråœ¨è®­ç»ƒæ•ˆç‡ã€IDç»„åˆçˆ†ç‚¸å’Œå†…å­˜é™åˆ¶ä¸Šçš„ç“¶é¢ˆã€‚</p>

<!-- å†…å®¹ç¿»è¯‘ -->
<h2 class="section-title">å†…å®¹ç¿»è¯‘</h2>

<div class="original">
Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY Zhifang Fan et al.
is based on two scalable components: 1. Multi-head token mixing achieves cross-token feature interactions only through the parameter-free operator. This strategy outperforms the self-attention mechanism in terms of performance and computational efficiency. 2.Per-token feed-forward networks (FFNs) expand model capacity significantly and tackle inter-feature-space domination problem by allocating independent parameters for different feature subspace modeling. These FFNs also align well with the recommendation data patterns, enabling a better scaling behavior.
</div>
<div class="translation">
ä¼šè®®ç¼©å†™ 'XX, 2018å¹´6æœˆ3-5æ—¥, çº½çº¦å·ä¼å¾·æ–¯æ‰˜å…‹, èŒƒå¿—èŠ³ç­‰äºº
åŸºäºä¸¤ä¸ªå¯æ‰©å±•ç»„ä»¶ï¼š1. <b class="highlight">å¤šå¤´ä»¤ç‰Œæ··åˆ(Multi-head token mixing)</b>ä»…é€šè¿‡æ— å‚æ•°ç®—å­å®ç°è·¨ä»¤ç‰Œç‰¹å¾äº¤äº’ã€‚è¯¥ç­–ç•¥åœ¨æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡ä¸Šä¼˜äºè‡ªæ³¨æ„åŠ›æœºåˆ¶ã€‚2. <b class="highlight">æ¯ä»¤ç‰Œå‰é¦ˆç½‘ç»œ(FFNs)</b>é€šè¿‡ä¸ºä¸åŒç‰¹å¾å­ç©ºé—´å»ºæ¨¡åˆ†é…ç‹¬ç«‹å‚æ•°ï¼Œæ˜¾è‘—æ‰©å±•æ¨¡å‹å®¹é‡å¹¶è§£å†³ç‰¹å¾ç©ºé—´ä¸»å¯¼é—®é¢˜ã€‚è¿™äº›FFNä¹Ÿä¸æ¨èæ•°æ®æ¨¡å¼é«˜åº¦å¥‘åˆï¼Œå®ç°æ›´ä¼˜çš„æ‰©å±•æ€§ã€‚
</div>

<div class="original">
To further boost the ROI of large-scale models, we extend the per-token FFN modules into a Sparse Mixture-of-Experts (MoE) structure. By dynamically activating only a subset of experts per token for different data, we can significantly increase the model capacity with a minimal increase in computational cost. RankMixer adopts a highly parallel architecture similar to transformers, but overcomes several critical limitations of self-attention based feature-interaction: low training efficiency, the combinatorial explosion when modeling cross-space ID similarity and severe memory-bound caused by attention weights matrix.
</div>
<div class="translation">
ä¸ºæå‡å¤§è§„æ¨¡æ¨¡å‹çš„æŠ•èµ„å›æŠ¥ç‡ï¼Œæˆ‘ä»¬å°†æ¯ä»¤ç‰ŒFFNæ¨¡å—æ‰©å±•ä¸º<b class="highlight">ç¨€ç–ä¸“å®¶æ··åˆ(Sparse Mixture-of-Experts, MoE)</b>ç»“æ„ã€‚é€šè¿‡ä¸ºä¸åŒæ•°æ®åŠ¨æ€æ¿€æ´»æ¯ä»¤ç‰Œçš„éƒ¨åˆ†ä¸“å®¶å­é›†ï¼Œèƒ½ä»¥æœ€å°è®¡ç®—æˆæœ¬æ˜¾è‘—æå‡æ¨¡å‹å®¹é‡ã€‚RankMixeré‡‡ç”¨ç±»ä¼¼Transformerçš„é«˜åº¦å¹¶è¡Œæ¶æ„ï¼Œä½†å…‹æœäº†åŸºäºè‡ªæ³¨æ„åŠ›çš„ç‰¹å¾äº¤äº’çš„å…³é”®ç¼ºé™·ï¼šè®­ç»ƒæ•ˆç‡ä½ã€è·¨ç©ºé—´IDç›¸ä¼¼æ€§å»ºæ¨¡çš„ç»„åˆçˆ†ç‚¸é—®é¢˜ï¼Œä»¥åŠæ³¨æ„åŠ›æƒé‡çŸ©é˜µå¯¼è‡´çš„å†…å­˜ç“¶é¢ˆã€‚
</div>

<div class="original">
At the same time, RankMixer offers greater model capacity and learning capability under the same Flops compared with Vanilla Transformer. In production deployment on Douyinâ€™s recommendation system, we demonstrate that it is feasible to scale model parameters by over 100 Ã—while maintaining shorter inference latency compared with previous baseline. This is made possible by the RankMixer architectureâ€™s ability to decouple parameter growth from FLOPs, and decouple FLOPs growth from actual cost through high MFU and engineering optimization.
</div>
<div class="translation">
åŒæ—¶ï¼Œåœ¨ç›¸åŒæµ®ç‚¹è¿ç®—é‡(Flops)ä¸‹ï¼ŒRankMixeræ¯”åŸå§‹Transformeræä¾›æ›´å¤§æ¨¡å‹å®¹é‡å’Œå­¦ä¹ èƒ½åŠ›ã€‚åœ¨æŠ–éŸ³æ¨èç³»ç»Ÿçš„ç”Ÿäº§éƒ¨ç½²ä¸­ï¼Œæˆ‘ä»¬è¯æ˜å‚æ•°é‡å¯æ‰©å±•100å€ä»¥ä¸Šï¼ŒåŒæ—¶ä¿æŒæ¯”åŸºçº¿æ›´ä½çš„æ¨ç†å»¶è¿Ÿã€‚è¿™å¾—ç›ŠäºRankMixeræ¶æ„èƒ½å°†å‚æ•°å¢é•¿ä¸FLOPsè§£è€¦ï¼Œå¹¶é€šè¿‡é«˜<b class="highlight">MFUï¼ˆçŸ©é˜µä¹˜æ³•åˆ©ç”¨ç‡ï¼‰</b>å’Œå·¥ç¨‹ä¼˜åŒ–å°†FLOPså¢é•¿ä¸å®é™…æˆæœ¬è§£è€¦ã€‚
</div>

<div class="original">
The main contributions can be summarized as follows:
â€¢We propose a novel architecture called RankMixer follows a hardware-aware model-design philosophy. We design the multi-head token mixing and per-token FFN strategies to capture heterogeneous feature interactions efficiently, and using dynamic routing strategy to improve the scalability of SparseMoE in RankMixer.
â€¢Leveraging levers of high MFU and performance-optimization, we scale the model parameters by 100 Ã—without increasing the inference cost, including improving GPU utilization, SparseMoE acceleration, and low-precision inference.
â€¢We conduct extensive offline and online experiments and investigate the modelâ€™s scaling law on the trillion-scale industrial recommendation dataset. The RankMixer model has been successfully deployed on the Douyin Feed Recommendation Ranking for full-traffic serving, achieving 0.2% and 0.5% increase on active days and App duration, respectively.
</div>
<div class="translation">
ä¸»è¦è´¡çŒ®æ€»ç»“å¦‚ä¸‹ï¼š
â€¢ æå‡ºéµå¾ªç¡¬ä»¶æ„ŸçŸ¥è®¾è®¡ç†å¿µçš„RankMixeræ¶æ„ï¼Œè®¾è®¡å¤šå¤´ä»¤ç‰Œæ··åˆå’Œæ¯ä»¤ç‰ŒFFNç­–ç•¥é«˜æ•ˆæ•è·å¼‚æ„ç‰¹å¾äº¤äº’ï¼Œå¹¶é‡‡ç”¨åŠ¨æ€è·¯ç”±ç­–ç•¥æå‡ç¨€ç–MoEçš„å¯æ‰©å±•æ€§ã€‚
â€¢ åˆ©ç”¨é«˜MFUå’Œæ€§èƒ½ä¼˜åŒ–æ æ†ï¼Œå®ç°å‚æ•°é‡100å€æ‰©å±•è€Œä¸å¢åŠ æ¨ç†æˆæœ¬ï¼ŒåŒ…æ‹¬æå‡GPUåˆ©ç”¨ç‡ã€ç¨€ç–MoEåŠ é€Ÿå’Œä½ç²¾åº¦æ¨ç†ã€‚
â€¢ åœ¨ä¸‡äº¿çº§å·¥ä¸šæ¨èæ•°æ®é›†ä¸Šå¼€å±•å®éªŒå¹¶ç ”ç©¶æ¨¡å‹æ‰©å±•è§„å¾‹ã€‚RankMixerå·²åœ¨æŠ–éŸ³Feedæ¨èæ’åºç³»ç»Ÿå…¨æµé‡éƒ¨ç½²ï¼Œæ´»è·ƒå¤©æ•°å’ŒAppæ—¶é•¿åˆ†åˆ«æå‡0.2%å’Œ0.5%ã€‚
</div>

<div class="original">
RankMixerâ€™s overall architecture consists of T input tokens processed by L successive RankMixer blocks, followed by an output pooling operator. Each RankMixer block has two main components: (1) a Multi-Head Token Mixing layer, and (2) a Per-Token Feed-Forward Network (PFFN) layer, as illustrated in Figure 1. First, the input vector einput is tokenized into ğ‘‡ feature tokens x1,x2,...,xğ‘‡, each representing a coherent feature vector. RankMixer blocks iteratively refine token representations for ğ¿ layers through:
</div>
<div class="translation">
RankMixeræ•´ä½“æ¶æ„åŒ…å«Tä¸ªè¾“å…¥ä»¤ç‰Œï¼Œç»Lä¸ªè¿ç»­RankMixerå—å¤„ç†åè¿›å…¥è¾“å‡ºæ± åŒ–ç®—å­ã€‚æ¯ä¸ªå—å«ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼š(1)<b class="highlight">å¤šå¤´ä»¤ç‰Œæ··åˆå±‚</b>ï¼Œ(2)<b class="highlight">æ¯ä»¤ç‰Œå‰é¦ˆç½‘ç»œ(PFFN)å±‚</b>ï¼ˆå¦‚å›¾1æ‰€ç¤ºï¼‰ã€‚è¾“å…¥å‘é‡einputè¢«ä»¤ç‰ŒåŒ–ä¸ºğ‘‡ä¸ªç‰¹å¾ä»¤ç‰Œx1,x2,...,xğ‘‡ï¼Œæ¯ä¸ªä»£è¡¨ä¸€ä¸ªè¿è´¯ç‰¹å¾å‘é‡ã€‚RankMixerå—é€šè¿‡ä»¥ä¸‹å…¬å¼è¿­ä»£ç²¾ç‚¼ä»¤ç‰Œè¡¨ç¤ºï¼š
</div>

<div class="formula-container">
    \[ S^{n-1} = \text{LN}(\text{TokenMixing}(X^{n-1}) + X^{n-1}) \]
    \[ X^n = \text{LN}(\text{PFFN}(S^{n-1}) + S^{n-1}) \quad (1) \]
    <div class="formula-label">å…¬å¼(1): RankMixerå—çš„è®¡ç®—æµç¨‹</div>
</div>

<div class="original">
where LN(Â·) is a layer normalization function, TokenMixing(Â·) and PFFN(Â·) are the multi-head Token Mixing module and the per-token FFN module, Xğ‘›âˆˆRğ‘‡Ã—ğ· is the output of the ğ‘›-th RankMixer block, X0âˆˆRğ‘‡Ã—ğ· is stacked by x1,x2,...,xğ‘‡, andğ· is the hidden dimension of the model.
</div>
<div class="translation">
å…¶ä¸­LN(Â·)æ˜¯å±‚å½’ä¸€åŒ–å‡½æ•°ï¼ŒTokenMixing(Â·)å’ŒPFFN(Â·)åˆ†åˆ«å¯¹åº”å¤šå¤´ä»¤ç‰Œæ··åˆæ¨¡å—å’Œæ¯ä»¤ç‰ŒFFNæ¨¡å—ï¼ŒXğ‘›âˆˆRğ‘‡Ã—ğ·è¡¨ç¤ºç¬¬ğ‘›ä¸ªå—çš„è¾“å‡ºï¼ŒX0âˆˆRğ‘‡Ã—ğ·ç”±x1,x2,...,xğ‘‡å †å è€Œæˆï¼Œğ·ä¸ºæ¨¡å‹éšè—ç»´åº¦ã€‚
</div>

<!-- æ‘˜è¦æ€»ç»“ -->
<h2 class="section-title">æ‘˜è¦æ€»ç»“</h2>
<p>æœ¬æ–‡æå‡º<b class="highlight">RankMixer</b>æ¨èæ¶æ„ï¼Œæ ¸å¿ƒåˆ›æ–°æ˜¯ï¼š1ï¼‰<b class="highlight">å¤šå¤´ä»¤ç‰Œæ··åˆ</b>ç”¨æ— å‚æ•°æ“ä½œå®ç°é«˜æ•ˆç‰¹å¾äº¤äº’ï¼Œæ›¿ä»£è®¡ç®—æ˜‚è´µçš„è‡ªæ³¨æ„åŠ›ï¼›2ï¼‰<b class="highlight">æ¯ä»¤ç‰ŒFFN</b>é€šè¿‡ç‹¬ç«‹å‚æ•°è§£å†³ç‰¹å¾ç©ºé—´ä¸»å¯¼é—®é¢˜ã€‚é€šè¿‡æ‰©å±•ä¸º<b class="highlight">ç¨€ç–MoE</b>ç»“æ„ï¼Œåœ¨æŠ–éŸ³æ¨èç³»ç»Ÿå®ç°å‚æ•°é‡100å€å¢é•¿ï¼ˆ>100Ã—ï¼‰ä¸”é™ä½å»¶è¿Ÿï¼Œå…³é”®çªç ´åœ¨äºï¼šå‚æ•°å¢é•¿ä¸è®¡ç®—æˆæœ¬è§£è€¦ã€é«˜MFUä¼˜åŒ–ã€åŠ¨æ€ä¸“å®¶è·¯ç”±ã€‚å®éªŒè¯æ˜å…¶åœ¨ä¸‡äº¿çº§æ•°æ®é›†æå‡æ´»è·ƒåº¦0.2%å’Œæ—¶é•¿0.5%ï¼Œè§£å†³äº†ä¼ ç»ŸTransformerçš„IDç»„åˆçˆ†ç‚¸å’Œå†…å­˜ç“¶é¢ˆé—®é¢˜ã€‚</p>

<!-- æœ¯è¯­è¯†åˆ« -->
<h2 class="section-title">æœ¯è¯­è§£é‡Š</h2>
<dl>
    <dt><b class="highlight">å¤šå¤´ä»¤ç‰Œæ··åˆ (Multi-head token mixing)</b></dt>
    <dd>æ— å‚æ•°çš„ç‰¹å¾äº¤äº’æœºåˆ¶ï¼Œé€šè¿‡å¹¶è¡Œå¤šç»„æ“ä½œæ›¿ä»£è‡ªæ³¨æ„åŠ›ï¼Œè§£å†³è®¡ç®—æ•ˆç‡å’Œå†…å­˜ç“¶é¢ˆé—®é¢˜ï¼ˆåŸæ–‡ï¼šachieves cross-token feature interactions through parameter-free operatorï¼‰ã€‚</dd>
    
    <dt><b class="highlight">æ¯ä»¤ç‰Œå‰é¦ˆç½‘ç»œ (Per-token FFN)</b></dt>
    <dd>ä¸ºæ¯ä¸ªç‰¹å¾ä»¤ç‰Œåˆ†é…ç‹¬ç«‹å‚æ•°çš„å­ç½‘ç»œï¼Œé’ˆå¯¹æ€§å»ºæ¨¡ä¸åŒç‰¹å¾å­ç©ºé—´ï¼Œé˜²æ­¢ç‰¹å¾ä¸»å¯¼å¹¶æå‡æ‰©å±•æ€§ï¼ˆåŸæ–‡ï¼šallocating independent parameters for different feature subspace modelingï¼‰ã€‚</dd>
    
    <dt><b class="highlight">ç¨€ç–ä¸“å®¶æ··åˆ (Sparse MoE)</b></dt>
    <dd>åŠ¨æ€è·¯ç”±æ¶æ„ï¼šå¯¹æ¯ä¸ªè¾“å…¥ä»…æ¿€æ´»éƒ¨åˆ†ä¸“å®¶ç½‘ç»œï¼Œå®ç°å‚æ•°é‡å¢é•¿ä¸è®¡ç®—æˆæœ¬è§£è€¦ï¼ˆåŸæ–‡ï¼šdynamically activating only a subset of experts per tokenï¼‰ã€‚</dd>
    
    <dt><b class="highlight">MFU (Matrix Multiply Utilization)</b></dt>
    <dd>ç¡¬ä»¶æ•ˆç‡æŒ‡æ ‡ï¼šè¡¡é‡GPUçŸ©é˜µä¹˜æ³•å•å…ƒçš„å®é™…åˆ©ç”¨ç‡ï¼Œé€šè¿‡ä¼˜åŒ–å¯è¾¾è¿‘100%ï¼ˆåŸæ–‡ï¼šdecouple FLOPs growth from actual cost through high MFUï¼‰ã€‚</dd>
    
    <dt><b class="highlight">ç‰¹å¾ä»¤ç‰ŒåŒ– (Feature Tokenization)</b></dt>
    <dd>å°†ç”¨æˆ·ç‰¹å¾ã€è§†é¢‘IDç­‰åŸå§‹è¾“å…¥è½¬åŒ–ä¸ºç»“æ„åŒ–ä»¤ç‰Œåºåˆ—ï¼Œæ¯ä¸ªä»¤ç‰Œè¡¨ç¤ºä¸€ä¸ªè¯­ä¹‰è¿è´¯çš„ç‰¹å¾å‘é‡ï¼ˆåŸæ–‡ï¼šinput vector is tokenized into ğ‘‡ feature tokensï¼‰ã€‚</dd>
</dl>

<div class="figure">
    <b>å›¾ç¤ºï¼š</b> RankMixeræ¶æ„ç¤ºæ„å›¾ï¼ˆå¯¹åº”åŸæ–‡Figure 1ï¼‰
    <div>ã€å¤šå¤´ä»¤ç‰Œæ··åˆå±‚ã€‘â†’ã€æ¯ä»¤ç‰ŒFFNå±‚ã€‘â†’ å±‚å½’ä¸€åŒ–æ®‹å·®è¿æ¥</div>
</div>

</body>
</html>
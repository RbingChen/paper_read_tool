<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <title>è®ºæ–‡è§£ææŠ¥å‘Š</title>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style>
        body { font-family: 'Segoe UI', sans-serif; line-height: 1.6; }
        .original { background-color: #f0f0f0; border: 1px solid #ccc; padding: 15px; margin-bottom: 20px; }
        .translation { background-color: #e0ffe0; border: 1px solid #4CAF50; padding: 15px; margin-bottom: 20px; }
        .figure { background-color: #fffde7; padding: 15px; margin: 20px 0; text-align: center; }
        .term { color: red; font-weight: bold; }
        h2 { color: #2c3e50; border-bottom: 2px solid #3498db; padding-bottom: 5px; }
        table { width: 100%; border-collapse: collapse; margin: 15px 0; }
        th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
        th { background-color: #f2f2f2; }
        .formula-container { text-align: center; margin: 20px 0; }
        .formula-label { font-style: italic; margin-top: 5px; }
    </style>
</head>
<body>

<h1>è®ºæ–‡è§£ææŠ¥å‘Š</h1>

<!-- å†…å®¹ç†è§£ -->
<h2>1. å†…å®¹ç†è§£</h2>
<div>
    <p>æœ¬æ–‡èšç„¦äºRankMixeræ¨¡å‹çš„æ¶æ„è®¾è®¡ä¸æ€§èƒ½åˆ†æï¼š</p>
    <ul>
        <li><span class="term">æ‰©å±•æ€§ï¼ˆscalabilityï¼‰</span>ï¼šé€šè¿‡å¢åŠ å®½åº¦ï¼ˆ<em>D</em>ï¼‰ã€ç‰¹å¾ä»¤ç‰Œï¼ˆ<em>T</em>ï¼‰å’Œå±‚æ•°ï¼ˆ<em>L</em>ï¼‰å®ç°æ¨¡å‹æ‰©å±•ï¼ŒéªŒè¯äº†æ¨¡å‹è´¨é‡ä¸å‚æ•°æ€»é‡æ­£ç›¸å…³</li>
        <li><span class="term">æ¶ˆèç ”ç©¶ï¼ˆablation studyï¼‰</span>ï¼šç§»é™¤è·³è·ƒè¿æ¥ã€å¤šå¤´ä»¤ç‰Œæ··åˆç­‰ç»„ä»¶å¯¼è‡´AUCæ˜¾è‘—ä¸‹é™ï¼Œè¯æ˜å…¶å¿…è¦æ€§</li>
        <li><span class="term">è·¯ç”±ç­–ç•¥ï¼ˆrouting strategyï¼‰</span>ï¼šå¯¹æ¯”ä¸‰ç§ä»¤ç‰Œæ··åˆæ–¹æ¡ˆï¼Œå¤šå¤´ä»¤ç‰Œæ··åˆåœ¨æ€§èƒ½ä¸è®¡ç®—æ•ˆç‡ä¸Šæœ€ä¼˜</li>
        <li><span class="term">ç¨€ç–æ··åˆä¸“å®¶ï¼ˆSparse-MoEï¼‰</span>ï¼šé‡‡ç”¨DTSIï¼ˆå¯†é›†è®­ç»ƒ-ç¨€ç–æ¨ç†ï¼‰ä¸ReLUè·¯ç”±è§£å†³ä¸“å®¶ä¸å¹³è¡¡é—®é¢˜ï¼Œå®ç°8å€å‚æ•°æ‰©å±•</li>
    </ul>
</div>

<!-- å†…å®¹ç¿»è¯‘ -->
<h2>2. å†…å®¹ç¿»è¯‘</h2>

<div class="original">
    Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY Zhifang Fan et al.
    scalability of its cross structure. Moreover, MoEâ€™s strategy of scaling
    by adding experts brings about challenges in maintaining expert
    balance, which results in suboptimal scaling performance.
    To be Specific, We can scale up RankMixer model by increasing
    width (ğ·), feature token ( ğ‘‡) and Layer( ğ¿). In our experiments we
    observed a conclusion shared with LLM scaling laws: model qual-
    ity correlates primarily with the total number of parameters, and
    different scaling directions (depth L, width D, tokens T) yield al-
    most identical performance. From a compute-efficiency standpoint,
    while larger hidden-dim generate larger matrix-multiply shapes
    and thus achieve higher MFU than stacking more layers. So the
    final configuration for 100M and 1B are set as ( ğ·=768,ğ‘‡=16,ğ¿=2)
    and (ğ·=1536,ğ‘‡=32,ğ¿=2) respectively.
</div>
<div class="translation">
    Zhifang Fan ç­‰ï¼Œä¼šè®®ç¼©å†™ 'XXï¼Œ2018å¹´6æœˆ3-5æ—¥ï¼Œçº½çº¦å·ä¼å¾·æ–¯æ‰˜å…‹
    å…¶äº¤å‰ç»“æ„çš„å¯æ‰©å±•æ€§ã€‚æ­¤å¤–ï¼Œæ··åˆä¸“å®¶ï¼ˆMoEï¼‰é€šè¿‡å¢åŠ ä¸“å®¶å®ç°æ‰©å±•çš„ç­–ç•¥
    åœ¨ç»´æŒä¸“å®¶å¹³è¡¡æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œå¯¼è‡´æ‰©å±•æ€§èƒ½æ¬ ä½³ã€‚
    å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬å¯é€šè¿‡å¢åŠ å®½åº¦ï¼ˆğ·ï¼‰ã€ç‰¹å¾ä»¤ç‰Œï¼ˆğ‘‡ï¼‰å’Œå±‚æ•°ï¼ˆğ¿ï¼‰æ‰©å±•RankMixeræ¨¡å‹ã€‚
    å®éªŒä¸­è§‚å¯Ÿåˆ°ä¸LLMæ‰©å±•å®šå¾‹ä¸€è‡´çš„ç»“è®ºï¼šæ¨¡å‹è´¨é‡ä¸»è¦ä¸å‚æ•°æ€»é‡ç›¸å…³ï¼Œ
    ä¸åŒæ‰©å±•æ–¹å‘ï¼ˆæ·±åº¦Lã€å®½åº¦Dã€ä»¤ç‰Œæ•°Tï¼‰äº§ç”Ÿå‡ ä¹ç›¸åŒçš„æ€§èƒ½ã€‚ä»è®¡ç®—æ•ˆç‡è§’åº¦çœ‹ï¼Œ
    å¢å¤§éšè—ç»´åº¦ä¼šäº§ç”Ÿæ›´å¤§çš„çŸ©é˜µä¹˜æ³•å½¢çŠ¶ï¼Œä»è€Œæ¯”å †å æ›´å¤šå±‚è·å¾—æ›´é«˜çš„MFUï¼ˆæ¨¡å‹æµ®ç‚¹åˆ©ç”¨ç‡ï¼‰ã€‚
    å› æ­¤æœ€ç»ˆ100Må’Œ1Bå‚æ•°æ¨¡å‹çš„é…ç½®åˆ†åˆ«ä¸ºï¼ˆğ·=768, ğ‘‡=16, ğ¿=2ï¼‰
    å’Œï¼ˆğ·=1536, ğ‘‡=32, ğ¿=2ï¼‰ã€‚
</div>

<h3>4.4 æ¶ˆèç ”ç©¶</h3>
<div class="original">
    Table 2: Ablation on components of RankMixer-100M
    Setting Î”AUC
    w/o skip connections âˆ’0.07%
    w/o multi-head token mixing âˆ’0.50%
    w/o layer normalization âˆ’0.05%
    Per-token FFNâ†’shared FFNâˆ’0.31%
</div>
<div class="translation">
    <strong>è¡¨2ï¼šRankMixer-100Mç»„ä»¶æ¶ˆèç ”ç©¶</strong>
    <table>
        <tr><th>è®¾ç½®</th><th>Î”AUC</th></tr>
        <tr><td>æ— è·³è·ƒè¿æ¥</td><td>âˆ’0.07%</td></tr>
        <tr><td>æ— å¤šå¤´ä»¤ç‰Œæ··åˆ</td><td>âˆ’0.50%</td></tr>
        <tr><td>æ— å±‚å½’ä¸€åŒ–</td><td>âˆ’0.05%</td></tr>
        <tr><td>é€ä»¤ç‰ŒFFNâ†’å…±äº«FFN</td><td>âˆ’0.31%</td></tr>
    </table>
</div>

<div class="original">
    Table 3: Token2FFN Routingâ€“strategy comparison
    Routing strategy Î”AUC Î”Params Î”FLOPs
    All-Concat-MLP âˆ’0.18% 0.0% 0.0%
    All-Share âˆ’0.25% 0.0% 0.0%
    Self-Attention âˆ’0.03% +16% +71.8%
</div>
<div class="translation">
    <strong>è¡¨3ï¼šä»¤ç‰Œåˆ°FFNçš„è·¯ç”±ç­–ç•¥æ¯”è¾ƒ</strong>
    <table>
        <tr><th>è·¯ç”±ç­–ç•¥</th><th>Î”AUC</th><th>Î”Params</th><th>Î”FLOPs</th></tr>
        <tr><td>å…¨è¿æ¥MLP</td><td>âˆ’0.18%</td><td>0.0%</td><td>0.0%</td></tr>
        <tr><td>å…¨å…±äº«</td><td>âˆ’0.25%</td><td>0.0%</td><td>0.0%</td></tr>
        <tr><td>è‡ªæ³¨æ„åŠ›</td><td>âˆ’0.03%</td><td>+16%</td><td>+71.8%</td></tr>
    </table>
</div>

<div class="original">
    In the RankMixer-100M model, we performed ablation studies on
    residual connections, Multi-Head Token-Mixing. From Table 2, we
    can see that removing these components significantly decreased the
    modelâ€™s performance. Removing Multi-Head Token-Mixing loses
    global information, as each FFN only models partial features with-
    out interaction. Removing residual connections and LayerNorm
    also worsens performance, reducing training stability and making
    gradient explosion or vanishing issues more likely.
</div>
<div class="translation">
    åœ¨RankMixer-100Mæ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬å¯¹æ®‹å·®è¿æ¥å’Œ<span class="term">å¤šå¤´ä»¤ç‰Œæ··åˆï¼ˆMulti-Head Token-Mixingï¼‰</span>è¿›è¡Œæ¶ˆèç ”ç©¶ã€‚
    ä»è¡¨2å¯è§ï¼Œç§»é™¤è¿™äº›ç»„ä»¶æ˜¾è‘—é™ä½æ¨¡å‹æ€§èƒ½ã€‚ç§»é™¤å¤šå¤´ä»¤ç‰Œæ··åˆä¼šä¸¢å¤±å…¨å±€ä¿¡æ¯ï¼Œ
    å› ä¸ºæ¯ä¸ªFFNä»…å»ºæ¨¡éƒ¨åˆ†ç‰¹å¾ä¸”æ— äº¤äº’ã€‚ç§»é™¤æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–åŒæ ·æŸå®³æ€§èƒ½ï¼Œ
    é™ä½è®­ç»ƒç¨³å®šæ€§å¹¶å¢åŠ æ¢¯åº¦çˆ†ç‚¸/æ¶ˆå¤±é£é™©ã€‚
</div>

<div class="original">
    We further analyzed the token mixing strategies, i.e., the routing
    strategies from feature tokens to FFNs in the Table 3. The routing
    strategies compared with Multi-Head Token Mixing (Multi-Head
    Token-Mixing) include: All-Concat-MLP: Concatenates all tokens
    and processes them through a large MLP before splitting them
    into the same number of tokens. The decrease of performance
    shows the challenges in learning large matrices and weakening
    local information learning. All-Share: No splitting, the entire input
    vector is shared and fed to each per-token FFN similar as MoE. The
    performane declines siginifantly which show the importance of
    feature subspace split and independent modeling contrast to all-
    shared input. Self-Attention: Applies self-attention between tokens
    for routing. Its performance is slightly inferior to the Multi-Head
    Token-Mixing and also suffers from high computational cost, which
    shows the difficulty of learning similarity across hundreds of differ-
    ent feature subspaces.
</div>
<div class="translation">
    æˆ‘ä»¬è¿›ä¸€æ­¥åˆ†æä»¤ç‰Œæ··åˆç­–ç•¥ï¼ˆå³è¡¨3ä¸­ç‰¹å¾ä»¤ç‰Œåˆ°FFNçš„è·¯ç”±ç­–ç•¥ï¼‰ã€‚ä¸<span class="term">å¤šå¤´ä»¤ç‰Œæ··åˆ</span>å¯¹æ¯”çš„ç­–ç•¥åŒ…æ‹¬ï¼š
    <strong>å…¨è¿æ¥MLP</strong>ï¼šæ‹¼æ¥æ‰€æœ‰ä»¤ç‰Œå¹¶é€šè¿‡å¤§å‹MLPå¤„ç†åå†åˆ†å‰²ï¼Œæ€§èƒ½ä¸‹é™è¡¨æ˜å­¦ä¹ å¤§çŸ©é˜µçš„æŒ‘æˆ˜åŠå±€éƒ¨ä¿¡æ¯å¼±åŒ–ï¼›
    <strong>å…¨å…±äº«</strong>ï¼šä¸åˆ†å‰²è¾“å…¥å‘é‡ï¼Œæ•´ä¸ªå‘é‡å…±äº«è‡³æ¯ä¸ªFFNï¼ˆç±»ä¼¼MoEï¼‰ï¼Œæ€§èƒ½æ˜¾è‘—ä¸‹é™è¯æ˜ç‰¹å¾å­ç©ºé—´åˆ†å‰²ä¸ç‹¬ç«‹å»ºæ¨¡çš„é‡è¦æ€§ï¼›
    <strong>è‡ªæ³¨æ„åŠ›</strong>ï¼šåœ¨ä»¤ç‰Œé—´åº”ç”¨è‡ªæ³¨æ„åŠ›è·¯ç”±ï¼Œæ€§èƒ½ç•¥ä½äºå¤šå¤´ä»¤ç‰Œæ··åˆä¸”è®¡ç®—æˆæœ¬é«˜ï¼Œè¡¨æ˜è·¨æ•°ç™¾ç‰¹å¾å­ç©ºé—´å­¦ä¹ ç›¸ä¼¼æ€§çš„å›°éš¾ã€‚
</div>

<div class="figure">
    <strong>å›¾3ï¼š</strong>ä¸åŒä¸“å®¶æ¿€æ´»ç¨€ç–åº¦ä¸‹RankMixerå˜ä½“çš„AUCæ€§èƒ½ï¼ˆ1,1/2,1/4,1/8ï¼‰<br>
    <em>å¯†é›†è®­ç»ƒ+ReLUè·¯ç”±çš„SMoEå‡ ä¹ä¿ç•™1Bå¯†é›†æ¨¡å‹å…¨éƒ¨ç²¾åº¦</em>
</div>

<div class="figure">
    <strong>å›¾4ï¼š</strong>RankMixerä¸­ä¸åŒä»¤ç‰Œçš„ä¸“å®¶æ¿€æ´»æ¯”ä¾‹
</div>

<h3>4.5 ç¨€ç–MoEçš„å¯æ‰©å±•æ€§ä¸ä¸“å®¶å¹³è¡¡</h3>
<div class="original">
    Scalability. Figure 3 plots offline AUC gains versus Sparsity of
    SMoE. Combining Dense-Training-Sparse-Inference with ReLU
    routing is essential for preserving accuracy under aggressive spar-
    sity, enabling RankMixer to scale parameter capacity (and memory
    footprint) by > 8 Ã—with nearly no loss in AUC and with substan-
    tial inference-time savings (+50% improvement over throughput).
    Vanilla SMoEâ€™s performance drops monotonically as fewer experts
    are activated, illustrating the expert-imbalance and under-training
    issues we identified. Adding a load-balancing loss reduces the degra-
    dation relative to vanilla SMoE, yet still falls short of the DTSI +
    ReLU version because the problems mostly lies in the expert train-
    ing instead of the router. This validates Sparse-MoE as the path to
    scale RankMixer from the current 1 B parameters to future 10 B-
    scale deployments without breaching the cost budgets.
</div>
<div class="translation">
    <strong>å¯æ‰©å±•æ€§</strong>ï¼šå›¾3ç»˜åˆ¶äº†ç¦»çº¿AUCå¢ç›Šä¸<span class="term">ç¨€ç–æ··åˆä¸“å®¶ï¼ˆSparse-MoEï¼‰</span>ç¨€ç–åº¦çš„å…³ç³»ã€‚<span class="term">å¯†é›†è®­ç»ƒ-ç¨€ç–æ¨ç†ï¼ˆDTSIï¼‰</span>
    ç»“åˆ<span class="term">ReLUè·¯ç”±</span>å¯¹åœ¨æ¿€è¿›ç¨€ç–åº¦ä¸‹ä¿æŒç²¾åº¦è‡³å…³é‡è¦ï¼Œä½¿RankMixerå‚æ•°å®¹é‡ï¼ˆåŠå†…å­˜å ç”¨ï¼‰æ‰©å±•>8å€ï¼Œ
    å‡ ä¹æ— AUCæŸå¤±ä¸”æ¨ç†æ—¶é—´å¤§å¹…èŠ‚çœï¼ˆååé‡æå‡50%ï¼‰ã€‚åŸå§‹SMoEæ€§èƒ½éšä¸“å®¶æ¿€æ´»æ•°å‡å°‘å•è°ƒä¸‹é™ï¼Œ
    è¡¨æ˜ä¸“å®¶ä¸å¹³è¡¡å’Œè®­ç»ƒä¸è¶³é—®é¢˜ã€‚æ·»åŠ è´Ÿè½½å‡è¡¡æŸå¤±å¯ç¼“è§£æ€§èƒ½ä¸‹é™ï¼Œä½†ä»ä¸åŠDTSI+ReLUæ–¹æ¡ˆï¼Œ
    å› é—®é¢˜ä¸»è¦åœ¨ä¸“å®¶è®­ç»ƒè€Œéè·¯ç”±å™¨ã€‚è¿™éªŒè¯äº†ç¨€ç–MoEæ˜¯å°†RankMixerä»1Bæ‰©å±•åˆ°10Bè§„æ¨¡çš„ç»æµè·¯å¾„ã€‚
</div>

<div class="original">
    Expert balance and diversity. Vanilla Sparse MoE often suffers
    from expert imbalance, which in turn leaves some experts under-
    trained and eventually leads to â€œdying expertsâ€ (experts that are
    almost never activated) and only a few fixed experts are constantly
    activated. Figure 4 shows that combining DTSI (dense-training,
    sparse-inference) with ReLU routing effectively resolves this issue:
</div>
<div class="translation">
    <strong>ä¸“å®¶å¹³è¡¡ä¸å¤šæ ·æ€§</strong>ï¼šåŸå§‹ç¨€ç–MoEå¸¸å—<span class="term">ä¸“å®¶ä¸å¹³è¡¡ï¼ˆexpert imbalanceï¼‰</span>å½±å“ï¼Œå¯¼è‡´éƒ¨åˆ†ä¸“å®¶è®­ç»ƒä¸è¶³ï¼Œ
    æœ€ç»ˆäº§ç”Ÿâ€œæ­»äº¡ä¸“å®¶â€ï¼ˆå‡ ä¹ä¸è¢«æ¿€æ´»ï¼‰ï¼Œä»…å°‘æ•°å›ºå®šä¸“å®¶æŒç»­æ¿€æ´»ã€‚å›¾4æ˜¾ç¤ºDTSIï¼ˆå¯†é›†è®­ç»ƒ-ç¨€ç–æ¨ç†ï¼‰
    ç»“åˆReLUè·¯ç”±æœ‰æ•ˆè§£å†³æ­¤é—®é¢˜ã€‚
</div>

<!-- æ‘˜è¦æ€»ç»“ -->
<h2>3. æ‘˜è¦æ€»ç»“</h2>
<div>
    <p>æœ¬æ–‡æ ¸å¿ƒç ”ç©¶RankMixeræ¨¡å‹çš„æ‰©å±•æ€§ä¸ä¼˜åŒ–ç­–ç•¥ï¼š</p>
    <ol>
        <li><strong>æ‰©å±•å®šå¾‹éªŒè¯</strong>ï¼šæ¨¡å‹è´¨é‡ä¸å‚æ•°æ€»é‡å¼ºç›¸å…³ï¼Œå¢åŠ å®½åº¦ï¼ˆ<em>D</em>ï¼‰æ¯”æ·±åº¦ï¼ˆ<em>L</em>ï¼‰æ›´å…·è®¡ç®—æ•ˆç‡</li>
        <li><strong>å…³é”®ç»„ä»¶éªŒè¯</strong>ï¼šæ¶ˆèå®éªŒè¯æ˜è·³è·ƒè¿æ¥ã€å¤šå¤´ä»¤ç‰Œæ··åˆå’Œå±‚å½’ä¸€åŒ–å¯¹æ€§èƒ½è‡³å…³é‡è¦ï¼ˆAUCä¸‹é™0.05%-0.50%ï¼‰</li>
        <li><strong>è·¯ç”±ç­–ç•¥ä¼˜åŒ–</strong>ï¼šå¤šå¤´ä»¤ç‰Œæ··åˆåœ¨æ€§èƒ½ä¸è®¡ç®—æ•ˆç‡ä¸Šä¼˜äºå…¨è¿æ¥MLPï¼ˆ-0.18% AUCï¼‰ã€å…¨å…±äº«ï¼ˆ-0.25%ï¼‰å’Œè‡ªæ³¨æ„åŠ›ï¼ˆ+71.8% FLOPsï¼‰</li>
        <li><strong>ç¨€ç–MoEçªç ´</strong>ï¼šDTSI+ReLUè·¯ç”±å®ç°>8å€å‚æ•°æ‰©å±•ï¼Œè§£å†³ä¸“å®¶ä¸å¹³è¡¡é—®é¢˜ï¼Œæ¨ç†é€Ÿåº¦æå‡50%</li>
    </ol>
    <p>æœ€ç»ˆæ–¹æ¡ˆæ”¯æŒRankMixerä»1Bå‚æ•°å‘10Bè§„æ¨¡ç»æµæ‰©å±•ã€‚</p>
</div>

<!-- æœ¯è¯­è¯†åˆ« -->
<h2>4. æœ¯è¯­è§£é‡Š</h2>
<div>
    <ul>
        <li><span class="term">å¤šå¤´ä»¤ç‰Œæ··åˆï¼ˆMulti-Head Token-Mixingï¼‰</span>ï¼šå°†ç‰¹å¾ä»¤ç‰Œåˆ†å‰²è‡³ç‹¬ç«‹å­ç©ºé—´ï¼Œç”±ä¸åŒFFNå¹¶è¡Œå¤„ç†åå†åˆå¹¶çš„æœºåˆ¶ï¼Œç”¨äºæ•è·å…¨å±€ç‰¹å¾äº¤äº’</li>
        <li><span class="term">DTSIï¼ˆDense-Training-Sparse-Inferenceï¼‰</span>ï¼šè®­ç»ƒæ—¶æ¿€æ´»æ‰€æœ‰ä¸“å®¶ï¼Œæ¨ç†æ—¶ä»…æ¿€æ´»éƒ¨åˆ†ä¸“å®¶çš„æ··åˆä¸“å®¶è®­ç»ƒèŒƒå¼</li>
        <li><span class="term">ReLUè·¯ç”±ï¼ˆReLU routingï¼‰</span>ï¼šä½¿ç”¨ReLUæ¿€æ´»å‡½æ•°å®ç°ä¸“å®¶é€‰æ‹©çš„ç¨€ç–è·¯ç”±æœºåˆ¶ï¼Œå…¬å¼è¡¨è¾¾ä¸ºï¼š
            <div class="formula-container">
                \( \\text{Routing} = \\text{ReLU}(W \\cdot X + b) \)
                <div class="formula-label">(å…¬å¼1: è·¯ç”±è®¡ç®—)</div>
            </div>
        </li>
        <li><span class="term">ä¸“å®¶ä¸å¹³è¡¡ï¼ˆExpert Imbalanceï¼‰</span>ï¼šç¨€ç–MoEä¸­éƒ¨åˆ†ä¸“å®¶è¢«è¿‡åº¦æ¿€æ´»è€Œå…¶ä»–ä¸“å®¶é—²ç½®çš„ç°è±¡ï¼Œå¯¼è‡´æ¨¡å‹å®¹é‡åˆ©ç”¨ç‡ä½ä¸‹</li>
        <li><span class="term">MFUï¼ˆModel FLOPs Utilizationï¼‰</span>ï¼šæ¨¡å‹æµ®ç‚¹è¿ç®—åˆ©ç”¨ç‡ï¼Œè¡¡é‡ç¡¬ä»¶è®¡ç®—æ•ˆç‡çš„æ ¸å¿ƒæŒ‡æ ‡</li>
        <li><span class="term">SMoEï¼ˆSparse Mixture of Expertsï¼‰</span>ï¼šåŸºäºç¨€ç–æ¿€æ´»çš„æ··åˆä¸“å®¶æ¶æ„ï¼Œæ¯ä¸ªè¾“å…¥ä»…æ¿€æ´»éƒ¨åˆ†ä¸“å®¶å­é›†</li>
    </ul>
</div>

</body>
</html>
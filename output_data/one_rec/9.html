<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <title>OneRec Technical Report Analysis</title>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <style>
    body { font-family: Arial, sans-serif; line-height: 1.6; }
    .original { background-color: #f0f0f0; border: 1px solid #999; padding: 15px; margin-bottom: 20px; border-radius: 5px; }
    .translation { background-color: #e0ffe0; border: 1px solid #2ecc71; padding: 15px; margin-bottom: 30px; border-radius: 5px; }
    .formula-container { background-color: #fffde7; padding: 15px; margin: 20px 0; text-align: center; border-left: 4px solid #f1c40f; }
    .formula-number { font-style: italic; margin-top: 5px; }
    .term { color: red; font-weight: bold; }
    h1, h2, h3 { color: #2c3e50; }
    .section { margin-bottom: 40px; }
    ul { list-style-type: square; padding-left: 20px; }
  </style>
</head>
<body>
  <h1>OneRecæŠ€æœ¯æŠ¥å‘Šè§£æ</h1>

  <!-- å†…å®¹ç†è§£ -->
  <div class="section">
    <h2>å†…å®¹ç†è§£</h2>
    <p>è¯¥æŠ€æœ¯æŠ¥å‘Šè¯¦ç»†æè¿°äº†OneRecæ¨èç³»ç»Ÿçš„æ ¸å¿ƒæ¶æ„ï¼š</p>
    <ol>
      <li><span class="term">è§£ç å™¨(Decoder)</span>é‡‡ç”¨å¤šå±‚Transformerç»“æ„ï¼Œæ¯å±‚åŒ…å«ä¸‰ä¸ªæ ¸å¿ƒæ“ä½œï¼š<span class="term">å› æœè‡ªæ³¨æ„åŠ›(CausalSelfAttn)</span>ã€<span class="term">äº¤å‰æ³¨æ„åŠ›(CrossAttn)</span>å’Œ<span class="term">æ··åˆä¸“å®¶ç½‘ç»œ(MoE)</span></li>
      <li><span class="term">æ··åˆä¸“å®¶ç½‘ç»œ(MoE)</span>ä½¿ç”¨top-kè·¯ç”±ç­–ç•¥åŠ¨æ€é€‰æ‹©ä¸“å®¶ï¼Œå¹¶é€šè¿‡<span class="term">æ— æŸå¤±è´Ÿè½½å‡è¡¡ç­–ç•¥(Loss-free Load Balancing)</span>ä¼˜åŒ–èµ„æºåˆ†é…</li>
      <li>æ¨¡å‹è®­ç»ƒä½¿ç”¨<span class="term">äº¤å‰ç†µæŸå¤±(Cross-entropy Loss)</span>è¿›è¡Œ<span class="term">ä¸‹ä¸€ä»¤ç‰Œé¢„æµ‹(Next-token Prediction)</span></li>
      <li>å¼•å…¥<span class="term">å¥–åŠ±ç³»ç»Ÿ(Reward System)</span>è§£å†³ä¼ ç»Ÿæ¨èå±€é™ï¼š
        <ul>
          <li><span class="term">ç”¨æˆ·åå¥½å¯¹é½(User Preference Alignment)</span>ï¼šé€šè¿‡<span class="term">P-Score</span>æ¨¡å‹å­¦ä¹ ä¸ªæ€§åŒ–ç›®æ ‡èåˆ</li>
          <li>æ ¼å¼å¥–åŠ±ç¡®ä¿è¾“å‡ºç»“æ„åˆè§„</li>
          <li>å·¥ä¸šåœºæ™¯å¥–åŠ±æ»¡è¶³ç‰¹å®šéœ€æ±‚</li>
        </ul>
      </li>
    </ol>
  </div>

  <!-- å†…å®¹ç¿»è¯‘ -->
  <div class="section">
    <h2>å†…å®¹ç¿»è¯‘</h2>
    
    <div class="original">
      <p>The decoder processes this sequence through ğ¿<sub>dec</sub> transformer layers. Each layer performs sequential operations:</p>
      <div class="formula-container">
        \\( \\mathbf{d}^{(i+1)}_m = \\mathbf{d}^{(i)}_m + \\text{CausalSelfAttn}(\\mathbf{d}^{(i)}_m) \\)
        <div class="formula-number">(24)</div>
      </div>
      <div class="formula-container">
        \\( \\mathbf{d}^{(i+1)}_m = \\mathbf{d}^{(i+1)}_m + \\text{CrossAttn}(\\mathbf{d}^{(i+1)}_m, \\mathbf{Z}_{enc}, \\mathbf{Z}_{enc}) \\)
        <div class="formula-number">(25)</div>
      </div>
      <div class="formula-container">
        \\( \\mathbf{d}^{(i+1)}_m = \\mathbf{d}^{(i+1)}_m + \\text{MoE}(\\text{RMSNorm}(\\mathbf{d}^{(i+1)}_m)) \\)
        <div class="formula-number">(26)</div>
      </div>
      <p>Each decoder layer incorporates a <span class="term">Mixture of Experts (MoE)</span> feed-forward network to enhance model capacity while maintaining computational efficiency. The MoE layer employs ğ‘<sub>experts</sub> expert networks with a <span class="term">top-ğ‘˜ routing strategy</span>:</p>
      <div class="formula-container">
        \\( \\text{MoE}(\\mathbf{x}) = \\sum_{j=1}^k \\text{Gate}_j(\\mathbf{x}) \\cdot \\text{Expert}_j(\\mathbf{x}) \\)
        <div class="formula-number">(27)</div>
      </div>
      <p>where <span class="term">Gate<sub>ğ‘—</sub>(x)</span> represents the gating weights determined by the routing mechanism, and <span class="term">Expert<sub>ğ‘—</sub>(x)</span> denotes the output of the ğ‘—-th selected expert network. To ensure balanced expert utilization without introducing interference gradients, we implement a <span class="term">loss-free load balancing strategy</span> following (Liu et al., 2024).</p>
    </div>
    <div class="translation">
      <p>è§£ç å™¨é€šè¿‡ğ¿<sub>dec</sub>å±‚<span class="term">Transformer</span>å¤„ç†è¯¥åºåˆ—ã€‚æ¯å±‚æ‰§è¡Œé¡ºåºæ“ä½œï¼š</p>
      <div class="formula-container">
        \\( \\mathbf{d}^{(i+1)}_m = \\mathbf{d}^{(i)}_m + \\text{CausalSelfAttn}(\\mathbf{d}^{(i)}_m) \\)
        <div class="formula-number">(24)</div>
      </div>
      <div class="formula-container">
        \\( \\mathbf{d}^{(i+1)}_m = \\mathbf{d}^{(i+1)}_m + \\text{CrossAttn}(\\mathbf{d}^{(i+1)}_m, \\mathbf{Z}_{enc}, \\mathbf{Z}_{enc}) \\)
        <div class="formula-number">(25)</div>
      </div>
      <div class="formula-container">
        \\( \\mathbf{d}^{(i+1)}_m = \\mathbf{d}^{(i+1)}_m + \\text{MoE}(\\text{RMSNorm}(\\mathbf{d}^{(i+1)}_m)) \\)
        <div class="formula-number">(26)</div>
      </div>
      <p>æ¯ä¸ªè§£ç å™¨å±‚åŒ…å«<span class="term">æ··åˆä¸“å®¶ç½‘ç»œ(MoE)</span>å‰é¦ˆç½‘ç»œï¼Œåœ¨ä¿æŒè®¡ç®—æ•ˆç‡çš„åŒæ—¶å¢å¼ºæ¨¡å‹å®¹é‡ã€‚MoEå±‚é‡‡ç”¨ğ‘<sub>experts</sub>ä¸ªä¸“å®¶ç½‘ç»œå’Œ<span class="term">top-ğ‘˜è·¯ç”±ç­–ç•¥</span>ï¼š</p>
      <div class="formula-container">
        \\( \\text{MoE}(\\mathbf{x}) = \\sum_{j=1}^k \\text{Gate}_j(\\mathbf{x}) \\cdot \\text{Expert}_j(\\mathbf{x}) \\)
        <div class="formula-number">(27)</div>
      </div>
      <p>å…¶ä¸­<span class="term">Gate<sub>ğ‘—</sub>(x)</span>è¡¨ç¤ºè·¯ç”±æœºåˆ¶ç¡®å®šçš„é—¨æ§æƒé‡ï¼Œ<span class="term">Expert<sub>ğ‘—</sub>(x)</span>è¡¨ç¤ºç¬¬ğ‘—ä¸ªé€‰å®šä¸“å®¶ç½‘ç»œçš„è¾“å‡ºã€‚ä¸ºç¡®ä¿ä¸“å®¶å‡è¡¡åˆ©ç”¨ä¸”ä¸å¼•å…¥å¹²æ‰°æ¢¯åº¦ï¼Œæˆ‘ä»¬é‡‡ç”¨(Liu et al., 2024)æå‡ºçš„<span class="term">æ— æŸå¤±è´Ÿè½½å‡è¡¡ç­–ç•¥</span>ã€‚</p>
    </div>

    <div class="original">
      <p>The model is trained using <span class="term">cross-entropy loss</span> for <span class="term">next-token prediction</span> on the semantic identifiers of target video ğ‘š:</p>
      <div class="formula-container">
        \\( \\mathcal{L}_{\\text{NTP}} = -\\sum_{j=1}^{L_t-1} \\log P\\left( s_{j+1}^m | \\langle s_{\\text{[BOS]}}, s_1^m, s_2^m, \\cdots, s_j^m \\rangle \\right) \\)
        <div class="formula-number">(28)</div>
      </div>
    </div>
    <div class="translation">
      <p>æ¨¡å‹ä½¿ç”¨<span class="term">äº¤å‰ç†µæŸå¤±(Cross-entropy Loss)</span>å¯¹ç›®æ ‡è§†é¢‘ğ‘šçš„è¯­ä¹‰æ ‡è¯†ç¬¦è¿›è¡Œ<span class="term">ä¸‹ä¸€ä»¤ç‰Œé¢„æµ‹(Next-token Prediction)</span>ï¼š</p>
      <div class="formula-container">
        \\( \\mathcal{L}_{\\text{NTP}} = -\\sum_{j=1}^{L_t-1} \\log P\\left( s_{j+1}^m | \\langle s_{\\text{[BOS]}}, s_1^m, s_2^m, \\cdots, s_j^m \\rangle \\right) \\)
        <div class="formula-number">(28)</div>
      </div>
    </div>

    <div class="original">
      <h3>2.4. Reward System</h3>
      <p>The pre-trained model only fits the distribution of the exposed item space through next token prediction, and the exposed items are obtained from the past traditional recommendation system. This results in the model being unable to break through the ceiling of traditional recommendations. To address this issue, we introduce <span class="term">preference alignment</span> based on a <span class="term">reward system</span>, using <span class="term">on-policy reinforcement learning</span> to train the model in the generated item space. Through rewards, the model perceives more fine-grained preference information. We introduce the preference reward to align user preferences, the format reward to ensure the generation format is as legal as possible, and the specific industrial reward to align with some special industrial scenario needs.</p>
    </div>
    <div class="translation">
      <h3>2.4. å¥–åŠ±ç³»ç»Ÿ</h3>
      <p>é¢„è®­ç»ƒæ¨¡å‹ä»…é€šè¿‡ä¸‹ä¸€ä»¤ç‰Œé¢„æµ‹æ‹Ÿåˆæ›å…‰ç‰©å“ç©ºé—´çš„åˆ†å¸ƒï¼Œè€Œæ›å…‰ç‰©å“æ¥è‡ªè¿‡å»çš„ä¼ ç»Ÿæ¨èç³»ç»Ÿã€‚è¿™å¯¼è‡´æ¨¡å‹æ— æ³•çªç ´ä¼ ç»Ÿæ¨èçš„ä¸Šé™ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥åŸºäº<span class="term">å¥–åŠ±ç³»ç»Ÿ(Reward System)</span>çš„<span class="term">åå¥½å¯¹é½(Preference Alignment)</span>ï¼Œä½¿ç”¨<span class="term">åœ¨çº¿ç­–ç•¥å¼ºåŒ–å­¦ä¹ (On-policy Reinforcement Learning)</span>åœ¨ç”Ÿæˆç‰©å“ç©ºé—´ä¸­è®­ç»ƒæ¨¡å‹ã€‚é€šè¿‡å¥–åŠ±ï¼Œæ¨¡å‹æ„ŸçŸ¥æ›´ç»†ç²’åº¦çš„åå¥½ä¿¡æ¯ã€‚æˆ‘ä»¬å¼•å…¥åå¥½å¥–åŠ±å¯¹é½ç”¨æˆ·åå¥½ï¼Œæ ¼å¼å¥–åŠ±ç¡®ä¿ç”Ÿæˆæ ¼å¼å°½å¯èƒ½åˆæ³•ï¼Œå·¥ä¸šåœºæ™¯å¥–åŠ±æ»¡è¶³ç‰¹æ®Šå·¥ä¸šéœ€æ±‚ã€‚</p>
    </div>

    <div class="original">
      <h3>2.4.1. User Preference Alignment</h3>
      <p>In recommendation systems, defining a "good recommendation" is much more challenging than determining the correctness of a mathematical solution. Traditional approaches (Chang et al., 2023; Wang et al., 2024) often define multiple objectives, such as clicks, likes, comments, and watch time, which are then combined into a score through a weighted fusion of the predicted values (x<sub>tr</sub>) for each objective. However, manually tuning these fusion weights is challenging, not only lacking accuracy but also lacking personalization, and often results in optimization conflicts between objectives.</p>
      <p>To address these limitations, we propose using a neural network to learn a personalized fusion score, referred to as <span class="term">P-Score (Preference Score)</span> (Cao et al., 2025). The overall framework of this model is illustrated in Figure 5 (middle). The modelâ€™s underlying architecture is based on the <span class="term">Search-based Interest Model (SIM)</span> (Pi et al., 2020). It includes multiple towers, each dedicated to learning specific objectives. During training, these towers compute <span class="term">binary cross-entropy (BCE) loss</span> using the corresponding objective labels as auxiliary tasks. The hidden states of each tower, along with user features, are fused to produce the final P-Score.</p>
    </div>
    <div class="translation">
      <h3>2.4.1. ç”¨æˆ·åå¥½å¯¹é½</h3>
      <p>åœ¨æ¨èç³»ç»Ÿä¸­ï¼Œå®šä¹‰"å¥½çš„æ¨è"æ¯”ç¡®å®šæ•°å­¦è§£çš„æ­£ç¡®æ€§æ›´å…·æŒ‘æˆ˜æ€§ã€‚ä¼ ç»Ÿæ–¹æ³•(Chang et al., 2023; Wang et al., 2024)é€šå¸¸å®šä¹‰å¤šä¸ªç›®æ ‡ï¼ˆå¦‚ç‚¹å‡»ã€ç‚¹èµã€è¯„è®ºå’Œè§‚çœ‹æ—¶é•¿ï¼‰ï¼Œç„¶åé€šè¿‡å„ç›®æ ‡é¢„æµ‹å€¼(x<sub>tr</sub>)çš„åŠ æƒèåˆåˆå¹¶ä¸ºåˆ†æ•°ã€‚ä½†æ‰‹åŠ¨è°ƒæ•´èåˆæƒé‡å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œä¸ä»…ç¼ºä¹å‡†ç¡®æ€§ä¸”ç¼ºä¹ä¸ªæ€§åŒ–ï¼Œå¸¸å¯¼è‡´ç›®æ ‡é—´ä¼˜åŒ–å†²çªã€‚</p>
      <p>ä¸ºå…‹æœè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºä½¿ç”¨ç¥ç»ç½‘ç»œå­¦ä¹ ä¸ªæ€§åŒ–èåˆåˆ†æ•°â€”â€”<span class="term">P-Score(Preference Score)</span>(Cao et al., 2025)ã€‚è¯¥æ¨¡å‹æ•´ä½“æ¡†æ¶å¦‚å›¾5ï¼ˆä¸­ï¼‰æ‰€ç¤ºï¼Œåº•å±‚æ¶æ„åŸºäº<span class="term">åŸºäºæœç´¢çš„å…´è¶£æ¨¡å‹(SIM)</span>(Pi et al., 2020)ã€‚æ¨¡å‹åŒ…å«å¤šä¸ªå¡”å¼ç»“æ„ï¼Œæ¯ä¸ªå¡”ä¸“ç”¨äºå­¦ä¹ ç‰¹å®šç›®æ ‡ã€‚è®­ç»ƒæ—¶ï¼Œè¿™äº›å¡”ä½¿ç”¨å¯¹åº”ç›®æ ‡æ ‡ç­¾ä½œä¸ºè¾…åŠ©ä»»åŠ¡è®¡ç®—<span class="term">äºŒå…ƒäº¤å‰ç†µæŸå¤±(BCE Loss)</span>ã€‚å„å¡”çš„éšè—çŠ¶æ€ä¸ç”¨æˆ·ç‰¹å¾èåˆç”Ÿæˆæœ€ç»ˆP-Scoreã€‚</p>
    </div>
  </div>

  <!-- æ‘˜è¦æ€»ç»“ -->
  <div class="section">
    <h2>æ‘˜è¦æ€»ç»“</h2>
    <p>OneRecæŠ€æœ¯æŠ¥å‘Šçš„æ ¸å¿ƒå†…å®¹åŒ…æ‹¬ï¼š</p>
    <ol>
      <li><strong>è§£ç å™¨æ¶æ„</strong>ï¼šé‡‡ç”¨å¤šå±‚Transformerï¼Œæ¯å±‚åŒ…å«<span class="term">å› æœè‡ªæ³¨æ„åŠ›(24)</span>ã€<span class="term">äº¤å‰æ³¨æ„åŠ›(25)</span>å’Œ<span class="term">æ··åˆä¸“å®¶ç½‘ç»œ(MoE)(26)</span>ä¸‰ä¸ªé¡ºåºæ“ä½œ</li>
      <li><strong>MoEæœºåˆ¶</strong>ï¼šé€šè¿‡<span class="term">top-kè·¯ç”±ç­–ç•¥(27)</span>åŠ¨æ€é€‰æ‹©ä¸“å®¶ï¼Œç»“åˆ<span class="term">æ— æŸå¤±è´Ÿè½½å‡è¡¡</span>ä¼˜åŒ–è®¡ç®—æ•ˆç‡</li>
      <li><strong>è®­ç»ƒç›®æ ‡</strong>ï¼šä½¿ç”¨<span class="term">äº¤å‰ç†µæŸå¤±(28)</span>è¿›è¡Œ<span class="term">ä¸‹ä¸€ä»¤ç‰Œé¢„æµ‹</span></li>
      <li><strong>å¥–åŠ±ç³»ç»Ÿåˆ›æ–°</strong>ï¼š
        <ul>
          <li>çªç ´ä¼ ç»Ÿæ¨èå±€é™çš„<span class="term">åå¥½å¯¹é½æ¡†æ¶</span></li>
          <li><span class="term">P-Scoreæ¨¡å‹</span>è§£å†³å¤šç›®æ ‡èåˆé—®é¢˜</li>
          <li>ä¸‰é˜¶æ®µå¥–åŠ±è®¾è®¡ï¼ˆç”¨æˆ·åå¥½/æ ¼å¼/å·¥ä¸šåœºæ™¯ï¼‰</li>
        </ul>
      </li>
      <li><strong>ç”¨æˆ·åå¥½å¯¹é½</strong>ï¼šåŸºäº<span class="term">SIMæ¶æ„</span>çš„å¤šå¡”æ¨¡å‹ï¼Œé€šè¿‡<span class="term">BCEæŸå¤±</span>å­¦ä¹ ä¸ªæ€§åŒ–ç›®æ ‡æƒé‡</li>
    </ol>
  </div>

  <!-- æœ¯è¯­è¯†åˆ« -->
  <div class="section">
    <h2>æœ¯è¯­è¯†åˆ«</h2>
    <ul>
      <li><span class="term">Transformer Layers</span>ï¼šTransformerå±‚ï¼Œç”±è‡ªæ³¨æ„åŠ›æœºåˆ¶å’Œå‰é¦ˆç½‘ç»œç»„æˆçš„æ·±åº¦å­¦ä¹ æ¨¡å—ï¼Œç”¨äºå¤„ç†åºåˆ—æ•°æ®</li>
      <li><span class="term">CausalSelfAttn</span>ï¼šå› æœè‡ªæ³¨æ„åŠ›ï¼Œé™åˆ¶æ¯ä¸ªä½ç½®ä»…å…³æ³¨å…¶å·¦ä¾§ä½ç½®çš„è‡ªæ³¨æ„åŠ›å˜ä½“ï¼Œé˜²æ­¢ä¿¡æ¯æ³„éœ²</li>
      <li><span class="term">CrossAttn</span>ï¼šäº¤å‰æ³¨æ„åŠ›ï¼Œè¿æ¥ç¼–ç å™¨å’Œè§£ç å™¨çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œå…è®¸è§£ç å™¨è®¿é—®ç¼–ç å™¨è¾“å‡º</li>
      <li><span class="term">MoE (Mixture of Experts)</span>ï¼šæ··åˆä¸“å®¶ç½‘ç»œï¼Œç”±å¤šä¸ªä¸“å®¶å­ç½‘ç»œå’Œé—¨æ§è·¯ç”±ç»„æˆçš„ç¨€ç–æ¿€æ´»æ¶æ„ï¼Œå…¬å¼(27)</li>
      <li><span class="term">Top-k Routing Strategy</span>ï¼šTop-kè·¯ç”±ç­–ç•¥ï¼Œé€‰æ‹©æƒé‡æœ€é«˜çš„kä¸ªä¸“å®¶å¤„ç†è¾“å…¥çš„åŠ¨æ€è·¯ç”±ç®—æ³•</li>
      <li><span class="term">Loss-free Load Balancing</span>ï¼šæ— æŸå¤±è´Ÿè½½å‡è¡¡ï¼Œä¼˜åŒ–ä¸“å®¶èµ„æºåˆ†é…è€Œä¸å¼•å…¥é¢å¤–æŸå¤±é¡¹çš„æŠ€æœ¯</li>
      <li><span class="term">Cross-entropy Loss</span>ï¼šäº¤å‰ç†µæŸå¤±ï¼Œè¡¡é‡é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒä¸çœŸå®åˆ†å¸ƒå·®å¼‚çš„æŸå¤±å‡½æ•°ï¼Œå…¬å¼(28)</li>
      <li><span class="term">Next-token Prediction</span>ï¼šä¸‹ä¸€ä»¤ç‰Œé¢„æµ‹ï¼ŒåŸºäºå†å²åºåˆ—é¢„æµ‹ä¸‹ä¸€ä¸ªå…ƒç´ çš„è®­ç»ƒç›®æ ‡</li>
      <li><span class="term">Reward System</span>ï¼šå¥–åŠ±ç³»ç»Ÿï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ä¿¡å·æŒ‡å¯¼æ¨¡å‹ä¼˜åŒ–çš„æ¡†æ¶</li>
      <li><span class="term">On-policy Reinforcement Learning</span>ï¼šåœ¨çº¿ç­–ç•¥å¼ºåŒ–å­¦ä¹ ï¼Œä½¿ç”¨å½“å‰ç­–ç•¥ç”Ÿæˆæ•°æ®è¿›è¡Œè®­ç»ƒçš„æ–¹æ³•</li>
      <li><span class="term">P-Score (Preference Score)</span>ï¼šåå¥½åˆ†æ•°ï¼Œé€šè¿‡ç¥ç»ç½‘ç»œå­¦ä¹ çš„ä¸ªæ€§åŒ–å¤šç›®æ ‡èåˆæŒ‡æ ‡</li>
      <li><
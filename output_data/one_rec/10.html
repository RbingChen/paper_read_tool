<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>OneRec技术报告解析</title>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <style>
    body { font-family: Arial, sans-serif; line-height: 1.6; }
    .original { background-color: #f0f0f0; border: 1px solid #ccc; padding: 15px; margin-bottom: 10px; }
    .translation { background-color: #e0f7e0; border: 1px solid #4CAF50; padding: 15px; margin-bottom: 20px; }
    .figure { background-color: #fffacd; padding: 15px; margin: 20px 0; text-align: center; }
    .term { color: red; font-weight: bold; }
    .formula-container { text-align: center; margin: 20px 0; }
    .formula-number { display: inline-block; margin-left: 10px; }
    section { margin-bottom: 30px; }
    h2 { border-bottom: 2px solid #333; padding-bottom: 5px; }
    table { width: 100%; border-collapse: collapse; margin: 20px 0; }
    th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
    th { background-color: #f2f2f2; }
  </style>
</head>
<body>

<h1>OneRec技术报告解析</h1>

<section id="translation">
  <h2>内容翻译</h2>
  
  <div class="figure">
    <strong>图示说明：</strong><br>
    EncoderDecoderPreference模型结构图示（包含MLP层、CTR塔、用户/项目输入、P-Score塔等组件）
  </div>
  
  <div class="original">
    Figure 5|Overall Framework of the Reward System. The Reward System is composed of three parts. They assign <span class="term">Preference Reward (P-Score)</span>, <span class="term">Format Reward</span>, and specific <span class="term">Industrial Reward</span> to the videos generated by the model, respectively.
  </div>
  <div class="translation">
    <strong>图5 | 奖励系统整体框架。</strong> 奖励系统由三部分组成，分别为模型生成的视频分配<span class="term">偏好奖励（P-Score）</span>、<span class="term">格式奖励</span>和特定的<span class="term">工业奖励</span>。
  </div>
  
  <div class="original">
    and item representations, are fed into the final layer's <span class="term">Multi-Layer Perceptron (MLP)</span>. This MLP is followed by a single tower outputting the <span class="term">P-Score</span>, which computes <span class="term">binary cross-entropy loss</span> using the labels of all objectives.
  </div>
  <div class="translation">
    和项目表示被输入到最终层的<span class="term">多层感知机（MLP）</span>。该MLP后接一个输出<span class="term">P-Score</span>的单塔结构，使用所有目标的标签计算<span class="term">二元交叉熵损失（BCE）</span>。
  </div>
  
  <div class="original">
    This method allows the model to receive specific user information and adjust the <span class="term">Preference Score</span> for that user appropriately, without compromising the experience of other users. Compared to the previous approach of indiscriminate weighted summation, this method is more likely to achieve <span class="term">Pareto optimization</span>. Therefore, we use the <span class="term">P-Score</span> obtained by this method as the reward for preference alignment.
  </div>
  <div class="translation">
    该方法使模型能接收特定用户信息并适当调整该用户的<span class="term">偏好分数</span>，同时不损害其他用户体验。相比之前不加区分的加权求和方法，此方法更可能实现<span class="term">帕累托优化</span>。因此，我们将此方法获得的<span class="term">P-Score</span>作为偏好对齐的奖励。
  </div>
  
  <div class="original">
    <span class="term">Early Clipped GRPO</span> In this section, we introduce how to use the <span class="term">Preference Score</span> to align user preferences. We use <span class="term">ECPO (EarlyClipped GRPO)</span> for optimization. Specifically, for a user 𝑢, we generate 𝐺 items using the old policy model. Each item, along with the user, is input into the <span class="term">Preference Reward Model</span> to obtain the <span class="term">P-Score</span> as reward 𝑟𝑖. The optimization objective is as follows:
  </div>
  <div class="translation">
    <span class="term">早期截断GRPO</span> 本节介绍如何使用<span class="term">偏好分数</span>对齐用户偏好。我们采用<span class="term">ECPO（早期截断GRPO）</span>进行优化。具体而言，对用户𝑢，使用旧策略模型生成𝐺个项目。每个项目与用户一起输入<span class="term">偏好奖励模型</span>，获得作为奖励𝑟𝑖的<span class="term">P-Score</span>。优化目标如下：
  </div>
  
  <div class="formula-container">
    \[
    J_{ECPO}(\theta) = \mathbb{E}_{u\sim P(U),\{o_i\}_{i=1}^G\sim\pi_{\theta_{old}}} \left[ \frac{1}{G} \sum_{i=1}^G \min\left( \frac{\pi_\theta(o_i|u)}{\pi'_{\theta_{old}}(o_i|u)} A_i, \text{clip}\left( \frac{\pi_\theta(o_i|u)}{\pi'_{\theta_{old}}(o_i|u)}, 1-\epsilon, 1+\epsilon \right) A_i \right) \right]
    \]
    <span class="formula-number">(29)</span>
  </div>
  
  <div class="formula-container">
    \[
    A_i = \frac{r_i - \text{mean}(\{r_1,r_2,...,r_G\})}{\text{std}(\{r_1,r_2,...,r_G\})}
    \]
    <span class="formula-number">(30)</span>
  </div>
  
  <div class="formula-container">
    \[
    \pi'_{\theta_{old}}(o_i|u) = \max\left( \frac{\text{sg}(\pi_\theta(o_i|u))}{1+\epsilon+\delta}, \pi_{\theta_{old}}(o_i|u) \right), \quad \delta > 0
    \]
    <span class="formula-number">(31)</span>
  </div>
  
  <div class="original">
    where <span class="term">sg</span> represents the <span class="term">stop gradient operation</span> and 𝛿 is a <span class="term">hyperparameter</span> greater than 0.
  </div>
  <div class="translation">
    其中<span class="term">sg</span>表示<span class="term">梯度截断操作</span>，𝛿是大于0的<span class="term">超参数</span>。
  </div>
  
  <div class="original">
    We make a modification to <span class="term">GRPO (Group Policy Relative Optimization)</span> (Liu et al., 2024) to make its training process more stable. The illustration is presented in Figure 6. In the original GRPO, a large <span class="term">policy ratio</span> (𝜋𝜃/𝜋𝜃𝑜𝑙𝑑) is allowed for negative advantages, which can easily lead to gradient
  </div>
  <div class="translation">
    我们对<span class="term">GRPO（组策略相对优化）</span>（Liu等人，2024）进行了改进以提升训练稳定性（图示见图6）。原始GRPO允许负优势情况下的高<span class="term">策略比率</span>（𝜋𝜃/𝜋𝜃𝑜𝑙𝑑），这容易导致梯度
  </div>
</section>

<section id="understanding">
  <h2>内容理解</h2>
  <p>该技术报告主要描述OneRec推荐系统的奖励框架和优化方法：</p>
  <ol>
    <li><strong>三层奖励系统</strong>：通过<span class="term">偏好奖励（P-Score）</span>、<span class="term">格式奖励</span>和<span class="term">工业奖励</span>综合评价生成内容</li>
    <li><strong>P-Score计算架构</strong>：
      <ul>
        <li>用户和项目特征输入<span class="term">多层感知机（MLP）</span></li>
        <li>通过单塔结构输出P-Score</li>
        <li>使用<span class="term">二元交叉熵损失（BCE）</span>进行多目标优化</li>
      </ul>
    </li>
    <li><strong>个性化优势</strong>：该架构能针对特定用户调整评分，实现<span class="term">帕累托优化</span>，避免传统加权求和的负面影响</li>
    <li><strong>ECPO优化算法</strong>：
      <ul>
        <li>改进自<span class="term">GRPO（组策略相对优化）</span></li>
        <li>使用旧策略生成候选项目集</li>
        <li>通过P-Score计算优势值A_i（公式30）</li>
        <li>目标函数包含策略比率裁剪机制（公式29）</li>
        <li>引入<span class="term">梯度截断操作（sg）</span>稳定训练（公式31）</li>
      </ul>
    </li>
  </ol>
</section>

<section id="summary">
  <h2>摘要总结</h2>
  <p>本报告介绍了OneRec推荐系统的奖励框架和优化算法：</p>
  <ul>
    <li>提出<strong>三层奖励评估系统</strong>（偏好/格式/工业奖励）用于视频内容评价</li>
    <li>设计基于<strong>MLP架构的P-Score模型</strong>，通过二元交叉熵损失实现多目标优化</li>
    <li>开发<strong>ECPO（早期截断GRPO）算法</strong>：
      <ul>
        <li>使用策略梯度方法对齐用户偏好</li>
        <li>引入优势归一化（公式30）和策略比率裁剪（公式29）</li>
        <li>通过梯度截断操作（公式31）解决原始GRPO的训练不稳定问题</li>
      </ul>
    </li>
    <li>该方法在个性化推荐中实现<strong>帕累托优化</strong>，平衡用户偏好与系统目标</li>
  </ul>
</section>

<section id="terminology">
  <h2>术语解释</h2>
  <table>
    <tr>
      <th>术语（英文）</th>
      <th>术语（中文）</th>
      <th>详细解释</th>
    </tr>
    <tr>
      <td>P-Score (Preference Reward)</td>
      <td>偏好分数（偏好奖励）</td>
      <td>核心用户偏好度量指标，通过MLP模型计算，反映内容与用户偏好的匹配度</td>
    </tr>
    <tr>
      <td>ECPO (EarlyClipped GRPO)</td>
      <td>早期截断GRPO</td>
      <td>改进的强化学习算法，在GRPO基础上增加策略比率裁剪和梯度截断，提升训练稳定性</td>
    </tr>
    <tr>
      <td>GRPO (Group Policy Relative Optimization)</td>
      <td>组策略相对优化</td>
      <td>基于策略梯度的推荐优化方法，通过组内相对比较计算优势函数</td>
    </tr>
    <tr>
      <td>MLP (Multi-Layer Perceptron)</td>
      <td>多层感知机</td>
      <td>深度神经网络基础架构，用于学习用户和项目的非线性特征交互</td>
    </tr>
    <tr>
      <td>BCE (Binary Cross-Entropy)</td>
      <td>二元交叉熵</td>
      <td>损失函数，用于衡量模型预测概率分布与真实分布的差异</td>
    </tr>
    <tr>
      <td>Pareto optimization</td>
      <td>帕累托优化</td>
      <td>多目标优化方法，在不损害其他目标的前提下改进至少一个目标</td>
    </tr>
    <tr>
      <td>Stop gradient operation (sg)</td>
      <td>梯度截断操作</td>
      <td>阻止梯度回传的技术，用于稳定对抗性训练（公式31中的关键组件）</td>
    </tr>
    <tr>
      <td>Policy ratio</td>
      <td>策略比率</td>
      <td>新策略与旧策略的概率比值（π_θ/π_{old}），在策略优化中控制更新幅度</td>
    </tr>
  </table>
</section>

</body>
</html>
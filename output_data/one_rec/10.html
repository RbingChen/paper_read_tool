<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>OneRecæŠ€æœ¯æŠ¥å‘Šè§£æ</title>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <style>
    body { font-family: Arial, sans-serif; line-height: 1.6; }
    .original { background-color: #f0f0f0; border: 1px solid #ccc; padding: 15px; margin-bottom: 10px; }
    .translation { background-color: #e0f7e0; border: 1px solid #4CAF50; padding: 15px; margin-bottom: 20px; }
    .figure { background-color: #fffacd; padding: 15px; margin: 20px 0; text-align: center; }
    .term { color: red; font-weight: bold; }
    .formula-container { text-align: center; margin: 20px 0; }
    .formula-number { display: inline-block; margin-left: 10px; }
    section { margin-bottom: 30px; }
    h2 { border-bottom: 2px solid #333; padding-bottom: 5px; }
    table { width: 100%; border-collapse: collapse; margin: 20px 0; }
    th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
    th { background-color: #f2f2f2; }
  </style>
</head>
<body>

<h1>OneRecæŠ€æœ¯æŠ¥å‘Šè§£æ</h1>

<section id="translation">
  <h2>å†…å®¹ç¿»è¯‘</h2>
  
  <div class="figure">
    <strong>å›¾ç¤ºè¯´æ˜ï¼š</strong><br>
    EncoderDecoderPreferenceæ¨¡å‹ç»“æ„å›¾ç¤ºï¼ˆåŒ…å«MLPå±‚ã€CTRå¡”ã€ç”¨æˆ·/é¡¹ç›®è¾“å…¥ã€P-Scoreå¡”ç­‰ç»„ä»¶ï¼‰
  </div>
  
  <div class="original">
    Figure 5|Overall Framework of the Reward System. The Reward System is composed of three parts. They assign <span class="term">Preference Reward (P-Score)</span>, <span class="term">Format Reward</span>, and specific <span class="term">Industrial Reward</span> to the videos generated by the model, respectively.
  </div>
  <div class="translation">
    <strong>å›¾5 | å¥–åŠ±ç³»ç»Ÿæ•´ä½“æ¡†æ¶ã€‚</strong> å¥–åŠ±ç³»ç»Ÿç”±ä¸‰éƒ¨åˆ†ç»„æˆï¼Œåˆ†åˆ«ä¸ºæ¨¡å‹ç”Ÿæˆçš„è§†é¢‘åˆ†é…<span class="term">åå¥½å¥–åŠ±ï¼ˆP-Scoreï¼‰</span>ã€<span class="term">æ ¼å¼å¥–åŠ±</span>å’Œç‰¹å®šçš„<span class="term">å·¥ä¸šå¥–åŠ±</span>ã€‚
  </div>
  
  <div class="original">
    and item representations, are fed into the final layer's <span class="term">Multi-Layer Perceptron (MLP)</span>. This MLP is followed by a single tower outputting the <span class="term">P-Score</span>, which computes <span class="term">binary cross-entropy loss</span> using the labels of all objectives.
  </div>
  <div class="translation">
    å’Œé¡¹ç›®è¡¨ç¤ºè¢«è¾“å…¥åˆ°æœ€ç»ˆå±‚çš„<span class="term">å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰</span>ã€‚è¯¥MLPåæ¥ä¸€ä¸ªè¾“å‡º<span class="term">P-Score</span>çš„å•å¡”ç»“æ„ï¼Œä½¿ç”¨æ‰€æœ‰ç›®æ ‡çš„æ ‡ç­¾è®¡ç®—<span class="term">äºŒå…ƒäº¤å‰ç†µæŸå¤±ï¼ˆBCEï¼‰</span>ã€‚
  </div>
  
  <div class="original">
    This method allows the model to receive specific user information and adjust the <span class="term">Preference Score</span> for that user appropriately, without compromising the experience of other users. Compared to the previous approach of indiscriminate weighted summation, this method is more likely to achieve <span class="term">Pareto optimization</span>. Therefore, we use the <span class="term">P-Score</span> obtained by this method as the reward for preference alignment.
  </div>
  <div class="translation">
    è¯¥æ–¹æ³•ä½¿æ¨¡å‹èƒ½æ¥æ”¶ç‰¹å®šç”¨æˆ·ä¿¡æ¯å¹¶é€‚å½“è°ƒæ•´è¯¥ç”¨æˆ·çš„<span class="term">åå¥½åˆ†æ•°</span>ï¼ŒåŒæ—¶ä¸æŸå®³å…¶ä»–ç”¨æˆ·ä½“éªŒã€‚ç›¸æ¯”ä¹‹å‰ä¸åŠ åŒºåˆ†çš„åŠ æƒæ±‚å’Œæ–¹æ³•ï¼Œæ­¤æ–¹æ³•æ›´å¯èƒ½å®ç°<span class="term">å¸•ç´¯æ‰˜ä¼˜åŒ–</span>ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†æ­¤æ–¹æ³•è·å¾—çš„<span class="term">P-Score</span>ä½œä¸ºåå¥½å¯¹é½çš„å¥–åŠ±ã€‚
  </div>
  
  <div class="original">
    <span class="term">Early Clipped GRPO</span> In this section, we introduce how to use the <span class="term">Preference Score</span> to align user preferences. We use <span class="term">ECPO (EarlyClipped GRPO)</span> for optimization. Specifically, for a user ğ‘¢, we generate ğº items using the old policy model. Each item, along with the user, is input into the <span class="term">Preference Reward Model</span> to obtain the <span class="term">P-Score</span> as reward ğ‘Ÿğ‘–. The optimization objective is as follows:
  </div>
  <div class="translation">
    <span class="term">æ—©æœŸæˆªæ–­GRPO</span> æœ¬èŠ‚ä»‹ç»å¦‚ä½•ä½¿ç”¨<span class="term">åå¥½åˆ†æ•°</span>å¯¹é½ç”¨æˆ·åå¥½ã€‚æˆ‘ä»¬é‡‡ç”¨<span class="term">ECPOï¼ˆæ—©æœŸæˆªæ–­GRPOï¼‰</span>è¿›è¡Œä¼˜åŒ–ã€‚å…·ä½“è€Œè¨€ï¼Œå¯¹ç”¨æˆ·ğ‘¢ï¼Œä½¿ç”¨æ—§ç­–ç•¥æ¨¡å‹ç”Ÿæˆğºä¸ªé¡¹ç›®ã€‚æ¯ä¸ªé¡¹ç›®ä¸ç”¨æˆ·ä¸€èµ·è¾“å…¥<span class="term">åå¥½å¥–åŠ±æ¨¡å‹</span>ï¼Œè·å¾—ä½œä¸ºå¥–åŠ±ğ‘Ÿğ‘–çš„<span class="term">P-Score</span>ã€‚ä¼˜åŒ–ç›®æ ‡å¦‚ä¸‹ï¼š
  </div>
  
  <div class="formula-container">
    \[
    J_{ECPO}(\theta) = \mathbb{E}_{u\sim P(U),\{o_i\}_{i=1}^G\sim\pi_{\theta_{old}}} \left[ \frac{1}{G} \sum_{i=1}^G \min\left( \frac{\pi_\theta(o_i|u)}{\pi'_{\theta_{old}}(o_i|u)} A_i, \text{clip}\left( \frac{\pi_\theta(o_i|u)}{\pi'_{\theta_{old}}(o_i|u)}, 1-\epsilon, 1+\epsilon \right) A_i \right) \right]
    \]
    <span class="formula-number">(29)</span>
  </div>
  
  <div class="formula-container">
    \[
    A_i = \frac{r_i - \text{mean}(\{r_1,r_2,...,r_G\})}{\text{std}(\{r_1,r_2,...,r_G\})}
    \]
    <span class="formula-number">(30)</span>
  </div>
  
  <div class="formula-container">
    \[
    \pi'_{\theta_{old}}(o_i|u) = \max\left( \frac{\text{sg}(\pi_\theta(o_i|u))}{1+\epsilon+\delta}, \pi_{\theta_{old}}(o_i|u) \right), \quad \delta > 0
    \]
    <span class="formula-number">(31)</span>
  </div>
  
  <div class="original">
    where <span class="term">sg</span> represents the <span class="term">stop gradient operation</span> and ğ›¿ is a <span class="term">hyperparameter</span> greater than 0.
  </div>
  <div class="translation">
    å…¶ä¸­<span class="term">sg</span>è¡¨ç¤º<span class="term">æ¢¯åº¦æˆªæ–­æ“ä½œ</span>ï¼Œğ›¿æ˜¯å¤§äº0çš„<span class="term">è¶…å‚æ•°</span>ã€‚
  </div>
  
  <div class="original">
    We make a modification to <span class="term">GRPO (Group Policy Relative Optimization)</span> (Liu et al., 2024) to make its training process more stable. The illustration is presented in Figure 6. In the original GRPO, a large <span class="term">policy ratio</span> (ğœ‹ğœƒ/ğœ‹ğœƒğ‘œğ‘™ğ‘‘) is allowed for negative advantages, which can easily lead to gradient
  </div>
  <div class="translation">
    æˆ‘ä»¬å¯¹<span class="term">GRPOï¼ˆç»„ç­–ç•¥ç›¸å¯¹ä¼˜åŒ–ï¼‰</span>ï¼ˆLiuç­‰äººï¼Œ2024ï¼‰è¿›è¡Œäº†æ”¹è¿›ä»¥æå‡è®­ç»ƒç¨³å®šæ€§ï¼ˆå›¾ç¤ºè§å›¾6ï¼‰ã€‚åŸå§‹GRPOå…è®¸è´Ÿä¼˜åŠ¿æƒ…å†µä¸‹çš„é«˜<span class="term">ç­–ç•¥æ¯”ç‡</span>ï¼ˆğœ‹ğœƒ/ğœ‹ğœƒğ‘œğ‘™ğ‘‘ï¼‰ï¼Œè¿™å®¹æ˜“å¯¼è‡´æ¢¯åº¦
  </div>
</section>

<section id="understanding">
  <h2>å†…å®¹ç†è§£</h2>
  <p>è¯¥æŠ€æœ¯æŠ¥å‘Šä¸»è¦æè¿°OneRecæ¨èç³»ç»Ÿçš„å¥–åŠ±æ¡†æ¶å’Œä¼˜åŒ–æ–¹æ³•ï¼š</p>
  <ol>
    <li><strong>ä¸‰å±‚å¥–åŠ±ç³»ç»Ÿ</strong>ï¼šé€šè¿‡<span class="term">åå¥½å¥–åŠ±ï¼ˆP-Scoreï¼‰</span>ã€<span class="term">æ ¼å¼å¥–åŠ±</span>å’Œ<span class="term">å·¥ä¸šå¥–åŠ±</span>ç»¼åˆè¯„ä»·ç”Ÿæˆå†…å®¹</li>
    <li><strong>P-Scoreè®¡ç®—æ¶æ„</strong>ï¼š
      <ul>
        <li>ç”¨æˆ·å’Œé¡¹ç›®ç‰¹å¾è¾“å…¥<span class="term">å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰</span></li>
        <li>é€šè¿‡å•å¡”ç»“æ„è¾“å‡ºP-Score</li>
        <li>ä½¿ç”¨<span class="term">äºŒå…ƒäº¤å‰ç†µæŸå¤±ï¼ˆBCEï¼‰</span>è¿›è¡Œå¤šç›®æ ‡ä¼˜åŒ–</li>
      </ul>
    </li>
    <li><strong>ä¸ªæ€§åŒ–ä¼˜åŠ¿</strong>ï¼šè¯¥æ¶æ„èƒ½é’ˆå¯¹ç‰¹å®šç”¨æˆ·è°ƒæ•´è¯„åˆ†ï¼Œå®ç°<span class="term">å¸•ç´¯æ‰˜ä¼˜åŒ–</span>ï¼Œé¿å…ä¼ ç»ŸåŠ æƒæ±‚å’Œçš„è´Ÿé¢å½±å“</li>
    <li><strong>ECPOä¼˜åŒ–ç®—æ³•</strong>ï¼š
      <ul>
        <li>æ”¹è¿›è‡ª<span class="term">GRPOï¼ˆç»„ç­–ç•¥ç›¸å¯¹ä¼˜åŒ–ï¼‰</span></li>
        <li>ä½¿ç”¨æ—§ç­–ç•¥ç”Ÿæˆå€™é€‰é¡¹ç›®é›†</li>
        <li>é€šè¿‡P-Scoreè®¡ç®—ä¼˜åŠ¿å€¼A_iï¼ˆå…¬å¼30ï¼‰</li>
        <li>ç›®æ ‡å‡½æ•°åŒ…å«ç­–ç•¥æ¯”ç‡è£å‰ªæœºåˆ¶ï¼ˆå…¬å¼29ï¼‰</li>
        <li>å¼•å…¥<span class="term">æ¢¯åº¦æˆªæ–­æ“ä½œï¼ˆsgï¼‰</span>ç¨³å®šè®­ç»ƒï¼ˆå…¬å¼31ï¼‰</li>
      </ul>
    </li>
  </ol>
</section>

<section id="summary">
  <h2>æ‘˜è¦æ€»ç»“</h2>
  <p>æœ¬æŠ¥å‘Šä»‹ç»äº†OneRecæ¨èç³»ç»Ÿçš„å¥–åŠ±æ¡†æ¶å’Œä¼˜åŒ–ç®—æ³•ï¼š</p>
  <ul>
    <li>æå‡º<strong>ä¸‰å±‚å¥–åŠ±è¯„ä¼°ç³»ç»Ÿ</strong>ï¼ˆåå¥½/æ ¼å¼/å·¥ä¸šå¥–åŠ±ï¼‰ç”¨äºè§†é¢‘å†…å®¹è¯„ä»·</li>
    <li>è®¾è®¡åŸºäº<strong>MLPæ¶æ„çš„P-Scoreæ¨¡å‹</strong>ï¼Œé€šè¿‡äºŒå…ƒäº¤å‰ç†µæŸå¤±å®ç°å¤šç›®æ ‡ä¼˜åŒ–</li>
    <li>å¼€å‘<strong>ECPOï¼ˆæ—©æœŸæˆªæ–­GRPOï¼‰ç®—æ³•</strong>ï¼š
      <ul>
        <li>ä½¿ç”¨ç­–ç•¥æ¢¯åº¦æ–¹æ³•å¯¹é½ç”¨æˆ·åå¥½</li>
        <li>å¼•å…¥ä¼˜åŠ¿å½’ä¸€åŒ–ï¼ˆå…¬å¼30ï¼‰å’Œç­–ç•¥æ¯”ç‡è£å‰ªï¼ˆå…¬å¼29ï¼‰</li>
        <li>é€šè¿‡æ¢¯åº¦æˆªæ–­æ“ä½œï¼ˆå…¬å¼31ï¼‰è§£å†³åŸå§‹GRPOçš„è®­ç»ƒä¸ç¨³å®šé—®é¢˜</li>
      </ul>
    </li>
    <li>è¯¥æ–¹æ³•åœ¨ä¸ªæ€§åŒ–æ¨èä¸­å®ç°<strong>å¸•ç´¯æ‰˜ä¼˜åŒ–</strong>ï¼Œå¹³è¡¡ç”¨æˆ·åå¥½ä¸ç³»ç»Ÿç›®æ ‡</li>
  </ul>
</section>

<section id="terminology">
  <h2>æœ¯è¯­è§£é‡Š</h2>
  <table>
    <tr>
      <th>æœ¯è¯­ï¼ˆè‹±æ–‡ï¼‰</th>
      <th>æœ¯è¯­ï¼ˆä¸­æ–‡ï¼‰</th>
      <th>è¯¦ç»†è§£é‡Š</th>
    </tr>
    <tr>
      <td>P-Score (Preference Reward)</td>
      <td>åå¥½åˆ†æ•°ï¼ˆåå¥½å¥–åŠ±ï¼‰</td>
      <td>æ ¸å¿ƒç”¨æˆ·åå¥½åº¦é‡æŒ‡æ ‡ï¼Œé€šè¿‡MLPæ¨¡å‹è®¡ç®—ï¼Œåæ˜ å†…å®¹ä¸ç”¨æˆ·åå¥½çš„åŒ¹é…åº¦</td>
    </tr>
    <tr>
      <td>ECPO (EarlyClipped GRPO)</td>
      <td>æ—©æœŸæˆªæ–­GRPO</td>
      <td>æ”¹è¿›çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œåœ¨GRPOåŸºç¡€ä¸Šå¢åŠ ç­–ç•¥æ¯”ç‡è£å‰ªå’Œæ¢¯åº¦æˆªæ–­ï¼Œæå‡è®­ç»ƒç¨³å®šæ€§</td>
    </tr>
    <tr>
      <td>GRPO (Group Policy Relative Optimization)</td>
      <td>ç»„ç­–ç•¥ç›¸å¯¹ä¼˜åŒ–</td>
      <td>åŸºäºç­–ç•¥æ¢¯åº¦çš„æ¨èä¼˜åŒ–æ–¹æ³•ï¼Œé€šè¿‡ç»„å†…ç›¸å¯¹æ¯”è¾ƒè®¡ç®—ä¼˜åŠ¿å‡½æ•°</td>
    </tr>
    <tr>
      <td>MLP (Multi-Layer Perceptron)</td>
      <td>å¤šå±‚æ„ŸçŸ¥æœº</td>
      <td>æ·±åº¦ç¥ç»ç½‘ç»œåŸºç¡€æ¶æ„ï¼Œç”¨äºå­¦ä¹ ç”¨æˆ·å’Œé¡¹ç›®çš„éçº¿æ€§ç‰¹å¾äº¤äº’</td>
    </tr>
    <tr>
      <td>BCE (Binary Cross-Entropy)</td>
      <td>äºŒå…ƒäº¤å‰ç†µ</td>
      <td>æŸå¤±å‡½æ•°ï¼Œç”¨äºè¡¡é‡æ¨¡å‹é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒä¸çœŸå®åˆ†å¸ƒçš„å·®å¼‚</td>
    </tr>
    <tr>
      <td>Pareto optimization</td>
      <td>å¸•ç´¯æ‰˜ä¼˜åŒ–</td>
      <td>å¤šç›®æ ‡ä¼˜åŒ–æ–¹æ³•ï¼Œåœ¨ä¸æŸå®³å…¶ä»–ç›®æ ‡çš„å‰æä¸‹æ”¹è¿›è‡³å°‘ä¸€ä¸ªç›®æ ‡</td>
    </tr>
    <tr>
      <td>Stop gradient operation (sg)</td>
      <td>æ¢¯åº¦æˆªæ–­æ“ä½œ</td>
      <td>é˜»æ­¢æ¢¯åº¦å›ä¼ çš„æŠ€æœ¯ï¼Œç”¨äºç¨³å®šå¯¹æŠ—æ€§è®­ç»ƒï¼ˆå…¬å¼31ä¸­çš„å…³é”®ç»„ä»¶ï¼‰</td>
    </tr>
    <tr>
      <td>Policy ratio</td>
      <td>ç­–ç•¥æ¯”ç‡</td>
      <td>æ–°ç­–ç•¥ä¸æ—§ç­–ç•¥çš„æ¦‚ç‡æ¯”å€¼ï¼ˆÏ€_Î¸/Ï€_{old}ï¼‰ï¼Œåœ¨ç­–ç•¥ä¼˜åŒ–ä¸­æ§åˆ¶æ›´æ–°å¹…åº¦</td>
    </tr>
  </table>
</section>

</body>
</html>
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <title>OneRecæŠ€æœ¯æŠ¥å‘Šåˆ†æ</title>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style>
        body { font-family: Arial, sans-serif; line-height: 1.6; }
        .original { background-color: #f0f0f0; border: 1px solid #ccc; padding: 15px; margin-bottom: 10px; }
        .translation { background-color: #e0f7e0; border: 1px solid #4CAF50; padding: 15px; margin-bottom: 20px; }
        .table-container { background-color: #fffde7; padding: 15px; margin: 20px 0; border: 1px solid #ffd54f; }
        table { width: 100%; border-collapse: collapse; margin: 10px 0; }
        th, td { border: 1px solid #ddd; padding: 8px; text-align: center; }
        th { background-color: #f5f5f5; }
        .term { color: red; font-weight: bold; }
        h2 { color: #2c3e50; border-bottom: 2px solid #3498db; padding-bottom: 5px; }
        .section { margin-bottom: 30px; }
    </style>
</head>
<body>

<div class="section">
    <h2>1. å†…å®¹ç†è§£</h2>
    <p>è¯¥æŠ€æœ¯æŠ¥å‘Šæ¢è®¨äº†OneRecç³»ç»Ÿä¸­å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–çš„ä¸‰ä¸ªæ ¸å¿ƒç»´åº¦ï¼š</p>
    <ul>
        <li><span class="term">æœç´¢ç©ºé—´ï¼ˆSearch Spaceï¼‰</span>ï¼šå®éªŒè¡¨æ˜æ‰©å¤§åŠ¨ä½œæœç´¢ç©ºé—´ï¼ˆç»„å¤§å°ï¼‰å¯æå‡æœ€ä¼˜é¡¹å‘ç°æ¦‚ç‡ï¼Œä½†å­˜åœ¨è®¡ç®—æˆæœ¬æƒè¡¡ã€‚ç»„å¤§å°å¢è‡³512æ—¶æ€§èƒ½æ˜¾è‘—æå‡ï¼Œä½†2048æ—¶æ”¶ç›Šé€’å‡ï¼Œå»ºè®®è®¾ä¸ºæ¨ç†è¾“å‡ºé‡çš„4å€</li>
        <li><span class="term">æœç´¢ç­–ç•¥ï¼ˆSearch Strategyï¼‰</span>ï¼šæ¯”è¾ƒtop-k/top-pé‡‡æ ·ä¸æŸæœç´¢ï¼ˆBeam Searchï¼‰ã€‚å› è¯­ä¹‰IDçš„<strong>å‰ç¼€æ ‘ç»“æ„</strong>ç‰¹æ€§ï¼ŒæŸæœç´¢ç³»ç»Ÿæ€§æ¢ç´¢æ›´æœ‰æ•ˆï¼Œå„é¡¹æŒ‡æ ‡æå‡0.36%-1.45%</li>
        <li><span class="term">å‚è€ƒæ¨¡å‹ï¼ˆReference Modelï¼‰</span>ï¼šåœ¨çº¿ç­–ç•¥ï¼ˆon-policyï¼‰ä¼˜äºç¦»çº¿ç­–ç•¥ï¼ˆoff-policyï¼‰ï¼Œå°¤å…¶åœ¨ç¦»çº¿å¥–åŠ±è¯„ä¼°ä¸­ã€‚ä½†åœ¨çº¿å¥–åŠ±å®šä¹‰ç¼ºé™·å¯¼è‡´è½»å¾®<span class="term">å¥–åŠ±é»‘å®¢ï¼ˆReward Hackingï¼‰</span>é—®é¢˜</li>
    </ul>
    <p>æ ¸å¿ƒå®éªŒæŒ‡æ ‡ï¼šè§‚çœ‹å®Œæˆç‡ï¼ˆvtrï¼‰ã€è§‚çœ‹æ—¶é•¿ï¼ˆWatch timeï¼‰ã€åº”ç”¨åœç•™æ—¶é—´ï¼ˆApp Stay Timeï¼‰ã€è§†é¢‘è§‚çœ‹é‡ï¼ˆVideo Viewï¼‰</p>
</div>

<div class="section">
    <h2>2. å†…å®¹ç¿»è¯‘</h2>
    
    <div class="original">
        <p>OneRec Technical Report</p>
        <p>Search Space In ECPO training, expanding the action search space increases the likelihood of discovering the optimal item with maximum reward, albeit at higher computational costs. To investigate this trade-off, we examine how the search space size (i.e., group size) affects performance. The results for pass@128 are summarized in Table 7. From Table 7, we observe a significant improvement in performance when the group size is increased from 128 to 512. This clearly demonstrates the positive impact of expanding the search space. It is somewhat disappointing that increasing the search space to 2048 does not yield much additional benefit, which might be due to the current reference modelâ€™s diversity not being sufficient to discover more and better items. Nonetheless, this finding is promising, and we empirically suggest setting the ECPO training group size to approximately four times the inference output quantity for optimal results.</p>
    </div>
    <div class="translation">
        <p>OneRecæŠ€æœ¯æŠ¥å‘Š</p>
        <p><strong>æœç´¢ç©ºé—´</strong> åœ¨ECPOè®­ç»ƒä¸­ï¼Œæ‰©å¤§åŠ¨ä½œæœç´¢ç©ºé—´è™½ä¼šå¢åŠ è®¡ç®—æˆæœ¬ï¼Œä½†èƒ½æé«˜å‘ç°å…·æœ‰æœ€å¤§å¥–åŠ±çš„æœ€ä¼˜é¡¹çš„å¯èƒ½æ€§ã€‚ä¸ºç ”ç©¶æ­¤æƒè¡¡å…³ç³»ï¼Œæˆ‘ä»¬æ£€éªŒäº†æœç´¢ç©ºé—´å¤§å°ï¼ˆå³ç»„å¤§å°ï¼‰å¦‚ä½•å½±å“æ€§èƒ½ã€‚pass@128çš„ç»“æœæ€»ç»“äºè¡¨7ä¸­ã€‚ä»è¡¨7å¯è§ï¼Œå½“ç»„å¤§å°ä»128å¢è‡³512æ—¶æ€§èƒ½æ˜¾è‘—æå‡ï¼Œè¿™æ˜ç¡®è¯æ˜äº†æ‰©å¤§æœç´¢ç©ºé—´çš„ç§¯æå½±å“ã€‚ä»¤äººå¤±æœ›çš„æ˜¯ï¼Œå°†æœç´¢ç©ºé—´æ‰©å¤§è‡³2048å¹¶æœªå¸¦æ¥æ˜æ˜¾é¢å¤–æ”¶ç›Šï¼Œå¯èƒ½æºäºå½“å‰å‚è€ƒæ¨¡å‹çš„å¤šæ ·æ€§ä¸è¶³ä»¥å‘ç°æ›´å¤šæ›´ä¼˜çš„é¡¹ã€‚å°½ç®¡å¦‚æ­¤ï¼Œè¯¥å‘ç°å…·æœ‰å‰æ™¯ï¼Œæˆ‘ä»¬æ ¹æ®ç»éªŒå»ºè®®å°†ECPOè®­ç»ƒç»„å¤§å°è®¾ä¸ºæ¨ç†è¾“å‡ºé‡çš„çº¦å››å€ä»¥è·å¾—æœ€ä¼˜ç»“æœã€‚</p>
    </div>
    
    <div class="table-container">
        <p>Table 7 | Performance of different group sizes when calculating ECPO loss on Pass@128.</p>
        <table>
            <tr>
                <th>ç»„å¤§å°</th>
                <th>vtr</th>
                <th>è§‚çœ‹æ—¶é•¿</th>
                <th>åº”ç”¨åœç•™æ—¶é—´</th>
                <th>è§†é¢‘è§‚çœ‹é‡</th>
            </tr>
            <tr>
                <td>0(æ— å¼ºåŒ–å­¦ä¹ )</td>
                <td>0.2198</td>
                <td>+4.61%</td>
                <td>+1.11%</td>
                <td>-12.75%</td>
            </tr>
            <tr>
                <td>128</td>
                <td>0.2303</td>
                <td>+5.22%</td>
                <td>+1.49%</td>
                <td>-15.06%</td>
            </tr>
            <tr>
                <td>512</td>
                <td>0.2350</td>
                <td>+5.73%</td>
                <td>+1.82%</td>
                <td>-15.49%</td>
            </tr>
            <tr>
                <td>2048</td>
                <td>0.2352</td>
                <td>+5.84%</td>
                <td>+1.78%</td>
                <td>-15.49%</td>
            </tr>
        </table>
        <p>è¡¨7 | åœ¨Pass@128ä¸Šè®¡ç®—ECPOæŸå¤±æ—¶ä¸åŒç»„å¤§å°çš„æ€§èƒ½å¯¹æ¯”</p>
    </div>
    
    <div class="original">
        <p>Search Strategy Reinforcement learning for large language models typically employs top-ğ‘˜ and top-ğ‘ sampling for sample generation. In OneRec, we also explore beam search as an alternative strategy. Table 8 compares the results of these two approaches, revealing that beam search significantly outperforms top-ğ‘˜ and top-ğ‘ sampling in OneRecâ€™s reinforcement learning framework. This improvement stems from the inherent regularity of semantic ID structures, which follow a prefix tree encoding scheme and thus align well with the systematic exploration of beam search.</p>
    </div>
    <div class="translation">
        <p><strong>æœç´¢ç­–ç•¥</strong> å¤§è¯­è¨€æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ é€šå¸¸é‡‡ç”¨top-kå’Œtop-pé‡‡æ ·è¿›è¡Œæ ·æœ¬ç”Ÿæˆã€‚åœ¨OneRecä¸­ï¼Œæˆ‘ä»¬è¿˜æ¢ç´¢äº†æŸæœç´¢ï¼ˆBeam Searchï¼‰ä½œä¸ºæ›¿ä»£ç­–ç•¥ã€‚è¡¨8æ¯”è¾ƒäº†ä¸¤ç§æ–¹æ³•çš„ç»“æœï¼Œè¡¨æ˜åœ¨OneRecçš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ä¸­ï¼ŒæŸæœç´¢æ˜¾è‘—ä¼˜äºtop-kå’Œtop-pé‡‡æ ·ã€‚æ­¤æ”¹è¿›æºäºè¯­ä¹‰IDç»“æ„å›ºæœ‰çš„è§„å¾‹æ€§â€”â€”å…¶éµå¾ªå‰ç¼€æ ‘ç¼–ç æ–¹æ¡ˆï¼Œå› æ­¤ä¸æŸæœç´¢çš„ç³»ç»Ÿæ€§æ¢ç´¢é«˜åº¦å¥‘åˆã€‚</p>
    </div>
    
    <div class="table-container">
        <p>Table 8 | Performance of reinforcement learning with different search strategies.</p>
        <table>
            <tr>
                <th>ç­–ç•¥</th>
                <th>vtr</th>
                <th>è§‚çœ‹æ—¶é•¿</th>
                <th>åº”ç”¨åœç•™æ—¶é—´</th>
                <th>è§†é¢‘è§‚çœ‹é‡</th>
            </tr>
            <tr>
                <td>Top-k+Top-p</td>
                <td>0.2131</td>
                <td>+4.45%</td>
                <td>+1.16%</td>
                <td>-13.61%</td>
            </tr>
            <tr>
                <td>æŸæœç´¢</td>
                <td>0.2162</td>
                <td>+5.35%</td>
                <td>+1.76%</td>
                <td>-13.30%</td>
            </tr>
            <tr>
                <td>ç›¸å¯¹æå‡</td>
                <td>+1.45%</td>
                <td>+0.87%</td>
                <td>+0.60%</td>
                <td>+0.36%</td>
            </tr>
        </table>
        <p>è¡¨8 | ä¸åŒæœç´¢ç­–ç•¥çš„å¼ºåŒ–å­¦ä¹ æ€§èƒ½å¯¹æ¯”</p>
    </div>
    
    <div class="original">
        <p>Reference Model In this section, we compare two reference models for strategy generation in ECPO: (1) the pre-trained model (off-policy) and (2) the current policy model (on-policy). The experimental results are summarized in Table 9. From the table, it is evident that using the current policy model yields better results, especially in offline reward evaluation. This indicates that the on-policy approach allows the model to continuously teach itself, breaking through the limitations of the reference model and achieving a higher upper limit. However, in terms of online performance, the improvement with the on-policy approach is not very significant. This is due to the suboptimal definition of the reward, leading to slight reward hacking. We will focus on this aspect as a key direction for future work.</p>
    </div>
    <div class="translation">
        <p><strong>å‚è€ƒæ¨¡å‹</strong> æœ¬èŠ‚æ¯”è¾ƒECPOç­–ç•¥ç”Ÿæˆçš„ä¸¤ç§å‚è€ƒæ¨¡å‹ï¼š(1)é¢„è®­ç»ƒæ¨¡å‹ï¼ˆç¦»çº¿ç­–ç•¥ï¼‰å’Œ(2)å½“å‰ç­–ç•¥æ¨¡å‹ï¼ˆåœ¨çº¿ç­–ç•¥ï¼‰ã€‚å®éªŒç»“æœæ€»ç»“äºè¡¨9ã€‚ä»è¡¨ä¸­å¯è§ï¼Œä½¿ç”¨å½“å‰ç­–ç•¥æ¨¡å‹æ•ˆæœæ›´ä¼˜ï¼Œå°¤å…¶åœ¨ç¦»çº¿å¥–åŠ±è¯„ä¼°ä¸­ã€‚è¿™è¡¨æ˜åœ¨çº¿ç­–ç•¥æ–¹æ³•ä½¿æ¨¡å‹èƒ½æŒç»­è‡ªæˆ‘å­¦ä¹ ï¼Œçªç ´å‚è€ƒæ¨¡å‹çš„é™åˆ¶å®ç°æ›´é«˜ä¸Šé™ã€‚ä½†åœ¨åœ¨çº¿æ€§èƒ½æ–¹é¢ï¼Œåœ¨çº¿ç­–ç•¥çš„æ”¹è¿›ä¸æ˜¾è‘—ï¼Œè¿™æºäºå¥–åŠ±å®šä¹‰æ¬ ä½³å¯¼è‡´çš„è½»å¾®å¥–åŠ±é»‘å®¢é—®é¢˜ã€‚æˆ‘ä»¬å°†ä»¥æ­¤ä½œä¸ºæœªæ¥å·¥ä½œçš„é‡ç‚¹æ–¹å‘ã€‚</p>
    </div>
</div>

<div class="section">
    <h2>3. æ‘˜è¦æ€»ç»“</h2>
    <p>æœ¬æŠ¥å‘Šé€šè¿‡ä¸‰ç»„å®éªŒä¼˜åŒ–OneRecç³»ç»Ÿçš„å¼ºåŒ–å­¦ä¹ ç»„ä»¶ï¼š</p>
    <ol>
        <li><strong>æœç´¢ç©ºé—´</strong>ï¼šç»„å¤§å°å¢è‡³512æ—¶å…³é”®æŒ‡æ ‡æå‡æ˜¾è‘—ï¼ˆå¦‚vträ»0.2303â†’0.2350ï¼‰ï¼Œä½†2048æ—¶è¾¹é™…æ”¶ç›Šé€’å‡</li>
        <li><strong>æœç´¢ç­–ç•¥</strong>ï¼šæŸæœç´¢å› åŒ¹é…è¯­ä¹‰IDçš„å‰ç¼€æ ‘ç»“æ„ï¼Œç›¸æ¯”top-k/top-pé‡‡æ ·å…¨é¢é¢†å…ˆï¼ˆæœ€é«˜ç›¸å¯¹æå‡1.45%ï¼‰</li>
        <li><strong>å‚è€ƒæ¨¡å‹</strong>ï¼šåœ¨çº¿ç­–ç•¥æ¨¡å‹çªç ´ç¦»çº¿ç­–ç•¥é™åˆ¶ï¼Œä½†å—é™äºå¥–åŠ±å‡½æ•°è®¾è®¡ï¼Œåœ¨çº¿åœºæ™¯å­˜åœ¨å¥–åŠ±é»‘å®¢é£é™©</li>
    </ol>
    <p>æ ¸å¿ƒç»“è®ºï¼šECPOè®­ç»ƒç»„å¤§å°å»ºè®®è®¾ä¸ºæ¨ç†è¾“å‡ºé‡çš„4å€ï¼ŒæŸæœç´¢ä½œä¸ºé¦–é€‰ç­–ç•¥ï¼Œéœ€ä¼˜åŒ–å¥–åŠ±å‡½æ•°è§£å†³åœ¨çº¿ç­–ç•¥ç“¶é¢ˆã€‚</p>
</div>

<div class="section">
    <h2>4. æœ¯è¯­è§£é‡Š</h2>
    <dl>
        <dt><span class="term">ECPOï¼ˆEnhanced Collaborative Policy Optimizationï¼‰</span></dt>
        <dd>å¢å¼ºå¼åä½œç­–ç•¥ä¼˜åŒ–ï¼Œä¸€ç§ç»“åˆå¤šç›®æ ‡ä¼˜åŒ–çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºæ¨èç³»ç»Ÿç­–ç•¥è®­ç»ƒ</dd>
        
        <dt><span class="term">æœç´¢ç©ºé—´/ç»„å¤§å°ï¼ˆSearch Space/Group Sizeï¼‰</span></dt>
        <dd>è®­ç»ƒæ—¶å•æ¬¡ç­–ç•¥ä¼˜åŒ–è€ƒè™‘çš„å€™é€‰åŠ¨ä½œæ•°é‡ï¼Œæ‰©å¤§ç©ºé—´æå‡æœ€ä¼˜é¡¹å‘ç°æ¦‚ç‡ä½†å¢åŠ è®¡ç®—å¼€é”€</dd>
        
        <dt><span class="term">æŸæœç´¢ï¼ˆBeam Searchï¼‰</span></dt>
        <dd>å¯å‘å¼å›¾æœç´¢ç®—æ³•ï¼Œæ¯æ­¥ä¿ç•™Top-Bä¸ªæœ€ä¼˜è·¯å¾„ï¼Œé€‚åˆç»“æ„åŒ–è¾“å‡ºï¼ˆå¦‚è¯­ä¹‰IDçš„å‰ç¼€æ ‘ï¼‰</dd>
        
        <dt><span class="term">Top-k/Top-pé‡‡æ ·</span></dt>
        <dd>
            <ul>
                <li>Top-kï¼šä»…ä»æ¦‚ç‡æœ€é«˜çš„kä¸ªå€™é€‰ä¸­é‡‡æ ·</li>
                <li>Top-pï¼šä»ç´¯ç§¯æ¦‚ç‡â‰¥pçš„æœ€å°å€™é€‰é›†ä¸­é‡‡æ ·</li>
            </ul>
        </dd>
        
        <dt><span class="term">åœ¨çº¿ç­–ç•¥ï¼ˆOn-policyï¼‰</span></dt>
        <dd>ä½¿ç”¨å½“å‰ç­–ç•¥ç”Ÿæˆçš„æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œå®ç°æŒç»­è‡ªæˆ‘ä¼˜åŒ–ï¼Œå…¬å¼ï¼š\( \
abla J(\\theta) = \\mathbb{E}_{\\tau\\sim\\pi_\\theta}[\
abla_\\theta\\log\\pi_\\theta(\\tau)r(\\tau)] \)</dd>
        
        <dt><span class="term">å¥–åŠ±é»‘å®¢ï¼ˆReward Hackingï¼‰</span></dt>
        <dd>æ¨¡å‹åˆ©ç”¨å¥–åŠ±å‡½æ•°ç¼ºé™·è·å–é«˜å›æŠ¥å´æœªå®ç°çœŸå®ç›®æ ‡ï¼Œå¦‚è¿‡åº¦ä¼˜åŒ–å•ä¸€æŒ‡æ ‡æŸå®³æ•´ä½“ä½“éªŒ</dd>
        
        <dt><span class="term">è¯­ä¹‰IDï¼ˆSemantic IDï¼‰</span></dt>
        <dd>ç‰©å“çš„ç»“æ„åŒ–å‘é‡è¡¨ç¤ºï¼Œé€šè¿‡å‰ç¼€æ ‘ç¼–ç ä¿æŒè¯­ä¹‰å±‚æ¬¡å…³ç³»</dd>
    </dl>
</div>

</body>
</html>
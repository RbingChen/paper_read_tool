<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>OneRec Technical Report 参考文献分析</title>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <style>
    body { font-family: Arial, sans-serif; line-height: 1.6; margin: 20px; }
    h1 { color: #2c3e50; text-align: center; }
    h2 { color: #3498db; border-bottom: 2px solid #3498db; padding-bottom: 5px; }
    .section { margin-bottom: 30px; }
    .original { background-color: #f0f0f0; border: 1px solid #cccccc; padding: 15px; margin: 10px 0; border-radius: 5px; }
    .translation { background-color: #e0f7e0; border: 1px solid #4caf50; padding: 15px; margin: 10px 0; border-radius: 5px; }
    .term { color: red; font-weight: bold; }
    .translation-pair { margin-bottom: 20px; }
    .term-list { list-style-type: none; padding: 0; }
    .term-list li { margin-bottom: 15px; }
  </style>
</head>
<body>
  <h1>OneRec Technical Report 参考文献分析</h1>
  
  <div class="section">
    <h2>内容理解</h2>
    <p>输入文本是“OneRec Technical Report”的参考文献部分，包含多个研究论文的引用条目。每个条目包括作者列表、论文标题、发表平台（如arXiv预印本或会议论文集）、出版年份等信息。文本主题集中在机器学习、自然语言处理（NLP）和推荐系统领域，具体涉及词熵计算、个性化推荐算法、用户行为建模、深度学习架构（如宽深学习）、稀疏专家模型、语言模型缩放定律、小型语言模型优化等。这些引用来自顶级会议（如ACM SIGKDD、IEEE/CVF CVPR）和arXiv预印本平台，年份跨度从2016到2025年，反映了OneRec技术报告所依赖的前沿研究基础。整体上，该参考文献列表支持了报告的核心内容，强调高效模型设计、多目标优化和实际应用（如CTR预测），展示了跨学科的技术整合。</p>
  </div>
  
  <div class="section">
    <h2>内容翻译</h2>
    <p>以下是英文参考文献列表的中文翻译。每个引用条目作为一个完整段落处理，英文原文与中文翻译对照展示，未逐句对照。</p>
    
    <div class="translation-pair">
      <div class="original">C. Bentz and D. Alikaniotis. The word entropy of natural languages. arXiv preprint arXiv:1606.06996 , 2016.</div>
      <div class="translation">C. Bentz 和 D. Alikaniotis。自然语言的词熵。arXiv 预印本 arXiv:1606.06996，2016年。</div>
    </div>
    
    <div class="translation-pair">
      <div class="original">J. Cao, P. Xu, Y. Cheng, K. Guo, J. Tang, S. Wang, D. Leng, S. Yang, Z. Liu, Y. Niu, et al. Pantheon: Personalized multi-objective ensemble sort via iterative pareto policy optimization. arXiv preprint arXiv:2505.13894 , 2025.</div>
      <div class="translation">J. Cao、P. Xu、Y. Cheng、K. Guo、J. Tang、S. Wang、D. Leng、S. Yang、Z. Liu、Y. Niu 等人。Pantheon：通过迭代帕累托策略优化的个性化多目标集成排序。arXiv 预印本 arXiv:2505.13894，2025年。</div>
    </div>
    
    <div class="translation-pair">
      <div class="original">J. Chang, C. Zhang, Z. Fu, X. Zang, L. Guan, J. Lu, Y. Hui, D. Leng, Y. Niu, Y. Song, et al. Twin: Two-stage interest network for lifelong user behavior modeling in ctr prediction at kuaishou. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining , pages 3785–3794, 2023.</div>
      <div class="translation">J. Chang、C. Zhang、Z. Fu、X. Zang、L. Guan、J. Lu、Y. Hui、D. Leng、Y. Niu、Y. Song 等人。Twin：用于快手CTR预测中终身用户行为建模的两阶段兴趣网络。发表于《第29届ACM SIGKDD知识发现与数据挖掘会议论文集》，第3785–3794页，2023年。</div>
    </div>
    
    <div class="translation-pair">
      <div class="original">H.-T. Cheng, L. Koc, J. Harmsen, T. Shaked, T. Chandra, H. Aradhye, G. Anderson, G. Corrado, W. Chai, M. Ispir, et al. Wide & deep learning for recommender systems. In Proceedings of the 1st workshop on deep learning for recommender systems , pages 7–10, 2016.</div>
      <div class="translation">H.-T. Cheng、L. Koc、J. Harmsen、T. Shaked、T. Chandra、H. Aradhye、G. Anderson、G. Corrado、W. Chai、M. Ispir 等人。用于推荐系统的宽深学习。发表于《第一届推荐系统深度学习研讨会论文集》，第7–10页，2016年。</div>
    </div>
    
    <div class="translation-pair">
      <div class="original">A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Yang, A. Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783 , 2024.</div>
      <div class="translation">A. Dubey、A. Jauhri、A. Pandey、A. Kadian、A. Al-Dahle、A. Letman、A. Mathur、A. Schelten、A. Yang、A. Fan 等人。LLaMA 3模型群。arXiv 预印本 arXiv:2407.21783，2024年。</div>
    </div>
    
    <div class="translation-pair">
      <div class="original">W. Fedus, J. Dean, and B. Zoph. A review of sparse expert models in deep learning. arXiv preprint arXiv:2209.01667 , 2022.</div>
      <div class="translation">W. Fedus、J. Dean 和 B. Zoph。深度学习中的稀疏专家模型综述。arXiv 预印本 arXiv:2209.01667，2022年。</div>
    </div>
    
    <div class="translation-pair">
      <div class="original">A.Grattafiori,A.Dubey,A.Jauhri,A.Pandey,A.Kadian,A.Al-Dahle,A.Letman,A.Mathur,A.Schelten, A. Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783 , 2024.</div>
      <div class="translation">A. Grattafiori、A. Dubey、A. Jauhri、A. Pandey、A. Kadian、A. Al-Dahle、A. Letman、A. Mathur、A. Schelten、A. Vaughan 等人。LLaMA 3模型群。arXiv 预印本 arXiv:2407.21783，2024年。</div>
    </div>
    
    <div class="translation-pair">
      <div class="original">H. Guo, R. Tang, Y. Ye, Z. Li, and X. He. Deepfm: a factorization-machine based neural network for ctr prediction. arXiv preprint arXiv:1703.04247 , 2017.</div>
      <div class="translation">H. Guo、R. Tang、Y. Ye、Z. Li 和 X. He。DeepFM：基于分解机的神经网络用于CTR预测。arXiv 预印本 arXiv:1703.04247，2017年。</div>
    </div>
    
    <div class="translation-pair">
      <div class="original">T. Henighan, J. Kaplan, M. Katz, M. Chen, C. Hesse, J. Jackson, H. Jun, T. B. Brown, P. Dhariwal, S. Gray, et al. Scaling laws for autoregressive generative modeling. arXiv preprint arXiv:2010.14701 , 2020.</div>
      <div class="translation">T. Henighan、J. Kaplan、M. Katz、M. Chen、C. Hesse、J. Jackson、H. Jun、T. B. Brown、P. Dhariwal、S. Gray 等人。自回归生成建模的缩放定律。arXiv 预印本 arXiv:2010.14701，2020年。</div>
    </div>
    
    <div class="translation-pair">
      <div class="original">J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. d. L. Casas, L. A. Hendricks, J. Welbl, A. Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556 , 2022.</div>
      <div class="translation">J. Hoffmann、S. Borgeaud、A. Mensch、E. Buchatskaya、T. Cai、E. Rutherford、D. d. L. Casas、L. A. Hendricks、J. Welbl、A. Clark 等人。训练计算最优的大型语言模型。arXiv 预印本 arXiv:2203.15556，2022年。</div>
    </div>
    
    <div class="translation-pair">
      <div class="original">S. Hu, Y. Tu, X. Han, C. He, G. Cui, X. Long, Z. Zheng, Y. Fang, Y. Huang, W. Zhao, et al. Minicpm: Unveiling the potential of small language models with scalable training strategies. arXiv preprint arXiv:2404.06395 , 2024.</div>
      <div class="translation">S. Hu、Y. Tu、X. Han、C. He、G. Cui、X. Long、Z. Zheng、Y. Fang、Y. Huang、W. Zhao 等人。MiniCPM：通过可扩展训练策略揭示小型语言模型的潜力。arXiv 预印本 arXiv:2404.06395，2024年。</div>
    </div>
    
    <div class="translation-pair">
      <div class="original">A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford, D. S. Chaplot, D. d. l. Casas, E. B. Hanna, F. Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088 , 2024.</div>
      <div class="translation">A. Q. Jiang、A. Sablayrolles、A. Roux、A. Mensch、B. Savary、C. Bamford、D. S. Chaplot、D. d. l. Casas、E. B. Hanna、F. Bressand 等人。专家混合模型。arXiv 预印本 arXiv:2401.04088，2024年。</div>
    </div>
    
    <div class="translation-pair">
      <div class="original">J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361 , 2020.</div>
      <div class="translation">J. Kaplan、S. McCandlish、T. Henighan、T. B. Brown、B. Chess、R. Child、S. Gray、A. Radford、J. Wu 和 D. Amodei。神经语言模型的缩放定律。arXiv 预印本 arXiv:2001.08361，2020年。</div>
    </div>
    
    <div class="translation-pair">
      <div class="original">D. Lee, C. Kim, S. Kim, M. Cho, and W.-S. Han. Autoregressive image generation using residual quantization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 11523–11532, 2022.</div>
      <div class="translation">D. Lee、C. Kim、S. Kim、M. Cho 和 W.-S. Han。使用残差量化的自回归图像生成。发表于《IEEE/CVF计算机视觉与模式识别会议论文集》，第11523–11532页，2022年。</div>
    </div>
    
    <div class="translation-pair">
      <div class="original">J. Li, D. Li, S. Savarese, and S. Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning , pages 19730–19742. PMLR, 2023.</div>
      <div class="translation">J. Li、D. Li、S. Savarese 和 S. Hoi。BLIP-2：使用冻结图像编码器和大型语言模型引导语言-图像预训练。发表于《国际机器学习会议》，第19730–19742页。PMLR，2023年。</div>
    </div>
  </div>
  
  <div class="section">
    <h2>摘要总结</h2>
    <p>文本是“OneRec Technical Report”的参考文献部分，共列出15篇研究论文的引用。核心内容概括为：这些论文覆盖了机器学习、自然语言处理（NLP）和推荐系统三大领域，重点主题包括（1）语言模型基础：如词熵（<span class="term">word entropy</span>）计算和缩放定律（<span class="term">scaling laws</span>），探讨模型效率；（2）推荐算法：如个性化多目标集成排序（<span class="term">personalized multi-objective ensemble sort</span>）、宽深学习（<span class="term">wide & deep learning</span>）和CTR预测（<span class="term">CTR prediction</span>），强调用户行为建模；（3）高效模型设计：如稀疏专家模型（<span class="term">sparse expert models</span>）、小型语言模型（<span class="term">small language models</span>）和自回归生成（<span class="term">autoregressive generation</span>），聚焦资源优化。所有引用来自arXiv预印本（2016-2025年）和顶级会议（如ACM SIGKDD、IEEE/CVF CVPR），支持OneRec报告在推荐系统中的创新应用。</p>
  </div>
  
  <div class="section">
    <h2>术语识别</h2>
    <p>识别文本中的关键术语，基于论文标题和上下文给出详细解释。所有术语以<span class="term">英文术语</span>形式高亮显示。</p>
    <ul class="term-list">
      <li><span class="term">Word entropy</span>（词熵）：衡量自然语言中单词分布的不确定性或信息量，用于量化语言的复杂性和预测难度。计算公式常基于香农熵，值越高表示语言越不可预测。</li>
      <li><span class="term">Personalized multi-objective ensemble sort</span>（个性化多目标集成排序）：一种推荐系统算法，通过集成多个模型和帕累托优化（Pareto optimization）平衡多个目标（如相关性、多样性），实现个性化排序。</li>
      <li><span class="term">Wide & deep learning</span>（宽深学习）：结合宽线性模型（处理特征交互）和深度神经网络（捕捉复杂模式）的混合架构，常用于推荐系统以提高预测准确性。</li>
      <li><span class="term">Sparse expert models</span>（稀疏专家模型）：深度学习模型，使用多个“专家”子网络，但每次推理仅激活少数专家，以提高计算效率；常见于大型语言模型（如Mixture of Experts）。</li>
      <li><span class="term">Scaling laws</span>（缩放定律）：描述神经网络性能如何随模型大小、数据量和计算资源变化的经验规律，用于指导高效模型训练。</li>
      <li><span class="term">Small language models</span>（小型语言模型）：参数规模较小的语言模型（如MiniCPM），通过优化训练策略（如可扩展训练）在有限资源下保持高性能。</li>
      <li><span class="term">CTR prediction</span>（CTR预测）：点击率预测（Click-Through Rate Prediction），推荐系统中的关键任务，预测用户点击广告或内容的概率，常基于深度模型（如DeepFM）。</li>
      <li><span class="term">Autoregressive image generation</span>（自回归图像生成）：使用自回归模型（如RNN或Transformer）逐步生成图像的计算机视觉方法，依赖残差量化（residual quantization）压缩表示。</li>
      <li><span class="term">Bootstrapping language-image pre-training</span>（引导语言-图像预训练）：多模态学习方法，结合冻结的图像编码器和大型语言模型进行预训练（如BLIP-2），减少计算开销。</li>
      <li><span class="term">Mixture of Experts</span>（专家混合模型）：一种稀疏专家模型架构，多个专家网络协同工作，但每个输入仅路由到少数专家，提升模型容量和效率。</li>
    </ul>
  </div>
</body>
</html>
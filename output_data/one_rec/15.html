<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <title>OneRecæŠ€æœ¯æŠ¥å‘Šåˆ†æ</title>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style>
        body { font-family: 'Segoe UI', Arial, sans-serif; line-height: 1.6; }
        .original { 
            background-color: #f5f5f5; 
            border: 1px solid #ddd; 
            padding: 15px; 
            margin-bottom: 5px; 
            border-radius: 4px;
        }
        .translation { 
            background-color: #e8f5e9; 
            border: 1px solid #4caf50; 
            padding: 15px; 
            margin-bottom: 20px; 
            border-radius: 4px;
        }
        .figure { 
            background-color: #fffde7; 
            padding: 15px; 
            margin: 15px 0; 
            border-left: 4px solid #ffd600;
        }
        .term { 
            color: #d32f2f; 
            font-weight: bold; 
        }
        section { margin-bottom: 30px; }
        h2 { border-bottom: 2px solid #eee; padding-bottom: 5px; }
        li { margin-bottom: 8px; }
    </style>
</head>
<body>

<h1>OneRecæŠ€æœ¯æŠ¥å‘Šåˆ†æ</h1>

<!-- å†…å®¹ç¿»è¯‘ -->
<section>
    <h2>ğŸ“– å†…å®¹ç¿»è¯‘</h2>
    
    <div class="original">
        <strong>OneRec Technical Report</strong>
        <h3>4.2. Scaling</h3>
        <h4>4.2.1. Training Scaling</h4>
        <p>Parameters Scaling The OneRec series includes models of varying sizes: OneRec-0.015B, OneRec-0.121B, OneRec-0.935B, and OneRec-2.633B, as detailed in Table 1. We investigated the impact of model parameter count on performance. Figure 9 illustrates the loss curves for these models, demonstrating a clear scaling trend where larger models achieve lower loss as training progresses. This indicates a strong capability for performance improvement with increased model size.</p>
    </div>
    <div class="translation">
        <strong>OneRecæŠ€æœ¯æŠ¥å‘Š</strong>
        <h3>4.2. æ‰©å±•æ€§</h3>
        <h4>4.2.1. è®­ç»ƒæ‰©å±•</h4>
        <p><span class="term">å‚æ•°æ‰©å±•ï¼ˆParameters Scalingï¼‰</span> OneRecç³»åˆ—åŒ…å«ä¸åŒè§„æ¨¡çš„æ¨¡å‹ï¼šOneRec-0.015Bã€OneRec-0.121Bã€OneRec-0.935Bå’ŒOneRec-2.633Bï¼ˆè¯¦è§è¡¨1ï¼‰ã€‚æˆ‘ä»¬ç ”ç©¶äº†æ¨¡å‹å‚æ•°é‡å¯¹æ€§èƒ½çš„å½±å“ã€‚å›¾9å±•ç¤ºäº†è¿™äº›æ¨¡å‹çš„æŸå¤±æ›²çº¿ï¼Œå‘ˆç°å‡ºæ˜ç¡®çš„æ‰©å±•è¶‹åŠ¿ï¼šéšç€è®­ç»ƒè¿›è¡Œï¼Œæ›´å¤§è§„æ¨¡çš„æ¨¡å‹è·å¾—æ›´ä½çš„æŸå¤±å€¼ã€‚è¿™è¡¨æ˜é€šè¿‡å¢åŠ æ¨¡å‹è§„æ¨¡å¯æ˜¾è‘—æå‡æ€§èƒ½ã€‚</p>
    </div>
    
    <div class="original">
        <p>Regarding the influence of training data size, our experiments show that performance converges rapidly within the initial approximately 10 billion samples. While the rate of improvement diminishes significantly beyond this point, performance does not completely plateau and continues to benefit, albeit more slowly, from additional data (i.e., beyond 100 billion samples). This suggests that while substantial gains are achieved early in training, further, more gradual improvements are possible with larger datasets.</p>
    </div>
    <div class="translation">
        <p>å…³äº<span class="term">è®­ç»ƒæ•°æ®è§„æ¨¡ï¼ˆtraining data sizeï¼‰</span>çš„å½±å“ï¼Œå®éªŒè¡¨æ˜æ€§èƒ½åœ¨åˆå§‹çº¦100äº¿æ ·æœ¬å†…å¿«é€Ÿæ”¶æ•›ã€‚è¶…è¿‡æ­¤è§„æ¨¡åæ”¹è¿›é€Ÿç‡æ˜¾è‘—ä¸‹é™ï¼Œä½†æ€§èƒ½å¹¶æœªå®Œå…¨åœæ»ï¼Œä»èƒ½ä»æ›´å¤šæ•°æ®ä¸­æŒç»­è·ç›Šï¼ˆå¦‚è¶…è¿‡1000äº¿æ ·æœ¬ï¼‰ï¼Œå°½ç®¡é€Ÿåº¦è¾ƒæ…¢ã€‚è¿™è¡¨æ˜è®­ç»ƒåˆæœŸå¯è·å¾—æ˜¾è‘—æ”¶ç›Šï¼Œè€Œæ›´å¤§æ•°æ®é›†ä»èƒ½å¸¦æ¥æ¸è¿›å¼æ”¹è¿›ã€‚</p>
    </div>
    
    <div class="figure">
        <strong>Figure 9</strong> | Comparison of loss curves for different OneRec model sizes, showing loss scaling with training samples.
    </div>
    <div class="figure">
        <strong>å›¾9</strong> | ä¸åŒè§„æ¨¡OneRecæ¨¡å‹çš„æŸå¤±æ›²çº¿å¯¹æ¯”ï¼Œå±•ç¤ºæŸå¤±éšè®­ç»ƒæ ·æœ¬çš„æ‰©å±•è¶‹åŠ¿ã€‚
    </div>
    
    <div class="original">
        <p>As model parameters scale up, load balancing among experts becomes a critical issue. Uneven expert utilization can lead to training inefficiency and suboptimal performance. We adopt DeepSeekâ€™s loss-free load balancing strategy (Liu et al., 2024), which maintains expert utilization balance without introducing additional loss terms. With this strategy, we observe a loss reduction of 0.2, demonstrating its effectiveness in improving convergence for scaled OneRec models.</p>
    </div>
    <div class="translation">
        <p>éšç€æ¨¡å‹å‚æ•°æ‰©å±•ï¼Œ<span class="term">ä¸“å®¶é—´è´Ÿè½½å‡è¡¡ï¼ˆload balancing among expertsï¼‰</span>æˆä¸ºå…³é”®é—®é¢˜ã€‚ä¸“å®¶åˆ©ç”¨ç‡ä¸å‡è¡¡ä¼šå¯¼è‡´è®­ç»ƒæ•ˆç‡ä½ä¸‹å’Œæ¬¡ä¼˜æ€§èƒ½ã€‚æˆ‘ä»¬é‡‡ç”¨DeepSeekçš„<span class="term">æ— æŸå¤±è´Ÿè½½å‡è¡¡ç­–ç•¥ï¼ˆloss-free load balancing strategyï¼‰</span>ï¼ˆLiuç­‰ï¼Œ2024ï¼‰ï¼Œè¯¥ç­–ç•¥åœ¨ä¸å¼•å…¥é¢å¤–æŸå¤±é¡¹çš„æƒ…å†µä¸‹ç»´æŒä¸“å®¶åˆ©ç”¨ç‡å¹³è¡¡ã€‚åº”ç”¨æ­¤ç­–ç•¥åï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°æŸå¤±é™ä½0.2ï¼Œè¯æ˜å…¶èƒ½æœ‰æ•ˆæå‡æ‰©å±•ç‰ˆOneRecæ¨¡å‹çš„æ”¶æ•›æ€§ã€‚</p>
    </div>
    
    <div class="original">
        <p>Beyond parameter scaling, we conduct additional experiments to validate the effectiveness of scaling across other key dimensions using our 0.935B model. These experiments encompass feature scaling (examining the impact of comprehensive feature engineering), codebook scaling (investigating the effect of vocabulary size expansion), and inference scaling (analyzing the influence of beam search parameters). Each dimension demonstrates distinct scaling behaviors and provides valuable insights for future model optimization.</p>
    </div>
    <div class="translation">
        <p>é™¤å‚æ•°æ‰©å±•å¤–ï¼Œæˆ‘ä»¬ä½¿ç”¨0.935Bæ¨¡å‹è¿›è¡Œäº†é¢å¤–å®éªŒï¼ŒéªŒè¯å…¶ä»–å…³é”®ç»´åº¦çš„æ‰©å±•æ•ˆæœï¼š<br>
        â€¢ <span class="term">ç‰¹å¾æ‰©å±•ï¼ˆfeature scalingï¼‰</span>ï¼šæ£€éªŒç»¼åˆç‰¹å¾å·¥ç¨‹çš„å½±å“<br>
        â€¢ <span class="term">ç æœ¬æ‰©å±•ï¼ˆcodebook scalingï¼‰</span>ï¼šç ”ç©¶è¯æ±‡è¡¨è§„æ¨¡æ‰©å¤§çš„æ•ˆæœ<br>
        â€¢ <span class="term">æ¨ç†æ‰©å±•ï¼ˆinference scalingï¼‰</span>ï¼šåˆ†ææŸæœç´¢å‚æ•°çš„å½±å“<br>
        æ¯ä¸ªç»´åº¦å‡å±•ç°å‡ºç‹¬ç‰¹çš„æ‰©å±•è¡Œä¸ºï¼Œä¸ºæœªæ¥æ¨¡å‹ä¼˜åŒ–æä¾›äº†å®è´µæ´è§ã€‚</p>
    </div>
</section>

<!-- å†…å®¹ç†è§£ -->
<section>
    <h2>ğŸ§  å†…å®¹ç†è§£</h2>
    <p>æœ¬æ®µç³»ç»Ÿç ”ç©¶äº†OneRecæ¨èç³»ç»Ÿæ¨¡å‹çš„æ‰©å±•æ€§ï¼ˆScalingï¼‰ç‰¹æ€§ï¼š</p>
    <ol>
        <li><strong>å‚æ•°æ‰©å±•è§„å¾‹</strong>ï¼šæ¨¡å‹è§„æ¨¡ï¼ˆ0.015Bâ†’2.633Bï¼‰ä¸æ€§èƒ½å‘ˆæ­£ç›¸å…³ï¼Œæ›´å¤§æ¨¡å‹åœ¨è®­ç»ƒä¸­æ›´å¿«é™ä½æŸå¤±å€¼</li>
        <li><strong>æ•°æ®è§„æ¨¡æ•ˆåº”</strong>ï¼šè®­ç»ƒæ•°æ®åœ¨100äº¿æ ·æœ¬å†…æ”¶ç›Šæ˜¾è‘—ï¼Œè¶…è¿‡1000äº¿æ ·æœ¬åè¿›å…¥è¾¹é™…æ”¶ç›Šé€’å‡é˜¶æ®µ</li>
        <li><strong>è´Ÿè½½å‡è¡¡åˆ›æ–°</strong>ï¼šé‡‡ç”¨æ— æŸå¤±è´Ÿè½½å‡è¡¡ç­–ç•¥è§£å†³ä¸“å®¶æ¨¡å‹æ‰©å±•æ—¶çš„åˆ©ç”¨ç‡ä¸å‡é—®é¢˜ï¼Œå®ç°0.2çš„æŸå¤±é™ä½</li>
        <li><strong>å¤šç»´åº¦æ‰©å±•</strong>ï¼šåœ¨ç‰¹å¾å·¥ç¨‹ã€è¯æ±‡è¡¨å¤§å°ã€æ¨ç†å‚æ•°ç­‰ç»´åº¦éªŒè¯æ‰©å±•æ•ˆæœï¼Œæ­ç¤ºå„ç»´åº¦çš„ç‹¬ç‰¹ä¼˜åŒ–è·¯å¾„</li>
    </ol>
    <p>æ ¸å¿ƒè®¤çŸ¥ï¼šæ¨¡å‹æ‰©å±•éœ€ç»¼åˆè€ƒè™‘å‚æ•°é‡ã€æ•°æ®é‡ã€æ¶æ„ä¼˜åŒ–ï¼ˆè´Ÿè½½å‡è¡¡ï¼‰åŠå¤šç»´åº¦ååŒï¼Œæ‰èƒ½å®ç°æœ€ä¼˜æ€§èƒ½æå‡ã€‚</p>
</section>

<!-- æ‘˜è¦æ€»ç»“ -->
<section>
    <h2>ğŸ“Œ æ‘˜è¦æ€»ç»“</h2>
    <p>æœ¬èŠ‚æ ¸å¿ƒç ”ç©¶OneRecæ¨èç³»ç»Ÿçš„æ‰©å±•æ€§ç‰¹æ€§ï¼š</p>
    <ul>
        <li>âœ… <strong>å‚æ•°æ‰©å±•æ€§</strong>ï¼šæ¨¡å‹è§„æ¨¡ï¼ˆ0.015B~2.633Bï¼‰ä¸æ€§èƒ½æ­£ç›¸å…³ï¼Œå¤§æ¨¡å‹æŸå¤±ä¸‹é™æ›´å¿«</li>
        <li>ğŸ“Š <strong>æ•°æ®è§„æ¨¡æ•ˆåº”</strong>ï¼š100äº¿æ ·æœ¬å†…å¿«é€Ÿæ”¶æ•›ï¼Œ1000äº¿æ ·æœ¬ä»¥ä¸Šä»å…·æ¸è¿›æ”¶ç›Š</li>
        <li>âš–ï¸ <strong>è´Ÿè½½å‡è¡¡çªç ´</strong>ï¼šé‡‡ç”¨æ— æŸå¤±è´Ÿè½½å‡è¡¡ç­–ç•¥ï¼Œæ˜¾è‘—æå‡ä¸“å®¶æ¨¡å‹æ•ˆç‡ï¼ˆæŸå¤±â†“0.2ï¼‰</li>
        <li>ğŸ§© <strong>å¤šç»´åº¦éªŒè¯</strong>ï¼šç‰¹å¾/ç æœ¬/æ¨ç†æ‰©å±•å‡å±•ç°ç‹¬ç‰¹ä¼˜åŒ–è·¯å¾„ï¼Œä¸ºæ¨¡å‹è®¾è®¡æä¾›æ–°æ´è§</li>
    </ul>
    <p>ç»“è®ºï¼šé€šè¿‡å‚æ•°ã€æ•°æ®ã€æ¶æ„çš„ååŒæ‰©å±•ï¼Œå¯æ˜¾è‘—æå‡æ¨èç³»ç»Ÿæ€§èƒ½ï¼Œä¸”å„æ‰©å±•ç»´åº¦å­˜åœ¨å·®å¼‚åŒ–ä¼˜åŒ–ç­–ç•¥ã€‚</p>
</section>

<!-- æœ¯è¯­è¯†åˆ« -->
<section>
    <h2>ğŸ” æœ¯è¯­è¯†åˆ«</h2>
    <ul>
        <li><span class="term">Parameters Scalingï¼ˆå‚æ•°æ‰©å±•ï¼‰</span>ï¼šé€šè¿‡å¢åŠ æ¨¡å‹å‚æ•°é‡ï¼ˆå¦‚ä»0.015Båˆ°2.633Bï¼‰æå‡æ¨¡å‹å®¹é‡çš„æ–¹æ³•ï¼Œå®éªŒè¯æ˜æ›´å¤§æ¨¡å‹èƒ½æ›´å¿«é™ä½è®­ç»ƒæŸå¤±</li>
        <li><span class="term">Loss Curvesï¼ˆæŸå¤±æ›²çº¿ï¼‰</span>ï¼šæ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­æŸå¤±å€¼éšæ—¶é—´/è¿­ä»£æ¬¡æ•°çš„å˜åŒ–å›¾ç¤ºï¼ˆFigure 9ï¼‰ï¼Œç”¨äºå¯è§†åŒ–æ‰©å±•è¶‹åŠ¿</li>
        <li><span class="term">Load Balancing among Expertsï¼ˆä¸“å®¶é—´è´Ÿè½½å‡è¡¡ï¼‰</span>ï¼šå¤šä¸“å®¶æ¨¡å‹æ¶æ„ä¸­ï¼Œç¡®ä¿å„ä¸“å®¶æ¨¡å—åˆ©ç”¨ç‡å‡è¡¡çš„æŠ€æœ¯ï¼Œé¿å…éƒ¨åˆ†ä¸“å®¶è¿‡è½½å¯¼è‡´çš„æ•ˆç‡ç“¶é¢ˆ</li>
        <li><span class="term">Loss-free Load Balancing Strategyï¼ˆæ— æŸå¤±è´Ÿè½½å‡è¡¡ç­–ç•¥ï¼‰</span>ï¼šDeepSeekæå‡ºçš„åˆ›æ–°æ–¹æ³•ï¼ˆLiuç­‰ï¼Œ2024ï¼‰ï¼Œåœ¨ä¸å¢åŠ é¢å¤–æŸå¤±é¡¹çš„å‰æä¸‹ä¼˜åŒ–ä¸“å®¶åˆ©ç”¨ç‡</li>
        <li><span class="term">Feature Scalingï¼ˆç‰¹å¾æ‰©å±•ï¼‰</span>ï¼šé€šè¿‡å¢å¼ºç‰¹å¾å·¥ç¨‹ï¼ˆå¦‚ç‰¹å¾ç»„åˆã€é«˜é˜¶ç‰¹å¾ï¼‰æå‡æ¨¡å‹è¡¨è¾¾èƒ½åŠ›çš„æ‰©å±•ç»´åº¦</li>
        <li><span class="term">Codebook Scalingï¼ˆç æœ¬æ‰©å±•ï¼‰</span>ï¼šæ‰©å¤§åµŒå…¥å±‚è¯æ±‡è¡¨è§„æ¨¡ä»¥å¢å¼ºæ¨¡å‹è¯­ä¹‰æ•æ‰èƒ½åŠ›çš„æ‰©å±•æ–¹æ³•</li>
        <li><span class="term">Inference Scalingï¼ˆæ¨ç†æ‰©å±•ï¼‰</span>ï¼šè°ƒæ•´æ¨ç†é˜¶æ®µå‚æ•°ï¼ˆå¦‚æŸæœç´¢å®½åº¦ï¼‰å¹³è¡¡é¢„æµ‹è´¨é‡ä¸è®¡ç®—æ•ˆç‡çš„æŠ€æœ¯</li>
        <li><span class="term">Scaling Trendï¼ˆæ‰©å±•è¶‹åŠ¿ï¼‰</span>ï¼šæ¨¡å‹æ€§èƒ½éšè§„æ¨¡ï¼ˆå‚æ•°/æ•°æ®/è®¡ç®—ï¼‰å¢é•¿çš„è§„å¾‹æ€§å˜åŒ–æ¨¡å¼</li>
    </ul>
</section>

</body>
</html>
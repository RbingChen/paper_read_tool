<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>OneRec技术报告分析</title>
  <style>
    body { font-family: Arial, sans-serif; line-height: 1.6; margin: 20px; }
    .section { margin-bottom: 30px; }
    h2 { color: #333; border-bottom: 2px solid #666; padding-bottom: 5px; }
    h3 { color: #444; margin-top: 15px; }
    .original { background-color: lightgrey; border: 1px solid grey; padding: 15px; margin-bottom: 10px; border-radius: 5px; }
    .translation { background-color: lightgreen; border: 1px solid green; padding: 15px; border-radius: 5px; }
    .illustration { background-color: yellow; padding: 15px; margin: 15px 0; border-radius: 5px; text-align: center; }
    .term { color: red; font-weight: bold; }
    table { width: 100%; border-collapse: collapse; margin: 10px 0; }
    th, td { border: 1px solid #ddd; padding: 8px; text-align: center; }
    th { background-color: #f2f2f2; }
  </style>
</head>
<body>
  <h1>OneRec技术报告分析</h1>
  
  <!-- 内容理解部分 -->
  <div class="section">
    <h2>内容理解</h2>
    <p>该文本是OneRec技术报告的一部分，重点讨论了强化学习（Reinforcement Learning）模型在推荐系统中的应用和优化。核心内容包括：通过不同参考模型（Reference Model）评估强化学习性能（如表9所示），在快手（Kuaishou）和快手极速版（Kuaishou Lite）场景中验证P-Score奖励机制的效果（如表10所示），以及引入格式正则化（Format Regularization）来解决模型输出合法性（Output Legality）问题。实验表明，强化学习能显著提升用户指标（如观看时间和App停留时间），但需结合格式奖励（Format Reward）来确保输出稳定性。最后，文本简要提及工业场景对齐（Industrial Scenario Alignment），针对快手平台的内容质量问题，但内容不完整。整体上，报告强调奖励设计在强化学习中的关键作用，以提升推荐系统的用户体验和可扩展性。</p>
  </div>
  
  <!-- 内容翻译部分 -->
  <div class="section">
    <h2>内容翻译</h2>
    
    <!-- 段落1: 标题和Table 9 -->
    <div class="original">
      OneRec Technical Report<br>
      Reference Model vtr Watch time App Stay Time Video View1<br>
      Pre-trained Model 0.2262 +5.35% +1.51% -13.51%<br>
      Current Policy Model 0.2389 +6.19% +1.56% -13.89%<br>
      Relative Impr. +5.61% +0.79% +0.04% -13.89%<br>
      Table 9|Performance of reinforcement learning with different reference models.
    </div>
    <div class="translation">
      OneRec技术报告<br>
      参考模型 vtr 观看时间 App停留时间 视频观看量1<br>
      预训练模型 0.2262 +5.35% +1.51% -13.51%<br>
      当前策略模型 0.2389 +6.19% +1.56% -13.89%<br>
      相对改进 +5.61% +0.79% +0.04% -13.89%<br>
      表9|使用不同参考模型的强化学习性能。
    </div>
    
    <!-- 表格9作为图示 -->
    <div class="illustration">
      <strong>Table 9: Performance of reinforcement learning with different reference models.</strong><br>
      <table>
        <tr>
          <th>Reference Model</th>
          <th>vtr</th>
          <th>Watch time</th>
          <th>App Stay Time</th>
          <th>Video View1</th>
        </tr>
        <tr>
          <td>Pre-trained Model</td>
          <td>0.2262</td>
          <td>+5.35%</td>
          <td>+1.51%</td>
          <td>-13.51%</td>
        </tr>
        <tr>
          <td>Current Policy Model</td>
          <td>0.2389</td>
          <td>+6.19%</td>
          <td>+1.56%</td>
          <td>-13.89%</td>
        </tr>
        <tr>
          <td>Relative Impr.</td>
          <td>+5.61%</td>
          <td>+0.79%</td>
          <td>+0.04%</td>
          <td>-13.89%</td>
        </tr>
      </table>
    </div>
    
    <!-- 段落2: 描述强化学习实验 -->
    <div class="original">
      sample generation and employing the current policy model as the reference model. We examine the impact of RL in two scenarios, including Kuaishou and Kuaishou Lite, with the results summarized in Table 1. From the table, we can conclude that in both scenarios, P-Score significantly improves App Stay Time and Watch Time while also increasing Video View, indicating an enhancement in the overall user recommendation experience.
    </div>
    <div class="translation">
      样本生成并采用当前策略模型作为参考模型。我们检验了强化学习（RL）在两种场景（包括快手和快手极速版）中的影响，结果总结在表1中。从表中可以得出结论：在两种场景下，P-Score显著提升了App停留时间和观看时间，同时也增加了视频观看量，这表明整体用户推荐体验得到了改善。
    </div>
    
    <!-- 段落3: Table 10 -->
    <div class="original">
      Scenario Watch time App Stay Time Video View<br>
      Kuaishou +0.21% +0.26% +0.17%<br>
      Kuaishou Lite +0.71% +0.22% +0.35%<br>
      Table 10|The relative improvement of OneRec with P-Score Reward compared to without it in the Kuaishou and Kuaishou Lite scenarios.
    </div>
    <div class="translation">
      场景 观看时间 App停留时间 视频观看量<br>
      快手 +0.21% +0.26% +0.17%<br>
      快手极速版 +0.71% +0.22% +0.35%<br>
      表10|在快手和快手极速版场景中，OneRec使用P-Score奖励相比不使用时的相对改进。
    </div>
    
    <!-- 表格10作为图示 -->
    <div class="illustration">
      <strong>Table 10: The relative improvement of OneRec with P-Score Reward compared to without it in the Kuaishou and Kuaishou Lite scenarios.</strong><br>
      <table>
        <tr>
          <th>Scenario</th>
          <th>Watch time</th>
          <th>App Stay Time</th>
          <th>Video View</th>
        </tr>
        <tr>
          <td>Kuaishou</td>
          <td>+0.21%</td>
          <td>+0.26%</td>
          <td>+0.17%</td>
        </tr>
        <tr>
          <td>Kuaishou Lite</td>
          <td>+0.71%</td>
          <td>+0.22%</td>
          <td>+0.35%</td>
        </tr>
      </table>
    </div>
    
    <!-- 段落4: 4.3.2标题和描述 -->
    <div class="original">
      4.3.2. Generation Format Regularization<br>
      In this section, we conduct experiments to verify the effectiveness of format reward. As mentioned in Section 2.4.2, after incorporating reinforcement learning into the pre-trained model, the legality of the model’s output significantly drops to below 50% due to the squeezing effect. This means that more than half of the generated semantic IDs do not correspond to actual video IDs, which is detrimental to the stability of recommendations and the scalability of inference. We evaluate the impact of format reward by comparing two sample selection methods for computing format loss: (1) selecting the top-5 highest-probability samples from 128 generated candidates, and (2) randomly selecting 5 samples.
    </div>
    <div class="translation">
      4.3.2. 生成格式正则化<br>
      在本节中，我们进行实验以验证格式奖励的有效性。如第2.4.2节所述，在将强化学习整合到预训练模型后，由于挤压效应，模型输出的合法性显著下降至50%以下。这意味着超过一半的生成语义ID不对应于实际视频ID，这对推荐的稳定性和推理的可扩展性有害。我们通过比较两种样本选择方法来评估格式奖励的影响：(1) 从128个生成候选样本中选择概率最高的前5个样本，(2) 随机选择5个样本。
    </div>
    
    <!-- 段落5: 实验描述 -->
    <div class="original">
      Figure 12 illustrates their effects on output legality. The left figure shows legality rates across all 128 generated samples, while the right panel focuses on the selected samples. Without format rewards, baseline legality remains below 50%. The Top-k Selection approach produces an interesting pattern: while overall legality initially rises then falls, the selected samples rapidly achieve 100% legality, suggesting the model learns to generate legal outputs only within the top-ranked subset. In contrast, Random Selection presents a more challenging learning objective, yet drives steady improvement - ultimately reaching 95% legality without showing a decline.
    </div>
    <div class="translation">
      图12展示了它们对输出合法性的影响。左图显示了所有128个生成样本的合法性率，而右图则聚焦于选定的样本。在没有格式奖励的情况下，基线合法性保持在50%以下。Top-k选择方法产生了一个有趣的模式：尽管整体合法性先上升后下降，但选定的样本迅速达到100%合法性，表明模型学会了仅在排名靠前的子集中生成合法输出。相比之下，随机选择提出了更具挑战性的学习目标，但驱动了稳定改进——最终达到95%合法性，且未出现下降。
    </div>
    
    <!-- 段落6: 4.3.3标题和描述（截断） -->
    <div class="original">
      4.3.3. Industrial Scenario Alignment<br>
      In this section, we present a practical example of using reinforcement learning to address industrial challenges. On the Kuaishou platform, viral content farms represent a significant portion of content creators, primarily producing repurposed and clipped videos with inconsistent quality. While OneRec 21
    </div>
    <div class="translation">
      4.3.3. 工业场景对齐<br>
      在本节中，我们展示了一个使用强化学习解决工业挑战的实际例子。在快手平台上，病毒式内容农场占据了内容创作者的很大一部分，主要生产重复利用和剪辑的视频，质量参差不齐。而OneRec 21（注：文本在此处截断，内容不完整）。
    </div>
    
    <!-- 段落7: 后续描述 -->
    <div class="original">
      Notably, format reward integration yields benefits beyond legality alone. Online metrics demonstrate substantial gains: +0.13% in APP Stay Time and +0.30% in Watch Time. This experimental case not only validates the format reward mechanism but also highlights the critical role of careful reward design in reinforcement learning systems.
    </div>
    <div class="translation">
      值得注意的是，格式奖励的整合带来了超越合法性的益处。在线指标显示出显著提升：App停留时间增加0.13%，观看时间增加0.30%。这个实验案例不仅验证了格式奖励机制，还突显了在强化学习系统中精心设计奖励的关键作用。
    </div>
  </div>
  
  <!-- 摘要总结部分 -->
  <div class="section">
    <h2>摘要总结</h2>
    <p>该文本摘要总结了OneRec技术报告的核心内容：通过强化学习（Reinforcement Learning）优化推荐系统，重点包括（1）使用不同参考模型（Reference Model）评估性能，显示当前策略模型优于预训练模型；（2）在快手和快手极速版场景中，P-Score奖励机制显著提升了用户指标（如观看时间、App停留时间和视频观看量）；（3）引入格式正则化（Format Regularization）解决输出合法性（Output Legality）问题，实验证明Top-k和随机选择方法能有效提升合法性至95-100%，并改善在线指标；（4）简要讨论工业场景对齐（Industrial Scenario Alignment）以处理内容质量问题。整体强调奖励设计在强化学习中的重要性，以增强推荐稳定性和用户体验。</p>
  </div>
  
  <!-- 术语识别部分 -->
  <div class="section">
    <h2>术语识别</h2>
    <p>文本中识别出的关键术语如下（术语以<strong class="term">红色粗体</strong>显示，并包含英文原文）：</p>
    <ul>
      <li><strong class="term">Reinforcement Learning (RL)</strong>：强化学习，一种机器学习方法，代理通过与环境交互并根据奖励信号学习最优策略。在文本中，用于优化推荐系统的决策过程，通过奖励机制提升用户指标。</li>
      <li><strong class="term">Reference Model</strong>：参考模型，在强化学习中用作基准的模型，用于比较和评估当前策略的性能。文本中对比了预训练模型和当前策略模型的效果。</li>
      <li><strong class="term">Pre-trained Model</strong>：预训练模型，在大量数据上预先训练的模型，作为强化学习的起点。文本中显示其性能较低（如vtr 0.2262）。</li>
      <li><strong class="term">Current Policy Model</strong>：当前策略模型，强化学习中正在优化的策略模型，性能优于预训练模型（如vtr 0.2389）。</li>
      <li><strong class="term">P-Score</strong>：一种评分或奖励机制，在强化学习中用于量化推荐质量。文本中表明它能显著提升App停留时间和观看时间。</li>
      <li><strong class="term">Generation Format Regularization</strong>：生成格式正则化，技术用于约束模型输出格式，确保语义ID符合实际视频ID。文本中通过格式奖励（Format Reward）实现，以解决输出合法性问题。</li>
      <li><strong class="term">Output Legality</strong>：输出合法性，指模型生成的语义ID是否有效对应真实视频ID。文本中强调其重要性，低于50%会影响推荐稳定性。</li>
      <li><strong class="term">Format Reward</strong>：格式奖励，强化学习中的自定义奖励项，用于惩罚非法输出。文本中比较了Top-k和随机选择方法，证明其能提升合法性至95-100%。</li>
      <li><strong class="term">Industrial Scenario Alignment</strong>：工业场景对齐，指将强化学习应用于实际工业问题（如快手平台的内容质量挑战）。文本中简要提及，但内容不完整。</li>
      <li><strong class="term">Squeezing Effect</strong>：挤压效应，描述强化学习整合后输出合法性下降的现象，归因于模型优化过程中的分布偏移。</li>
    </ul>
  </div>
</body>
</html>
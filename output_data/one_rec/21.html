<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>OneRecæŠ€æœ¯æŠ¥å‘Šåˆ†æ</title>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<style>
  body { font-family: Arial, sans-serif; line-height: 1.6; }
  .original { background-color: #f5f5f5; border: 1px solid #ccc; padding: 15px; margin-bottom: 10px; }
  .translation { background-color: #e8f5e9; border: 1px solid #4caf50; padding: 15px; margin-bottom: 20px; }
  .figure { background-color: #fffde7; padding: 15px; margin: 20px 0; text-align: center; font-style: italic; }
  .term { color: red; font-weight: bold; }
  .formula-container { text-align: center; margin: 20px 0; }
  .formula-number { display: block; font-style: italic; }
  section { margin-bottom: 30px; }
  h2 { color: #2c3e50; border-bottom: 2px solid #3498db; padding-bottom: 5px; }
  ul { list-style-type: none; padding-left: 0; }
  li { margin-bottom: 15px; }
</style>
</head>
<body>

<!-- å†…å®¹ç†è§£ -->
<section>
  <h2>å†…å®¹ç†è§£</h2>
  <p>è¯¥æŠ€æœ¯æŠ¥å‘Šä¸»è¦è®¨è®ºOneRecæ¨èç³»ç»Ÿçš„ä¸¤ä¸ªå…³é”®æŠ€æœ¯ï¼š</p>
  <ol>
    <li><strong class="term">æ ¼å¼å¥–åŠ±è®­ç»ƒï¼ˆFormat Reward Trainingï¼‰</strong>ï¼šé€šè¿‡ä¸åŒé‡‡æ ·ç­–ç•¥ï¼ˆéšæœºé€‰æ‹© vs TopKé€‰æ‹©ï¼‰ä¼˜åŒ–ç”Ÿæˆå†…å®¹çš„åˆæ³•æ€§ï¼ˆLegalityï¼‰ã€‚å®éªŒè¡¨æ˜æ·»åŠ æ ¼å¼å¥–åŠ±èƒ½æ˜¾è‘—æå‡æ¨¡å‹è¾“å‡ºçš„åˆè§„æ€§ã€‚</li>
    <li><strong class="term">ç‰¹å®šå·¥ä¸šå¥–åŠ±ï¼ˆSIRï¼‰</strong>ï¼šæå‡ºç—…æ¯’å†…å®¹æŠ‘åˆ¶æœºåˆ¶ï¼Œå½“ç—…æ¯’å†…å®¹æ¯”ä¾‹è¶…è¿‡é˜ˆå€¼ğ‘“æ—¶ï¼Œé€šè¿‡æŠ‘åˆ¶å› å­ğ›¼é™ä½å…¶å¥–åŠ±å€¼ï¼Œå…¬å¼è§å¼(33)ã€‚è¯¥æ–¹æ³•åœ¨ä¿æŒæ ¸å¿ƒæŒ‡æ ‡ç¨³å®šçš„åŒæ—¶ï¼ŒæˆåŠŸé™ä½ç—…æ¯’å†…å®¹æ›å…‰ç‡9.59%ã€‚</li>
    <li><strong class="term">Tokenizerè¯„ä¼°ä½“ç³»</strong>ï¼šä½¿ç”¨é‡æ„æŸå¤±ã€ç æœ¬åˆ©ç”¨ç‡å’Œä»¤ç‰Œåˆ†å¸ƒç†µä¸‰å¤§æŒ‡æ ‡è¯„ä¼°åˆ†è¯æ•ˆæœï¼Œå®éªŒè¯æ˜RQ-Kmeansæ–¹æ³•åœ¨å„é¡¹æŒ‡æ ‡ä¸Šå‡ä¼˜äºä¼ ç»ŸRQ-VAEã€‚</li>
  </ol>
</section>

<!-- å†…å®¹ç¿»è¯‘ -->
<section>
  <h2>å†…å®¹ç¿»è¯‘</h2>
  
  <div class="original">
    OneRec Technical Report
  </div>
  <div class="translation">
    OneRecæŠ€æœ¯æŠ¥å‘Š
  </div>
  
  <div class="original">
    01.00.5Training  StepLegalityRandomSelectionTopkSelectionAdd Format RewardLegality of Generated Items
    RandomSelectionTopkSelectionAdd Format RewardLegalityof the Format Loss Samples
    Training  StepLegality01.00.5
  </div>
  <div class="translation">
    01.00.5è®­ç»ƒæ­¥æ•°åˆæ³•æ€§éšæœºé€‰æ‹©TopKé€‰æ‹©æ·»åŠ æ ¼å¼å¥–åŠ±ç”Ÿæˆé¡¹ç›®çš„åˆæ³•æ€§
    éšæœºé€‰æ‹©TopKé€‰æ‹©æ·»åŠ æ ¼å¼å¥–åŠ±æ ¼å¼æŸå¤±æ ·æœ¬çš„åˆæ³•æ€§
    è®­ç»ƒæ­¥æ•°åˆæ³•æ€§01.00.5
  </div>
  
  <div class="figure">
    <strong>Figure 12</strong> | The impact of training with format reward with samples obtained through different sampling strategies on the modelâ€™s legality.
    <p><strong>å›¾12</strong> | ä½¿ç”¨ä¸åŒé‡‡æ ·ç­–ç•¥è·å¾—çš„æ ·æœ¬è¿›è¡Œæ ¼å¼å¥–åŠ±è®­ç»ƒå¯¹æ¨¡å‹åˆæ³•æ€§çš„å½±å“</p>
  </div>
  
  <div class="original">
    demonstrates superior performance over traditional recommendation systems across multiple business metrics, 
    we observe that without proper post-filtering strategies, the exposure ratio of viral content increases significantly, 
    which may negatively impact the platformâ€™s ecosystem.
  </div>
  <div class="translation">
    è™½ç„¶åœ¨å¤šä¸ªä¸šåŠ¡æŒ‡æ ‡ä¸Šä¼˜äºä¼ ç»Ÿæ¨èç³»ç»Ÿï¼Œä½†æˆ‘ä»¬è§‚å¯Ÿåˆ°è‹¥æ— åˆé€‚çš„åè¿‡æ»¤ç­–ç•¥ï¼Œ
    <strong class="term">ç—…æ¯’å†…å®¹ï¼ˆviral contentï¼‰</strong>çš„æ›å…‰ç‡ä¼šæ˜¾è‘—å¢åŠ ï¼Œ
    å¯èƒ½å¯¹å¹³å°ç”Ÿæ€ç³»ç»Ÿäº§ç”Ÿè´Ÿé¢å½±å“ã€‚
  </div>
  
  <div class="original">
    The optimal proportion of viral content videos can be set to ğ‘“. When the proportion exceeds ğ‘“,
    we down-weight their P-score reward to suppress them while maintaining the systemâ€™s perception of
    the quality of these contents.
  </div>
  <div class="translation">
    ç—…æ¯’å†…å®¹è§†é¢‘çš„æœ€ä½³æ¯”ä¾‹å¯è®¾ä¸ºğ‘“ã€‚å½“æ¯”ä¾‹è¶…è¿‡ğ‘“æ—¶ï¼Œ
    æˆ‘ä»¬é™ä½å…¶<strong class="term">P-scoreå¥–åŠ±ï¼ˆP-score rewardï¼‰</strong>ä»¥æŠ‘åˆ¶æ›å…‰ï¼Œ
    åŒæ—¶ä¿æŒç³»ç»Ÿå¯¹è¿™äº›å†…å®¹è´¨é‡çš„æ„ŸçŸ¥ã€‚
  </div>
  
  <div class="formula-container">
    \[
    r'_i = \begin{cases} 
    r_i & \text{if } o_i \notin I_{\text{viral}} \\
    \alpha r_i & \text{if } o_i \in I_{\text{viral}}
    \end{cases}
    \]
    <span class="formula-number">(33)</span>
  </div>
  
  <div class="original">
    where ğ›¼ âˆˆ (0,1) is the suppression factor.
  </div>
  <div class="translation">
    å…¶ä¸­ğ›¼ âˆˆ (0,1)ä¸º<strong class="term">æŠ‘åˆ¶å› å­ï¼ˆsuppression factorï¼‰</strong>ã€‚
  </div>
  
  <div class="original">
    We term this approach Specific Industrial Reward (SIR). Experimental results show that SIR
    effectively reduces viral content exposure by 9.59% while maintaining stable performance on core
    metrics (Watch time and APP Stay Time). This experiment highlights OneRecâ€™s key advantage: the
    ability to achieve precise and consistent optimization through reinforcement learningâ€™s reward-shaping
    capability, a feature fundamentally unavailable in traditional recommendation systems.
  </div>
  <div class="translation">
    æˆ‘ä»¬ç§°æ­¤æ–¹æ³•ä¸º<strong class="term">ç‰¹å®šå·¥ä¸šå¥–åŠ±ï¼ˆSpecific Industrial Reward, SIRï¼‰</strong>ã€‚
    å®éªŒè¡¨æ˜SIRåœ¨ä¿æŒæ ¸å¿ƒæŒ‡æ ‡ï¼ˆè§‚çœ‹æ—¶é—´å’ŒAPPåœç•™æ—¶é—´ï¼‰ç¨³å®šçš„åŒæ—¶ï¼Œ
    æœ‰æ•ˆé™ä½ç—…æ¯’å†…å®¹æ›å…‰ç‡9.59%ã€‚è¯¥å®éªŒå‡¸æ˜¾äº†OneRecçš„å…³é”®ä¼˜åŠ¿ï¼š
    é€šè¿‡<strong class="term">å¼ºåŒ–å­¦ä¹ ï¼ˆreinforcement learningï¼‰</strong>çš„<strong class="term">å¥–åŠ±å¡‘é€ ï¼ˆreward-shapingï¼‰</strong>èƒ½åŠ›
    å®ç°ç²¾ç¡®ä¸€è‡´çš„ä¼˜åŒ–ï¼Œè¿™æ˜¯ä¼ ç»Ÿæ¨èç³»ç»Ÿæ— æ³•å…·å¤‡çš„ç‰¹æ€§ã€‚
  </div>
  
  <div class="original">
    <h3>4.4. Tokenizer</h3>
    We employ three metrics to comprehensively evaluate our tokenization method, encompassing aspects
    of accuracy, resource utilization, and distribution uniformity:
  </div>
  <div class="translation">
    <h3>4.4. åˆ†è¯å™¨</h3>
    æˆ‘ä»¬é‡‡ç”¨ä¸‰ä¸ªæŒ‡æ ‡å…¨é¢è¯„ä¼°åˆ†è¯æ–¹æ³•ï¼Œæ¶µç›–å‡†ç¡®æ€§ã€èµ„æºåˆ©ç”¨ç‡å’Œåˆ†å¸ƒå‡åŒ€æ€§ï¼š
  </div>
  
  <div class="original">
    â€¢ Reconstruction Loss : This metric assesses the accuracy with which discrete tokens reconstruct the
    original input, serving as an indicator of the modelâ€™s fidelity in preserving the input data.
  </div>
  <div class="translation">
    â€¢ <strong class="term">é‡æ„æŸå¤±ï¼ˆReconstruction Lossï¼‰</strong>ï¼šè¯„ä¼°ç¦»æ•£ä»¤ç‰Œé‡æ„åŸå§‹è¾“å…¥çš„å‡†ç¡®æ€§ï¼Œ
    ä½œä¸ºæ¨¡å‹ä¿ç•™è¾“å…¥æ•°æ®ä¿çœŸåº¦çš„æŒ‡æ ‡ã€‚
  </div>
  
  <div class="original">
    â€¢ Codebook Utilization (Zhu et al., 2024) : This metric evaluates the efficiency of vector usage
    within the codebook, reflecting how effectively the model leverages available resources to represent
    data.
  </div>
  <div class="translation">
    â€¢ <strong class="term">ç æœ¬åˆ©ç”¨ç‡ï¼ˆCodebook Utilizationï¼‰</strong> (Zhu et al., 2024)ï¼š
    è¯„ä¼°ç æœ¬å†…å‘é‡ä½¿ç”¨æ•ˆç‡ï¼Œåæ˜ æ¨¡å‹åˆ©ç”¨å¯ç”¨èµ„æºè¡¨ç¤ºæ•°æ®çš„æœ‰æ•ˆæ€§ã€‚
  </div>
  
  <div class="original">
    â€¢ Token Distribution Entropy (Bentz and Alikaniotis, 2016) : Utilizing Shannon entropy, this
    metric quantifies the uniformity of token distribution, providing insight into the diversity and
    balance of token allocation across the model.
  </div>
  <div class="translation">
    â€¢ <strong class="term">ä»¤ç‰Œåˆ†å¸ƒç†µï¼ˆToken Distribution Entropyï¼‰</strong> (Bentz and Alikaniotis, 2016)ï¼š
    åˆ©ç”¨é¦™å†œç†µé‡åŒ–ä»¤ç‰Œåˆ†å¸ƒçš„å‡åŒ€æ€§ï¼Œæ­ç¤ºæ¨¡å‹ä¸­ä»¤ç‰Œåˆ†é…çš„å¤šæ ·æ€§å’Œå¹³è¡¡æ€§ã€‚
  </div>
  
  <div class="original">
    As shown in Table 11, compared to RQ-VAE, RQ-Kmeansâ€™s reconstruction loss is reduced by
    25.18%, demonstrating superior accuracy in preserving input information. Simultaneously, RQ-
    Kmeans achieves perfect utilization (1.0000) in all three layers, indicating optimal resource efficiency
    in the codebook, while RQ-VAE shows slightly lower utilization rates in layers 2 and 3. Furthermore,
    RQ-Kmeans exhibits higher entropy values in all three layers compared to RQ-VAE, with significant im-
    provements of 6.31%, 3.50%, and 1.44% in layers 1, 2, and 3, respectively, suggesting that RQ-Kmeans
  </div>
  <div class="translation">
    å¦‚è¡¨11æ‰€ç¤ºï¼Œç›¸æ¯”<strong class="term">RQ-VAE</strong>ï¼Œ<strong class="term">RQ-Kmeans</strong>çš„é‡æ„æŸå¤±é™ä½25.18%ï¼Œ
    åœ¨ä¿ç•™è¾“å…¥ä¿¡æ¯æ–¹é¢è¡¨ç°å‡ºæ›´é«˜å‡†ç¡®æ€§ã€‚åŒæ—¶RQ-Kmeansåœ¨æ‰€æœ‰ä¸‰å±‚å‡å®ç°å®Œç¾åˆ©ç”¨ç‡(1.0000)ï¼Œ
    è¡¨æ˜ç æœ¬èµ„æºæ•ˆç‡æœ€ä¼˜ï¼Œè€ŒRQ-VAEåœ¨2ã€3å±‚åˆ©ç”¨ç‡ç•¥ä½ã€‚æ­¤å¤–ï¼Œ
    RQ-Kmeansåœ¨æ‰€æœ‰ä¸‰å±‚çš„ç†µå€¼å‡é«˜äºRQ-VAEï¼Œåœ¨1ã€2ã€3å±‚åˆ†åˆ«æ˜¾è‘—æå‡6.31%ã€3.50%å’Œ1.44%ï¼Œ
    è¡¨æ˜RQ-Kmeans
  </div>
</section>

<!-- æ‘˜è¦æ€»ç»“ -->
<section>
  <h2>æ‘˜è¦æ€»ç»“</h2>
  <p>æœ¬æŠ€æœ¯æŠ¥å‘Šæ ¸å¿ƒå†…å®¹èšç„¦äºOneRecæ¨èç³»ç»Ÿçš„ä¸‰å¤§åˆ›æ–°ï¼š</p>
  <ol>
    <li>é€šè¿‡<strong class="term">æ ¼å¼å¥–åŠ±è®­ç»ƒï¼ˆFormat Reward Trainingï¼‰</strong>æå‡å†…å®¹åˆæ³•æ€§ï¼Œä¸åŒé‡‡æ ·ç­–ç•¥ï¼ˆéšæœºé€‰æ‹© vs TopKé€‰æ‹©ï¼‰å¯¹æ¨¡å‹åˆè§„æ€§äº§ç”Ÿæ˜¾è‘—å½±å“ï¼ˆå›¾12ï¼‰</li>
    <li>æå‡º<strong class="term">ç‰¹å®šå·¥ä¸šå¥–åŠ±ï¼ˆSIRï¼‰</strong>æœºåˆ¶ï¼Œé‡‡ç”¨åŠ¨æ€å¥–åŠ±æŠ‘åˆ¶å…¬å¼ï¼ˆå¼33ï¼‰æ§åˆ¶ç—…æ¯’å†…å®¹æ›å…‰ï¼Œåœ¨ä¿æŒæ ¸å¿ƒæŒ‡æ ‡ç¨³å®šçš„åŒæ—¶é™ä½ç—…æ¯’å†…å®¹æ›å…‰ç‡9.59%</li>
    <li>å»ºç«‹Tokenizerä¸‰ç»´è¯„ä¼°ä½“ç³»ï¼ˆé‡æ„æŸå¤±/ç æœ¬åˆ©ç”¨ç‡/ä»¤ç‰Œåˆ†å¸ƒç†µï¼‰ï¼ŒéªŒè¯RQ-Kmeansæ–¹æ³•ç›¸æ¯”RQ-VAEåœ¨ä¿¡æ¯ä¿ç•™ï¼ˆ+25.18%ï¼‰ã€èµ„æºåˆ©ç”¨ï¼ˆ100%ï¼‰å’Œåˆ†å¸ƒå‡åŒ€æ€§ï¼ˆç†µå€¼æå‡1.44-6.31%ï¼‰çš„å…¨é¢ä¼˜åŠ¿</li>
  </ol>
  <p>æ ¸å¿ƒç»“è®ºï¼šOneRecé€šè¿‡å¼ºåŒ–å­¦ä¹ çš„å¥–åŠ±å¡‘é€ èƒ½åŠ›ï¼Œå®ç°äº†ä¼ ç»Ÿç³»ç»Ÿæ— æ³•è¾¾åˆ°çš„ç²¾å‡†ä¸šåŠ¡æŒ‡æ ‡ä¼˜åŒ–ã€‚</p>
</section>

<!-- æœ¯è¯­è¯†åˆ« -->
<section>
  <h2>æœ¯è¯­è§£é‡Š</h2>
  <ul>
    <li><strong class="term">Legalityï¼ˆåˆæ³•æ€§ï¼‰</strong>ï¼šç”Ÿæˆå†…å®¹ç¬¦åˆé¢„å®šæ ¼å¼è§„èŒƒå’Œè´¨é‡æ ‡å‡†çš„ç¨‹åº¦ï¼Œé€šè¿‡æ ¼å¼å¥–åŠ±æœºåˆ¶ä¼˜åŒ–</li>
    <li><strong class="term">Specific Industrial Reward - SIRï¼ˆç‰¹å®šå·¥ä¸šå¥–åŠ±ï¼‰</strong>ï¼šåŠ¨æ€è°ƒæ•´ç—…æ¯’å†…å®¹å¥–åŠ±æƒé‡çš„æœºåˆ¶ï¼Œå½“å†…å®¹æ¯”ä¾‹è¶…è¿‡é˜ˆå€¼ğ‘“æ—¶åº”ç”¨æŠ‘åˆ¶å› å­ğ›¼ï¼ˆå¼33ï¼‰</li>
    <li><strong class="term">Suppression Factor Î±ï¼ˆæŠ‘åˆ¶å› å­ï¼‰</strong>ï¼šå–å€¼(0,1)çš„æƒé‡ç³»æ•°ï¼Œç”¨äºé™ä½ç—…æ¯’å†…å®¹çš„å¥–åŠ±å€¼</li>
    <li><strong class="term">Reconstruction Lossï¼ˆé‡æ„æŸå¤±ï¼‰</strong>ï¼šè¡¡é‡ç¦»æ•£ä»¤ç‰Œè¿˜åŸåŸå§‹è¾“å…¥æ•°æ®çš„å‡†ç¡®æ€§ï¼ŒæŸå¤±è¶Šä½è¡¨æ˜ä¿¡æ¯ä¿ç•™è¶Šå¥½</li>
    <li><strong class="term">Codebook Utilizationï¼ˆç æœ¬åˆ©ç”¨ç‡ï¼‰</strong>ï¼šè¯„ä¼°å‘é‡åœ¨ç æœ¬ä¸­è¢«ä½¿ç”¨çš„æ•ˆç‡ï¼Œ1.0è¡¨ç¤ºå®Œå…¨åˆ©ç”¨</li>
    <li><strong class="term">Token Distribution Entropyï¼ˆä»¤ç‰Œåˆ†å¸ƒç†µï¼‰</strong>ï¼šåŸºäºé¦™å†œç†µçš„åˆ†å¸ƒå‡åŒ€æ€§é‡åŒ–æŒ‡æ ‡ï¼Œç†µå€¼è¶Šé«˜è¡¨æ˜ä»¤ç‰Œåˆ†é…è¶Šå‡è¡¡</li>
    <li><strong class="term">RQ-VAEï¼ˆæ®‹å·®é‡åŒ–å˜åˆ†è‡ªç¼–ç å™¨ï¼‰</strong>ï¼šä¼ ç»Ÿå‘é‡é‡åŒ–æ–¹æ³•ï¼Œé€šè¿‡å˜åˆ†è‡ªç¼–ç å™¨å­¦ä¹ ç¦»æ•£è¡¨ç¤º</li>
    <li><strong class="term">RQ-Kmeans</strong>ï¼šæ”¹è¿›çš„æ®‹å·®é‡åŒ–æ–¹æ³•ï¼Œç»“åˆKå‡å€¼èšç±»æå‡ç æœ¬åˆ©ç”¨ç‡å’Œåˆ†å¸ƒå‡åŒ€æ€§</li>
    <li><strong class="term">Reward-Shapingï¼ˆå¥–åŠ±å¡‘é€ ï¼‰</strong>ï¼šå¼ºåŒ–å­¦ä¹ ä¸­é€šè¿‡ä¿®æ”¹å¥–åŠ±å‡½æ•°å¼•å¯¼æ™ºèƒ½ä½“è¡Œä¸ºçš„æŠ€æœ¯ï¼Œä¸ºOneRecçš„æ ¸å¿ƒä¼˜åŒ–æ‰‹æ®µ</li>
  </ul>
</section>

</body>
</html>
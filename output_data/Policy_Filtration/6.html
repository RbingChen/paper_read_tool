<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <title>论文解析：Policy Filtration for RLHF</title>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <style>
    body { font-family: Arial, sans-serif; line-height: 1.6; }
    .original { 
      background-color: #f5f5f5; 
      border: 1px solid #ccc; 
      padding: 15px; 
      margin-bottom: 10px;
    }
    .translation { 
      background-color: #e8f5e9; 
      border: 1px solid #4caf50; 
      padding: 15px; 
      margin-bottom: 20px;
    }
    .figure { 
      background-color: #fffde7; 
      padding: 15px; 
      margin: 15px 0;
      font-style: italic;
    }
    .term { 
      color: #d32f2f; 
      font-weight: bold;
    }
    .section { margin-bottom: 30px; }
    h2 { color: #2c3e50; border-bottom: 2px solid #eee; padding-bottom: 5px; }
    .formula-container { text-align: center; margin: 20px 0; }
    .formula-label { font-size: 0.9em; color: #666; }
  </style>
</head>
<body>

<div class="section">
  <h2>内容理解</h2>
  <p>本文核心研究RLHF（基于人类反馈的强化学习）中的奖励模型噪声问题，提出策略过滤方法PF-PPO。关键发现包括：</p>
  <ul>
    <li>在复杂推理任务中，BOND方法通过最佳N样本选择(BoN)有效降低噪声影响</li>
    <li>PF-PPO(BR/BW)在LeetCode等挑战任务中显著优于基准方法</li>
    <li>奖励模型在困难任务中可靠性下降（尤其当中等奖励区域）</li>
    <li>策略过滤通过优化可靠奖励区域缓解奖励过优化问题</li>
    <li>对比实验显示：RL方法需要好坏样本对比，而SFT只需好样本</li>
  </ul>
</div>

<div class="section">
  <h2>内容翻译</h2>
  
  <div class="figure">
    <div class="original">Figure 2. Left: The training and evaluation reward of PPO-S, PPO-M, and FP-PPO on HumanEval. The training reward and the evaluation reward are evaluated on the samples generated by the filtered policy µθ and the unfiltered policy πθ respectively. Right: The pass@1 of PPO-S, PPO-M, and PF-PPO on the HumanEval benchmark.</div>
    <div class="translation">图2. 左图：PPO-S、PPO-M和FP-PPO在HumanEval上的训练奖励和评估奖励。训练奖励和评估奖励分别通过过滤策略µθ和未过滤策略πθ生成的样本进行评估。右图：PPO-S、PPO-M和PF-PPO在HumanEval基准测试中的pass@1分数。</div>
  </div>

  <div class="original">on complex reasoning tasks</div>
  <div class="translation">在复杂推理任务中</div>

  <div class="original">• BOND achieves the highest score among the baseline methods. It demonstrates that iterative best-of-N (BoN) distillation is an effective alignment approach. We speculate that BOND also benefits from its ability to reduce learning on samples with unreliable rewards by selecting the best candidate from a set of N samples.</div>
  <div class="translation">• <span class="term">BOND</span>在基线方法中获得最高分，证明迭代式<span class="term">最佳N样本(BoN)</span>蒸馏是有效的对齐方法。我们推测BOND的优势还在于：通过从N个样本中选择最佳候选，减少了对奖励不可靠样本的学习。</div>

  <div class="original">• Motivated by the good performance of BOND, we implement PF-PPO (BoN) as a natural attempt to apply BoN to an RL-based algorithm. However, PF-PPO (BoN) results in poor performance. This indicates that compared with SFT methods that only need good samples, bad samples for the contrastive learning purposes are also important for RL-based methods. This explains the reason why PF-PPO (BR) and PF-PPO (BW) outperform PF-PPO (BoN).</div>
  <div class="translation">• 受BOND良好表现的启发，我们实现了<span class="term">PF-PPO (BoN)</span>以将BoN应用于RL算法。但PF-PPO (BoN)表现不佳，这表明：与只需好样本的<span class="term">SFT（监督微调）</span>方法不同，用于对比学习的坏样本对RL方法同样重要。这解释了为何<span class="term">PF-PPO (BR)</span>和<span class="term">PF-PPO (BW)</span>优于PF-PPO (BoN)。</div>

  <div class="original">• PF-PPO (BR) and PF-PPO (BW) outperform the others with a larger gap challenging LeetCode tasks. We find that the accuracy of the reward model decreases on this benchmark since it is more difficult for the reward model to distinguish whether one response is better than another, especially when both responses contain errors. This decreases the reliability of the reward model in the moderate reward region (cf. Figure 1). Consequently, PF-PPO (BR) and PF-PPO (BW) can improve the performance in these complex reasoning tasks by avoiding learning on unreliable rewards.</div>
  <div class="translation">• 在挑战性LeetCode任务中，PF-PPO (BR)和PF-PPO (BW)以较大优势超越其他方法。我们发现奖励模型在此类任务中准确性下降——当两个响应均含错误时，奖励模型更难区分优劣（参见图1），导致中等奖励区域的可靠性降低。因此，PF-PPO (BR/BW)通过避免学习不可靠奖励提升了复杂推理任务的性能。</div>

  <div class="original">Training curves. To provide a comprehensive view of the three algorithms, we show the details of the training process. We first present the training curves of PPO-S, PPO-M, and PF-PPO in Figure 2 (left). The training rewards are evaluated on the samples collected by the filtered policy µθ and the evaluation rewards are calculated on the unfiltered policy πθ. We observe that both the training reward and evaluation reward of PPO-M and PF-PPO surpass those of PPO-S. This indicates that sampling multiple responses from a context enhances the performance of the RLHF method, consistent with the findings in Shao et al. (2024). Moreover, in terms of optimizing reward for the same given reward model, FP-PPO achieves a higher or equal reward compared with PPO-S and PPO-M, which indicates that the approximation made in the FP-PPO (i.e., optimizing the unfiltered policy πθ as if it were the filtered policy µθ) does not induce negative effect on its capability of optimizing the reward.</div>
  <div class="translation">训练曲线。为全面展示三种算法，我们呈现训练过程细节。图2（左）显示PPO-S、PPO-M和PF-PPO的训练曲线：训练奖励基于过滤策略µθ收集的样本评估，评估奖励基于未过滤策略πθ计算。PPO-M和PF-PPO的训练/评估奖励均超越PPO-S，表明从同一上下文采样多个响应能提升<span class="term">RLHF</span>性能（与Shao等人2024年研究一致）。此外，在相同奖励模型下，FP-PPO获得比PPO-S/PPO-M更高或相等的奖励，说明FP-PPO的近似方法（将未过滤策略πθ当作过滤策略µθ优化）未损害奖励优化能力。</div>

  <div class="original">We also show the pass@1 results of different algorithms in Figure 2 (right). We observe that, while PF-PPO achieves a similar reward to that of PPO-M, the pass@1 result of PF-PPO exceeds that of PPO-M significantly. This results from the fact that PF-PPO optimizes on the reliable region of the reward model and thus alleviate the reward over-optimization issue.</div>
  <div class="translation">图2（右）展示各算法的<span class="term">pass@1</span>结果：虽然PF-PPO奖励与PPO-M相似，但其pass@1显著更高。这是因为PF-PPO在奖励模型的可靠区域优化，缓解了<span class="term">奖励过优化</span>问题。</div>

  <div class="original">5.4. Alternative Policy Filtering Strategies
PF-PPO modifies the sampling procedure of standard PPO by sampling N responses and randomly filtering responses based on their ranks. In this part, we consider other alternatives to filter by threshold or down-weight the responses with unreliable rewards in the sampling procedure.
• Filtering based on reward thresholds. Given a reward model, we can filter the responses based on their rewards using specified threshold. This results in three strategies, PPO-top that only keeps the top samples whose rewards exceeding a certain threshold, PPO-top-random that keeps also keeps random samples with 50% probability, and PPO-top-bottom that keeps top samples and bottom samples whose rewards are below another spec-</div>
  <div class="translation">5.4 替代策略过滤方法
PF-PPO通过采样N个响应并按排名随机过滤，修改了标准<span class="term">PPO（近端策略优化）</span>的采样过程。本节探索其他替代方案：按阈值过滤或在采样中对不可靠奖励响应降权。
• 基于奖励阈值的过滤：给定奖励模型，可用特定阈值过滤响应。衍生三种策略：<span class="term">PPO-top</span>（仅保留超过阈值的最佳样本）、<span class="term">PPO-top-random</span>（额外以50%概率保留随机样本）、<span class="term">PPO-top-bottom</span>（保留最佳样本和低于另一阈值的底部样本）</div>
</div>

<div class="section">
  <h2>摘要总结</h2>
  <p>本文提出<span class="term">策略过滤(PF-PPO)</span>方法解决RLHF奖励模型噪声问题。核心发现：</p>
  <ul>
    <li>在HumanEval和LeetCode基准测试中，PF-PPO(BR/BW)显著优于PPO-S/PPO-M</li>
    <li>奖励模型在复杂任务（尤其双错误响应场景）中可靠性下降</li>
    <li>PF-PPO通过聚焦可靠奖励区域缓解奖励过优化问题，提升pass@1指标</li>
    <li>RL方法需要好坏样本对比（与仅需好样本的SFT方法不同）</li>
    <li>提出三种阈值过滤策略：PPO-top/top-random/top-bottom</li>
  </ul>
  <p>关键结论：策略过滤通过避免不可靠奖励区域的学习，提升RLHF在复杂推理任务中的鲁棒性。</p>
</div>

<div class="section">
  <h2>术语识别</h2>
  <ul>
    <li><span class="term">RLHF (Reinforcement Learning from Human Feedback)</span>：基于人类反馈的强化学习，使用人类偏好数据训练奖励模型指导策略优化</li>
    <li><span class="term">PPO (Proximal Policy Optimization)</span>：近端策略优化算法，通过裁剪机制稳定策略更新的强化学习方法</li>
    <li><span class="term">PF-PPO (Policy Filtration for PPO)</span>：改进PPO的策略过滤方法，通过样本选择机制降低噪声奖励影响</li>
    <li><span class="term">BoN (Best-of-N)</span>：最佳N样本选择，从N个候选响应中选择奖励最高的样本</li>
    <li><span class="term">BR/BW (Best-Random/Best-Worst)</span>：策略过滤变体，BR保留最佳+随机样本，BW保留最佳+最差样本</li>
    <li><span class="term">pass@1</span>：评估指标，指模型生成结果首次通过测试的比例</li>
    <li><span class="term">奖励过优化(Reward Over-optimization)</span>：策略过度优化奖励模型预测值却损害实际性能的现象</li>
    <li><span class="term">SFT (Supervised Fine-Tuning)</span>：监督微调，使用高质量样本直接微调模型的训练范式</li>
    <li><span class="term">µθ/πθ (Filtered/Unfiltered Policy)</span>：µθ为过滤后策略（仅含可靠样本），πθ为标准策略（含所有样本）</li>
  </ul>
</div>

</body>
</html>
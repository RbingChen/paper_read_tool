<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>论文解析：Policy Filtration for RLHF</title>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<style>
  body { font-family: Arial, sans-serif; line-height: 1.6; }
  .original { background-color: #f0f0f0; border: 1px solid #cccccc; padding: 15px; margin-bottom: 10px; }
  .translation { background-color: #e0ffe0; border: 1px solid #00cc00; padding: 15px; margin-bottom: 20px; }
  .code-block { background-color: #ffffcc; padding: 15px; margin: 10px 0; overflow-x: auto; }
  .term { color: red; font-weight: bold; }
  h2, h3 { color: #2c3e50; }
  ul { list-style-type: none; padding-left: 0; }
  li { margin-bottom: 10px; }
</style>
</head>
<body>

<h1>Policy Filtration for RLHF to Mitigate Noise in Reward Models</h1>

<!-- 内容理解 -->
<h2>内容理解</h2>
<p>该文本探讨了在<span class="term">基于人类反馈的强化学习（RLHF）</span>中，通过<span class="term">策略过滤（Policy Filtration）</span>技术降低<span class="term">奖励模型（Reward Models）</span>噪声的方法。核心内容包括：</p>
<ol>
  <li>定性分析不同奖励等级（高/中/低）的响应特征，揭示中等奖励响应的不可靠性</li>
  <li>对比<span class="term">PF-PPO（Policy Filtration-PPO）</span>与标准<span class="term">PPO-S（Proximal Policy Optimization-Standard）</span>策略生成的代码质量差异</li>
  <li>通过具体编程案例（分数转小数）展示奖励模型误判场景：
    <ul>
      <li>正确响应被分配低奖励（代码可读性差）</li>
      <li>错误响应被分配高奖励（混合正确方法的隐蔽错误）</li>
    </ul>
  </li>
</ol>

<hr>

<!-- 内容翻译 -->
<h2>内容翻译</h2>

<div class="original">
  <h3>Policy Filtration for RLHF to Mitigate Noise in Reward Models</h3>
</div>
<div class="translation">
  <h3>基于<span class="term">RLHF（Reinforcement Learning from Human Feedback）</span>的策略过滤以降低<span class="term">奖励模型（Reward Models）</span>中的噪声</h3>
</div>

<div class="original">
  <h4>B. Qualitative results</h4>
  <p>In this section, we provide qualitative results on 1) how responses with high/middle/low rewards look like and why responses with middle rewards are unreliable; and 2) the qualitative difference between the code generated by the PF-PPO policy and the standard PPO (PPO-S) policy.</p>
</div>
<div class="translation">
  <h4>B. 定性结果</h4>
  <p>本节提供以下定性结果：1) 高/中/低奖励响应的特征及中等奖励响应不可靠的原因；2) <span class="term">PF-PPO（Policy Filtration-PPO）</span>策略与标准<span class="term">PPO-S（Proximal Policy Optimization-Standard）</span>策略生成代码的质性差异。</p>
</div>

<div class="original">
  <h4>B.1. Analysis on the responses associated with different rewards</h4>
  <p>We present a prompt along with several responses, including a correct response but assigned with a low reward, an incorrect response but assigned with a high reward, an incorrect response with a low reward, and a correct response with a high reward. The prompt describes a coding problem that requires to convert fractions to decimals.</p>
  <p>We have the following findings:</p>
  <ul>
    <li>For the correct response but assigned with a low reward, the generated code is less clear and harder to read. For example, the code may mix several steps into one line.</li>
    <li>For the incorrect response but assigned with a high reward, the response incorrectly mixes two correct approaches. This mistake can hardly be identified by the reward model (and even GPT-4).</li>
    <li>For the incorrect response assigned with a low reward, the response contains an obvious mistake which is easily detected by the reward model.</li>
  </ul>
</div>
<div class="translation">
  <h4>B.1. 不同奖励响应的分析</h4>
  <p>我们展示一个<span class="term">提示（Prompt）</span>及多个响应，包括：正确但分配低奖励的响应、错误但分配高奖励的响应、错误且分配低奖励的响应，以及正确且分配高奖励的响应。该提示描述了一个需要将分数转换为小数的编程问题。</p>
  <p>主要发现：</p>
  <ul>
    <li>对于正确但分配低奖励的响应：生成代码清晰度低、可读性差（例如将多个步骤混合在单行代码中）</li>
    <li>对于错误但分配高奖励的响应：错误地混合两种正确方法，该错误难以被<span class="term">奖励模型（Reward Models）</span>（甚至GPT-4）识别</li>
    <li>对于错误且分配低奖励的响应：包含明显错误，易被奖励模型检测</li>
  </ul>
</div>

<div class="original">
  <p>We also provide detailed analysis into the solutions to this problem. The given prompt is a coding task to convert fraction to decimal.</p>
  <div class="code-block">
    <pre>Given two integers representing the numerator and denominator of a fraction, return
the fraction in string format.
If the fractional part is repeating, enclose the repeating part in parentheses.
If multiple answers are possible, return any of them.
It is guaranteed that the length of the answer string is less than 10<sup>4</sup> for all the
given inputs.

Please complete the following python code precisely:
```python
class Solution:
    def fractionToDecimal(self, numerator: int, denominator: int) -> str:
```</pre>
  </div>
</div>
<div class="translation">
  <p>我们进一步分析该问题的解决方案。给定提示是一个将分数转换为小数的编程任务：</p>
  <div class="code-block">
    <pre>给定两个表示分数分子和分母的整数，以字符串形式返回该分数。
如果小数部分存在循环，用括号将循环部分括起。
若存在多个可能答案，返回任意一个。
保证所有输入的答案字符串长度小于10<sup>4</sup>。

请精确补全以下Python代码：
```python
class Solution:
    def fractionToDecimal(self, numerator: int, denominator: int) -> str:
```</pre>
  </div>
</div>

<div class="original">
  <p>This is a correct response with a high reward. This solution is thorough with clear variable names and properly structured steps. It is easier to read due to breaking down steps explicitly such as calculating the integer part and handling the remainder.</p>
  <div class="code-block">
    <pre>To solve the problem of converting a fraction to its decimal representation, we can
follow these steps:

1. **Determine the sign of the resultant decimal**: If both the numerator and the
denominator are negative, the result should be positive. Otherwise, the result
will have a leading minus sign if and only if either the numerator or the
denominator is negative.

2. **Convert the numerator and denominator to positive values**: Since the division
process involves integer division and the decimal part requires modulus operation
, both numerator and denominator should be made positive for easier computation.

3. **Calculate the integer part**: The integer part is the result of integer division
of the numerator by the denominator.

4. **Manage the remainder**: As we perform integer division, the remainder will be
used to form the fractional part. This remainder will be treated as the new
numerator for the next step.</pre>
  </div>
</div>
<div class="translation">
  <p>这是一个高奖励的正确响应。该解决方案完整，变量命名清晰且步骤结构合理。由于显式分解步骤（如计算整数部分和处理余数），代码可读性更高。</p>
  <div class="code-block">
    <pre>将分数转换为小数表示的解决步骤：

1. **确定结果的符号**：若分子分母均为负，结果为正；否则当分子或分母任一为负时，结果需添加负号。

2. **转换分子分母为正数**：因除法涉及整数运算和小数模运算，将分子分母转为正数以简化计算。

3. **计算整数部分**：整数部分为分子除以分母的整数除法结果。

4. **处理余数**：整数除法产生的余数将用于构建小数部分，该余数作为下一步的新分子。</pre>
  </div>
</div>

<hr>

<!-- 摘要总结 -->
<h2>摘要总结</h2>
<p>本节通过编程案例的定性分析揭示<span class="term">奖励模型（Reward Models）</span>的噪声问题：</p>
<ol>
  <li><strong>奖励分配不一致性</strong>：存在正确响应被分配低奖励（因代码可读性差）和错误响应被分配高奖励（因隐蔽性错误）的现象</li>
  <li><strong>中等奖励不可靠性</strong>：中等奖励响应因边界模糊易导致训练不稳定</li>
  <li><strong>PF-PPO优势</strong>：相比<span class="term">PPO-S（Proximal Policy Optimization-Standard）</span>，<span class="term">PF-PPO（Policy Filtration-PPO）</span>生成的代码具有更清晰的变量命名和模块化结构</li>
  <li><strong>错误类型分析</strong>：奖励模型易漏检混合正确方法的复杂错误，但能识别明显错误</li>
</ol>

<hr>

<!-- 术语识别 -->
<h2>术语识别</h2>
<ul>
  <li><span class="term">RLHF（Reinforcement Learning from Human Feedback）</span>：基于人类反馈的强化学习。通过人类偏好数据训练奖励模型，再通过强化学习优化策略的核心技术。</li>
  <li><span class="term">Reward Models（奖励模型）</span>：预测人类偏好的代理模型，为强化学习提供奖励信号。易受标注噪声影响导致奖励分配偏差。</li>
  <li><span class="term">Policy Filtration（策略过滤）</span>：RLHF的改进技术，通过筛选高置信度策略更新降低奖励模型噪声的影响。</li>
  <li><span class="term">PF-PPO（Policy Filtration-PPO）</span>：结合策略过滤的近端策略优化算法，生成更可靠且可读的代码响应。</li>
  <li><span class="term">PPO-S（Proximal Policy Optimization-Standard）</span>：标准近端策略优化算法，RLHF的基线方法，存在奖励敏感性问题。</li>
  <li><span class="term">Prompt（提示）</span>：模型输入指令（如编程问题描述），用于引导响应生成。</li>
  <li><span class="term">Response（响应）</span>：模型根据提示生成的输出（如代码解决方案），由奖励模型评估质量。</li>
</ul>

</body>
</html>
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <title>è®ºæ–‡è§£æ - Policy Filtration for RLHF</title>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <style>
    body { font-family: Arial, sans-serif; line-height: 1.6; }
    .section { margin-bottom: 30px; }
    h2 { color: #2c3e50; border-bottom: 2px solid #3498db; padding-bottom: 5px; }
    .original { background-color: #f0f0f0; border: 1px solid #ccc; padding: 15px; margin-bottom: 10px; }
    .translation { background-color: #e6ffe6; border: 1px solid #4CAF50; padding: 15px; }
    .figure { background-color: #ffffcc; padding: 15px; margin: 15px 0; font-style: italic; }
    .term { color: red; font-weight: bold; }
    .formula-container { text-align: center; margin: 20px 0; }
    .formula { display: inline-block; }
    .formula-number { font-style: italic; margin-top: 5px; }
  </style>
</head>
<body>

<div class="section">
  <h2>ğŸ“– å†…å®¹ç†è§£</h2>
  <p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º<strong class="term">ç­–ç•¥è¿‡æ»¤ï¼ˆPolicy Filtrationï¼‰</strong>çš„æ–°æ–¹æ³•ï¼Œç”¨äºè§£å†³<strong class="term">åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰</strong>ä¸­çš„å…³é”®é—®é¢˜ï¼šå¥–åŠ±æ¨¡å‹çš„ä¸å‡†ç¡®æ€§ã€‚ç ”ç©¶å‘ç°ï¼Œå¥–åŠ±æ¨¡å‹åœ¨ä¸åŒå¾—åˆ†åŒºåŸŸçš„å¯é æ€§å­˜åœ¨æ˜¾è‘—å·®å¼‚â€”â€”é«˜å¥–åŠ±åŒºåŸŸï¼ˆå¦‚å¥–åŠ±å€¼>0.8ï¼‰çš„é¢„æµ‹æ›´å¯é ã€‚åŸºäºæ­¤ï¼Œä½œè€…å¼€å‘äº†<strong class="term">PF-PPO</strong>ç®—æ³•ï¼Œé€šè¿‡åœ¨ç­–ç•¥ä¼˜åŒ–è¿‡ç¨‹ä¸­<strong>é€‰æ‹©æ€§è¿‡æ»¤ä½å¯é æ€§å¥–åŠ±æ ·æœ¬</strong>æ¥æå‡ä¿¡å·è´¨é‡ã€‚è¯¥æ–¹æ³•åœ¨ä»£ç ç”Ÿæˆï¼ˆHumanEval/MBPP/LeetCodeï¼‰å’Œæ•°å­¦æ¨ç†ï¼ˆApe210K/CMATHï¼‰ä»»åŠ¡ä¸­æ˜¾è‘—è¶…è¶ŠåŸºçº¿æ¨¡å‹ï¼Œæœ€é«˜æå‡è¾¾10%ã€‚æ ¸å¿ƒåˆ›æ–°ç‚¹åœ¨äºä½¿ç”¨<strong class="term">å†³å®šç³»æ•°ï¼ˆRÂ²ï¼‰</strong>ä½œä¸ºè¿‡æ»¤ç­–ç•¥çš„è¯„ä¼°æŒ‡æ ‡ï¼Œç¡®ä¿ä¿ç•™çš„å¥–åŠ±ä¿¡å·èƒ½æœ‰æ•ˆåæ˜ çœŸå®æ€§èƒ½ã€‚</p>
</div>

<div class="section">
  <h2>ğŸŒ å†…å®¹ç¿»è¯‘</h2>
  
  <div class="original">
    <strong>arXiv:2409.06957v5 [cs.LG] 7 Jun 2025</strong><br>
    Policy Filtration for RLHF to Mitigate Noise in Reward Models<br>
    Chuheng Zhang* 1Wei Shen* 2Li Zhao1Xuyun Zhang3Xiaolong Xu4Wanchun Dou5Jiang Bian1
  </div>
  <div class="translation">
    <strong>arXiv:2409.06957v5 [cs.LG] 2025å¹´6æœˆ7æ—¥</strong><br>
    ç”¨äºç¼“è§£å¥–åŠ±æ¨¡å‹å™ªå£°çš„RLHFç­–ç•¥è¿‡æ»¤<br>
    å¼ æ¥šæ’* 1 æ²ˆä¼Ÿ* 2 èµµè‰1 å¼ æ—­æ˜€3 å¾å°é¾™4 çª¦ä¸‡æ˜¥5 è¾¹æ±Ÿ1
  </div>
  
  <div class="original">
    <strong>Abstract</strong><br>
    While direct policy optimization methods exist, pioneering LLMs are fine-tuned with reinforcement learning from human feedback (RLHF) to generate better responses under the supervision of a reward model learned from preference data. One major challenge of RLHF is the inaccuracy of the intermediate reward model, especially in the tasks that requires complex reasoning for the reward model to score a response. We find that the reliability of the reward model varies across responses assigned with different rewards. This motivates us to filter the samples whose rewards may be unreliable to improve the signal-to-noise ratio during policy learning, resulting in Policy Filtration for Proximal Policy Optimization (PF-PPO). To choose a proper policy filtering strategy, we use the coefficient of determination (RÂ²) between the rewards and actual scores on filtered samples as the metrics to help us find promising strategies since it measures how well the rewards filtered by PF-PPO indicate real performance. We provide extensive experiments to validate the effectiveness of PF-PPO in code generation and math reasoning tasks. In code generation, PF-PPO achieves the state-of-the-art performance of 7-billion-parameter models on HumanEval (+7.9%), MBPP (+0.7%), and LeetCode Contest (+10.0%) which is a more challenging benchmark created by us. In math reasoning, PF-PPO yields performance increase using different reward models and benchmarks (Ape210K and CMATH). Code is available on https://github.com/DtYXs/verl/tree/pf-ppo.
  </div>
  <div class="translation">
    <strong>æ‘˜è¦</strong><br>
    å°½ç®¡å­˜åœ¨ç›´æ¥ç­–ç•¥ä¼˜åŒ–æ–¹æ³•ï¼Œä½†å‰æ²¿å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šè¿‡<strong class="term">åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰</strong>è¿›è¡Œå¾®è°ƒï¼Œåœ¨ä»åå¥½æ•°æ®å­¦ä¹ çš„å¥–åŠ±æ¨¡å‹ç›‘ç£ä¸‹ç”Ÿæˆæ›´ä¼˜å“åº”ã€‚RLHFçš„ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜æ˜¯ä¸­é—´å¥–åŠ±æ¨¡å‹çš„ä¸å‡†ç¡®æ€§ï¼Œå°¤å…¶åœ¨éœ€è¦å¥–åŠ±æ¨¡å‹å¯¹å“åº”è¿›è¡Œå¤æ‚æ¨ç†è¯„åˆ†çš„ä»»åŠ¡ä¸­ã€‚æˆ‘ä»¬å‘ç°å¥–åŠ±æ¨¡å‹çš„å¯é æ€§éšä¸åŒå¥–åŠ±åˆ†é…çš„å“åº”è€Œå˜åŒ–ã€‚è¿™ä¿ƒä½¿æˆ‘ä»¬è¿‡æ»¤å¥–åŠ±å¯èƒ½ä¸å¯é çš„æ ·æœ¬ï¼Œä»¥æå‡ç­–ç•¥å­¦ä¹ è¿‡ç¨‹ä¸­çš„ä¿¡å™ªæ¯”ï¼Œä»è€Œæå‡º<strong class="term">è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–çš„ç­–ç•¥è¿‡æ»¤ï¼ˆPF-PPOï¼‰</strong>ã€‚ä¸ºé€‰æ‹©åˆé€‚çš„ç­–ç•¥è¿‡æ»¤æ–¹æ³•ï¼Œæˆ‘ä»¬ä½¿ç”¨è¿‡æ»¤æ ·æœ¬çš„å¥–åŠ±ä¸å®é™…åˆ†æ•°é—´çš„<strong class="term">å†³å®šç³»æ•°ï¼ˆRÂ²ï¼‰</strong>ä½œä¸ºæŒ‡æ ‡æ¥å¯»æ‰¾æœ‰æ•ˆç­–ç•¥ï¼Œå› å…¶è¡¡é‡äº†PF-PPOè¿‡æ»¤åçš„å¥–åŠ±æŒ‡ç¤ºçœŸå®æ€§èƒ½çš„ç¨‹åº¦ã€‚æˆ‘ä»¬é€šè¿‡å¤§é‡å®éªŒéªŒè¯PF-PPOåœ¨ä»£ç ç”Ÿæˆå’Œæ•°å­¦æ¨ç†ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚åœ¨ä»£ç ç”Ÿæˆä¸­ï¼ŒPF-PPOåœ¨HumanEvalï¼ˆ+7.9%ï¼‰ã€MBPPï¼ˆ+0.7%ï¼‰å’Œæˆ‘ä»¬åˆ›å»ºçš„æ›´å…·æŒ‘æˆ˜æ€§çš„åŸºå‡†LeetCode Contestï¼ˆ+10.0%ï¼‰ä¸Šå®ç°äº†70äº¿å‚æ•°æ¨¡å‹çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚åœ¨æ•°å­¦æ¨ç†ä¸­ï¼ŒPF-PPOä½¿ç”¨ä¸åŒå¥–åŠ±æ¨¡å‹å’ŒåŸºå‡†ï¼ˆApe210Kå’ŒCMATHï¼‰å‡å¸¦æ¥æ€§èƒ½æå‡ã€‚ä»£ç å‘å¸ƒäºhttps://github.com/DtYXs/verl/tree/pf-ppoã€‚
  </div>
  
  <div class="figure">
    <strong>Figure 1.</strong> The reward model can be inaccurate, i.e., the actual score of the response does not align well with the reward given by the reward model. However, the reward model in specific regions (e.g., when it gives rewards higher than 0.8) is more reliable, i.e., the responses with similar rewards result in consistent performance. We use a fine-tuned policy to generate 10 responses for each of the 164 prompts in the HumanEval dataset and use a reward model trained with the common recipe to generate their rewards. We group the responses with similar rewards and calculate the average of their actual scores (i.e., the average correctness), indicating each group by one point. To evaluate the reliability of the reward model, we repeat the process ten times corresponding to the ten lines.
  </div>
  <div class="figure">
    <strong>å›¾1.</strong> å¥–åŠ±æ¨¡å‹å¯èƒ½ä¸å‡†ç¡®ï¼Œå³å“åº”çš„å®é™…åˆ†æ•°ä¸å¥–åŠ±æ¨¡å‹ç»™å‡ºçš„å¥–åŠ±ä¸ä¸€è‡´ã€‚ä½†åœ¨ç‰¹å®šåŒºåŸŸï¼ˆå¦‚å¥–åŠ±å€¼é«˜äº0.8æ—¶ï¼‰å¥–åŠ±æ¨¡å‹æ›´å¯é ï¼Œå³å…·æœ‰ç›¸ä¼¼å¥–åŠ±çš„å“åº”è¡¨ç°å‡ºä¸€è‡´æ€§èƒ½ã€‚æˆ‘ä»¬ä½¿ç”¨å¾®è°ƒç­–ç•¥ä¸ºHumanEvalæ•°æ®é›†çš„164ä¸ªæç¤ºå„ç”Ÿæˆ10ä¸ªå“åº”ï¼Œå¹¶ç”¨å¸¸è§„æ–¹æ³•è®­ç»ƒçš„å¥–åŠ±æ¨¡å‹ç”Ÿæˆå¥–åŠ±ã€‚æˆ‘ä»¬å°†å¥–åŠ±ç›¸ä¼¼çš„å“åº”åˆ†ç»„å¹¶è®¡ç®—å…¶å®é™…åˆ†æ•°çš„å¹³å‡å€¼ï¼ˆå³å¹³å‡æ­£ç¡®ç‡ï¼‰ï¼Œæ¯ç»„çš„å¹³å‡å€¼ç”¨ç‚¹è¡¨ç¤ºã€‚ä¸ºè¯„ä¼°å¥–åŠ±æ¨¡å‹å¯é æ€§ï¼Œè¯¥è¿‡ç¨‹é‡å¤åæ¬¡ï¼ˆå¯¹åº”åæ¡çº¿ï¼‰ã€‚
  </div>
  
  <div class="original">
    <strong>1. Introduction</strong><br>
    Reinforcement Learning from Human Feedback (RLHF) is a key technique to align large language models (LLMs) with human values and preferences (Christiano et al., 2017; Ziegler et al., 2019; Ouyang et al., 2022). RLHF has been proven to be an essential process for LLMs to produce more helpful, harmless, and honest responses (Bai et al., 2022). Despite various non-RL algorithms such as DPO (Rafailov et al., 2024) are proposed, state-of-the-art applications such as ChatGPT/GPT-4 (OpenAI, 2023), Claude (Anthropic, 2023), and Gemini (Team et al., 2023) adopt the RL algorithm (e.g., PPO) for policy optimization. The key challenge of RLHF is the inaccuracy of the intermediate reward model. While there are researchers investigate how to learn reliable reward models (see e.g., Wang et al., 2024), we focus on how to learn better policy under the guidance of such inaccurate reward models. We observe that, though the reward model gives inaccurate...
  </div>
  <div class="translation">
    <strong>1. å¼•è¨€</strong><br>
    <strong class="term">åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰</strong>æ˜¯å°†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸äººç±»ä»·å€¼è§‚å’Œåå¥½å¯¹é½çš„å…³é”®æŠ€æœ¯ï¼ˆChristianoç­‰, 2017; Zieglerç­‰, 2019; Ouyangç­‰, 2022ï¼‰ã€‚RLHFå·²è¢«è¯æ˜æ˜¯LLMsç”Ÿæˆæ›´æœ‰ç›Šã€æ— å®³ä¸”è¯šå®å“åº”çš„å¿…è¦è¿‡ç¨‹ï¼ˆBaiç­‰, 2022ï¼‰ã€‚å°½ç®¡å·²æå‡ºå¤šç§éRLç®—æ³•ï¼ˆå¦‚<strong class="term">DPO</strong>ï¼ˆRafailovç­‰, 2024ï¼‰ï¼‰ï¼Œä½†æœ€å…ˆè¿›çš„åº”ç”¨å¦‚ChatGPT/GPT-4ï¼ˆOpenAI, 2023ï¼‰ã€Claudeï¼ˆAnthropic, 2023ï¼‰å’ŒGeminiï¼ˆTeamç­‰, 2023ï¼‰ä»é‡‡ç”¨RLç®—æ³•ï¼ˆå¦‚<strong class="term">PPO</strong>ï¼‰è¿›è¡Œç­–ç•¥ä¼˜åŒ–ã€‚RLHFçš„å…³é”®æŒ‘æˆ˜åœ¨äºä¸­é—´å¥–åŠ±æ¨¡å‹çš„ä¸å‡†ç¡®æ€§ã€‚è™½æœ‰ç ”ç©¶è€…æ¢ç´¢å¦‚ä½•å­¦ä¹ å¯é å¥–åŠ±æ¨¡å‹ï¼ˆå¦‚Wangç­‰, 2024ï¼‰ï¼Œæˆ‘ä»¬ä¸“æ³¨äºå¦‚ä½•åœ¨æ­¤ç±»ä¸å‡†ç¡®å¥–åŠ±æ¨¡å‹æŒ‡å¯¼ä¸‹å­¦ä¹ æ›´ä¼˜ç­–ç•¥ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œå°½ç®¡å¥–åŠ±æ¨¡å‹ç»™å‡ºä¸å‡†ç¡®çš„...
  </div>
</div>

<div class="section">
  <h2>ğŸ“ æ‘˜è¦æ€»ç»“</h2>
  <p>æœ¬æ–‡æå‡º<strong class="term">PF-PPO</strong>æ–¹æ³•ï¼Œé€šè¿‡<strong>é€‰æ‹©æ€§è¿‡æ»¤å¥–åŠ±æ¨¡å‹ä¸­çš„ä¸å¯é ä¿¡å·</strong>æ¥æ”¹è¿›RLHFè®­ç»ƒã€‚æ ¸å¿ƒå‘ç°æ˜¯å¥–åŠ±æ¨¡å‹åœ¨é«˜å¥–åŠ±åŒºåŸŸçš„é¢„æµ‹æ›´å¯é ï¼ˆå¦‚å›¾1æ‰€ç¤ºï¼‰ã€‚PF-PPOä½¿ç”¨<strong class="term">å†³å®šç³»æ•°ï¼ˆRÂ²ï¼‰</strong>è¯„ä¼°è¿‡æ»¤ç­–ç•¥çš„æœ‰æ•ˆæ€§ï¼Œåœ¨ä»£ç ç”Ÿæˆï¼ˆHumanEval/MBPP/LeetCodeï¼‰å’Œæ•°å­¦æ¨ç†ï¼ˆApe210K/CMATHï¼‰ä»»åŠ¡ä¸­æ˜¾è‘—æå‡æ¨¡å‹æ€§èƒ½ï¼š</p>
  <ul>
    <li>HumanEvalå‡†ç¡®ç‡æå‡7.9%</li>
    <li>LeetCode Contestï¼ˆæ–°åŸºå‡†ï¼‰æå‡10.0%</li>
    <li>åœ¨ä¸åŒå¥–åŠ±æ¨¡å‹å’Œæ•°å­¦æ•°æ®é›†ä¸Šå‡è¡¨ç°é²æ£’</li>
  </ul>
  <p>è¯¥æ–¹æ³•ä¸ºç¼“è§£RLHFå¥–åŠ±å™ªå£°æä¾›äº†æ–°æ€è·¯ï¼Œä»£ç å·²å¼€æºã€‚</p>
</div>

<div class="section">
  <h2>ğŸ”‘ æœ¯è¯­è¯†åˆ«</h2>
  <ul>
    <li><strong class="term">RLHF (Reinforcement Learning from Human Feedback)</strong>: åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ã€‚é€šè¿‡äººç±»åå¥½æ•°æ®è®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼Œå†ç”¨å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–LLMç”Ÿæˆç­–ç•¥çš„æ ¸å¿ƒå¯¹é½æŠ€æœ¯ã€‚</li>
    <li><strong class="term">PF-PPO (Policy Filtration for Proximal Policy Optimization)</strong>: è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–çš„ç­–ç•¥è¿‡æ»¤ã€‚æœ¬æ–‡æå‡ºçš„æ–°ç®—æ³•ï¼Œåœ¨PPOæ›´æ–°å‰è¿‡æ»¤ä½å¯é æ€§å¥–åŠ±æ ·æœ¬ä»¥æå‡è®­ç»ƒä¿¡å·è´¨é‡ã€‚</li>
    <li><strong class="term">Coefficient of Determination (RÂ²)</strong>: å†³å®šç³»æ•°ã€‚ç”¨äºè¯„ä¼°è¿‡æ»¤ç­–ç•¥çš„å…³é”®æŒ‡æ ‡ï¼Œè®¡ç®—å…¬å¼ä¸ºï¼š
      <div class="formula-container">
        <div class="formula">
          \[ R^2 = 1 - \frac{\sum_{i}(y_i - \hat{y_i})^2}{\sum_{i}(y_i - \bar{y})^2} \]
        </div>
        <div class="formula-number">(1) å…¶ä¸­ \(y_i\)=å®é™…åˆ†æ•°, \(\hat{y_i}\)=é¢„æµ‹åˆ†æ•°, \(\bar{y}\)=å‡å€¼</div>
      </div>
      è¡¡é‡è¿‡æ»¤åå¥–åŠ±é¢„æµ‹å®é™…æ€§èƒ½çš„èƒ½åŠ›ï¼ˆå€¼è¶Šæ¥è¿‘1è¡¨ç¤ºè¶Šå¯é ï¼‰ã€‚
    </li>
    <li><strong class="term">PPO (Proximal Policy Optimization)</strong>: è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ã€‚RLHFä¸­å¹¿æ³›ä½¿ç”¨çš„ç­–ç•¥æ¢¯åº¦ç®—æ³•ï¼Œé€šè¿‡é™åˆ¶ç­–ç•¥æ›´æ–°å¹…åº¦ç¡®ä¿è®­ç»ƒç¨³å®šæ€§ã€‚</li>
    <li><strong class="term">DPO (Direct Preference Optimization)</strong>: ç›´æ¥åå¥½ä¼˜åŒ–ã€‚ä¸€ç§æ— éœ€æ˜¾å¼å¥–åŠ±æ¨¡å‹çš„RLHFæ›¿ä»£æ–¹æ¡ˆï¼Œé€šè¿‡åå¥½æ•°æ®ç›´æ¥ä¼˜åŒ–ç­–ç•¥ã€‚</li>
    <li><strong class="term">HumanEval/MBPP</strong>: ä»£ç ç”ŸæˆåŸºå‡†æ•°æ®é›†ã€‚HumanEvalåŒ…å«164ä¸ªæ‰‹å†™ç¼–ç¨‹é—®é¢˜ï¼ŒMBPPåŒ…å«974ä¸ªä¼—åŒ…Pythoné—®é¢˜ã€‚</li>
    <li><strong class="term">LeetCode Contest</strong>: æœ¬æ–‡åˆ›å»ºçš„æ–°ä»£ç åŸºå‡†ã€‚åŸºäºLeetCodeç«èµ›é¢˜ç›®ï¼Œæ¯”HumanEvalæ›´å…·æŒ‘æˆ˜æ€§ã€‚</li>
  </ul>
</div>

</body>
</html>
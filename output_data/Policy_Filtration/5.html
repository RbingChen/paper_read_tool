<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>论文解析：RLHF策略过滤方法</title>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<style>
  body { font-family: Arial, sans-serif; line-height: 1.6; }
  .original { background-color: #f0f0f0; border: 1px solid #999; padding: 15px; margin-bottom: 10px; }
  .translation { background-color: #e0ffe0; border: 1px solid #4CAF50; padding: 15px; margin-bottom: 30px; }
  .term { color: red; font-weight: bold; }
  table { width: 100%; border-collapse: collapse; margin: 20px 0; }
  th, td { border: 1px solid #ddd; padding: 8px; text-align: center; }
  th { background-color: #f2f2f2; }
  .formula-container { text-align: center; margin: 20px 0; }
  .formula-number { display: block; font-style: italic; }
  h2 { color: #2c3e50; border-bottom: 2px solid #3498db; padding-bottom: 5px; }
</style>
</head>
<body>

<h1>Policy Filtration for RLHF to Mitigate Noise in Reward Models 论文解析</h1>

<!-- 内容理解 -->
<h2>内容理解</h2>
<p>本文提出了一种名为PF-PPO（策略过滤PPO）的新方法，旨在解决强化学习人类反馈（RLHF）中奖励模型的噪声问题。核心创新点是在PPO训练过程中引入策略过滤机制，通过筛选高质量响应来优化训练数据。研究在HumanEval、MBPP和LeetCode三个代码生成基准上进行了全面实验，结果表明PF-PPO（特别是BR和BW变体）显著优于现有方法（如DPO、IPO、KTO等）。研究还发现：1）基于PPO的方法整体优于SFT和DPO类方法；2）在复杂任务（LeetCode）上RL方法的优势更明显；3）适当的数据构建能解决DPO的已知缺陷。</p>

<!-- 内容翻译 -->
<h2>内容翻译</h2>

<div class="original">
  <h3>Policy Filtration for RLHF to Mitigate Noise in Reward Models</h3>
  <table>
    <tr><th>Family</th><th>Method</th><th>HumanEval</th><th>MBPP</th><th>LeetCode</th></tr>
    <tr><td rowspan="3">Supervised Fine-Tuning</td><td>SFT</td><td>74.2</td><td>70.8</td><td>15.2</td></tr>
    <tr><td>RAFT (Dong et al., 2023)</td><td>76.9</td><td>71.3</td><td>17.8</td></tr>
    <tr><td>BOND (Sessa et al., 2024)</td><td>80.8</td><td>75.2</td><td>30.0</td></tr>
    <tr><td rowspan="4">Direct Policy Optimization</td><td>DPO (Rafailov et al., 2024)</td><td>78.4</td><td>73.7</td><td>23.0</td></tr>
    <tr><td>IPO (Azar et al., 2024)</td><td>78.2</td><td>72.9</td><td>23.2</td></tr>
    <tr><td>KTO (Ethayarajh et al., 2024)</td><td>77.9</td><td>72.5</td><td>22.4</td></tr>
    <tr><td>Iterative-DPO (Pang et al., 2024)</td><td>78.1</td><td>74.8</td><td>23.8</td></tr>
    <tr><td rowspan="5">Reinforcement Learning</td><td>PPO-S (Hu et al., 2024)</td><td>78.1</td><td>73.8</td><td>25.2</td></tr>
    <tr><td>PPO-M (cf. Shao et al., 2024)</td><td>80.2</td><td>75.0</td><td>29.8</td></tr>
    <tr><td>PF-PPO (BoN)</td><td>75.8</td><td>71.7</td><td>16.8</td></tr>
    <tr><td>PF-PPO (BR)</td><td><strong>82.9</strong></td><td><u>75.9</u></td><td><strong>33.0</strong></td></tr>
    <tr><td>PF-PPO (BW)</td><td><u>82.4</u></td><td><strong>76.2</strong></td><td><u>30.4</u></td></tr>
    <tr><td colspan="2">SOTA (7B models) Magicoder (Wei et al., 2023b)</td><td>76.8</td><td>75.7</td><td>-</td></tr>
  </table>
  <p>Table 2. The performance of different algorithms on Code Generation. We compare pass@1 of PF-PPO (our algorithm) against baseline methods. For each benchmark, we select the best score across 5 epochs for each method. The highest and the second highest scores on each benchmark are highlighted in bold and underline respectively. All experiments are based on the same code base for fair comparison, except for the scores reported by Magicoder which is the best 7B model so far.</p>
  <p>last iteration, and we train the policy for 4 iterations.</p>
  <p>Direct policy optimization. To implement direct policy optimization methods, we use our reward model dataset as the preference dataset required in these methods. We implement DPO (Rafailov et al., 2024), IPO (Azar et al., 2024), KTO (Ethayarajh et al., 2024), and iterative DPO (Pang et al., 2024). For iterative DPO, we train the DPO model for three iterations. For each iteration, we construct the preference dataset as follows: The prompts are sampled from the reward model dataset and responses are generated by the trained DPO model from the previous iteration (if exists) or the previous SFT phase.</p>
  <p>Reinforcement Learning. For standard RLHF, we use the implementation from OpenRLHF (Hu et al., 2024), which incorporates several advanced PPO training techniques and has demonstrates strong performance on various benchmarks. We denote this baseline as PPO-S. For our method PF-PPO, we implement three variants (BoN, BR, and BW) as introduced in the previous section. Since PF-PPO collects multiple responses for a given prompt/context, we introduce another baseline called PPO-M (PPO with multiple responses) that uses all the N responses for training without filtering. The effective difference between PPO-S and PPO-M is that the buffer in PPO-M contains more samples with the same context but with different responses which may provide detailed token-level instruction by comparing the responses corresponding to the same context. Therefore, comparing with PPO-M can help us distinguish the effect of collecting multiple responses and that of filtering collected responses. For fair comparison, we ensure that the computational costs of PF-PPO for each iteration is no larger than those of PPO-M and PPO-S, and we refer the readers to Appendix C for the detailed analysis on the computational efficiency of PPO-S, PPO-M, and PF-PPO.</p>
  <p>Experiment results. We present the pass@1 results of different methods on the three benchmarks in Table 2. The experiment results show that PF-PPO (BR) and PF-PPO (BW) obtain the highest scores on these benchmarks, indicating the effectiveness of our method. Furthermore, we have the following observations:</p>
  <ul>
    <li>IPO and KTO (improved versions of DPO) do not outperform DPO when trained on properly selected datasets. This indicates that appropriate dataset construction can address the weaknesses of DPO found in previous papers, enabling DPO to achieve a performance comparable to its improved versions.</li>
    <li>PPO-based algorithms outperform SFT-based and DPO-based algorithms in general, demonstrating that PPO is superior to these algorithms on reasoning tasks. We speculate that the good performance of PPO may stem from the generalization ability of the reward model and the value network used in PPO, which can be used to transform trajectory-level reward modeling to token-wise advantages and thus provides more fine-grained guidance. Moreover, the gap between PPO-based algorithms and the others becomes larger on the more challenging LeetCode benchmark, which further highlights the advantage of RL.</li>
  </ul>
</div>

<div class="translation">
  <h3>通过策略过滤降低RLHF中奖励模型噪声</h3>
  <table>
    <tr><th>族类</th><th>方法</th><th>HumanEval</th><th>MBPP</th><th>LeetCode</th></tr>
    <tr><td rowspan="3">监督微调</td><td>SFT</td><td>74.2</td><td>70.8</td><td>15.2</td></tr>
    <tr><td>RAFT (Dong 等, 2023)</td><td>76.9</td><td>71.3</td><td>17.8</td></tr>
    <tr><td>BOND (Sessa 等, 2024)</td><td>80.8</td><td>75.2</td><td>30.0</td></tr>
    <tr><td rowspan="4">直接策略优化</td><td>DPO (Rafailov 等, 2024)</td><td>78.4</td><td>73.7</td><td>23.0</td></tr>
    <tr><td>IPO (Azar 等, 2024)</td><td>78.2</td><td>72.9</td><td>23.2</td></tr>
    <tr><td>KTO (Ethayarajh 等, 2024)</td><td>77.9</td><td>72.5</td><td>22.4</td></tr>
    <tr><td>迭代DPO (Pang 等, 2024)</td><td>78.1</td><td>74.8</td><td>23.8</td></tr>
    <tr><td rowspan="5">强化学习</td><td>PPO-S (Hu 等, 2024)</td><td>78.1</td><td>73.8</td><td>25.2</td></tr>
    <tr><td>PPO-M (参考 Shao 等, 2024)</td><td>80.2</td><td>75.0</td><td>29.8</td></tr>
    <tr><td>PF-PPO (BoN)</td><td>75.8</td><td>71.7</td><td>16.8</td></tr>
    <tr><td>PF-PPO (BR)</td><td><strong>82.9</strong></td><td><u>75.9</u></td><td><strong>33.0</strong></td></tr>
    <tr><td>PF-PPO (BW)</td><td><u>82.4</u></td><td><strong>76.2</strong></td><td><u>30.4</u></td></tr>
    <tr><td colspan="2">SOTA (7B模型) Magicoder (Wei 等, 2023b)</td><td>76.8</td><td>75.7</td><td>-</td></tr>
  </table>
  <p>表2. 不同算法在代码生成任务上的性能对比。我们将PF-PPO（我们的算法）的pass@1分数与基线方法进行比较。每个基准测试中，我们选择每种方法在5个训练周期中的最佳分数。每个基准的最高分和次高分分别用<strong>粗体</strong>和<u>下划线</u>标注。除Magicoder（当前最佳7B模型）外，所有实验均基于相同代码库以确保公平比较。</p>
  <p>上一轮迭代，我们训练策略进行4轮迭代。</p>
  <p>直接策略优化。为实现直接策略优化方法，我们使用奖励模型数据集作为这些方法所需的偏好数据集。我们实现了<span class="term">DPO（直接偏好优化）</span>、<span class="term">IPO（身份偏好优化）</span>、<span class="term">KTO（Kahneman-Tversky优化）</span>和<span class="term">迭代DPO</span>。对于迭代DPO，我们训练DPO模型进行三轮迭代。每轮迭代中，偏好数据集构建方式如下：从奖励模型数据集中采样提示，响应由上一轮训练的DPO模型（若存在）或上一轮SFT阶段生成。</p>
  <p>强化学习。对于标准<span class="term">RLHF（基于人类反馈的强化学习）</span>，我们采用OpenRLHF（Hu等，2024）的实现，该框架整合了多种先进<span class="term">PPO（近端策略优化）</span>训练技术，并在多个基准测试中表现出色。我们将此基线记为PPO-S。对于我们的方法<span class="term">PF-PPO（策略过滤PPO）</span>，我们实现了三种变体（BoN、BR和BW）。由于PF-PPO为给定提示/上下文收集多个响应，我们引入了另一个基线<span class="term">PPO-M（多响应PPO）</span>，该基线使用所有N个响应进行无过滤训练。PPO-S与PPO-M的核心区别在于：PPO-M的缓冲区包含更多相同上下文但不同响应的样本，通过比较相同上下文对应的响应可提供细粒度的词元级指导。因此，与PPO-M比较有助于区分收集多响应和过滤响应的效果。为公平比较，我们确保PF-PPO每轮迭代的计算成本不高于PPO-M和PPO-S，详细计算效率分析见附录C。</p>
  <p>实验结果。表2展示了不同方法在三个基准测试上的pass@1结果。实验结果表明PF-PPO（BR）和PF-PPO（BW）在这些基准上获得最高分，证明了我们方法的有效性。此外，我们有以下发现：</p>
  <ul>
    <li>当在适当选择的数据集上训练时，<span class="term">IPO</span>和<span class="term">KTO</span>（DPO的改进版本）并未超越DPO。这表明恰当的数据集构建能解决先前论文中发现的DPO缺陷，使DPO达到与其改进版本相当的性能。</li>
    <li>基于<span class="term">PPO</span>的算法整体优于基于<span class="term">SFT（监督微调）</span>和<span class="term">DPO</span>的算法，证明PPO在推理任务上的优越性。我们推测PPO的优异性能可能源于奖励模型和值网络的泛化能力，它们能将轨迹级奖励建模转化为词元级优势，从而提供更细粒度的指导。此外，在更具挑战性的LeetCode基准上，基于PPO的算法与其他方法的差距进一步扩大，这凸显了<span class="term">RL（强化学习）</span>的优势。</li>
  </ul>
</div>

<!-- 摘要总结 -->
<h2>摘要总结</h2>
<p>本文提出<span class="term">PF-PPO（Policy Filtration for PPO）</span>方法，通过策略过滤机制解决RLHF中奖励模型噪声问题。在HumanEval、MBPP和LeetCode代码生成基准上的实验表明：</p>
<ol>
  <li>PF-PPO的BR和BW变体在三个基准上均取得最佳性能（HumanEval: 82.9/82.4, MBPP: 76.2/75.9, LeetCode: 33.0/30.4）</li>
  <li>基于PPO
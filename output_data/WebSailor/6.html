<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>论文解析报告</title>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<style>
  body { font-family: Arial, sans-serif; line-height: 1.6; }
  .original { background-color: #f0f0f0; border: 1px solid #ccc; padding: 15px; margin-bottom: 20px; }
  .translation { background-color: #e0f7e0; border: 1px solid #4CAF50; padding: 15px; margin-bottom: 30px; }
  .formula-container { background-color: #fffde7; text-align: center; padding: 20px; margin: 20px 0; }
  .term { color: red; font-weight: bold; }
  h2 { color: #2c3e50; border-bottom: 2px solid #3498db; padding-bottom: 5px; }
  section { margin-bottom: 40px; }
</style>
</head>
<body>

<!-- 内容理解 -->
<section>
  <h2>内容理解</h2>
  <p>该文本探讨了智能体强化学习（<span class="term">Agentic RL</span>）的核心挑战及解决方案。主要包含：</p>
  <ol>
    <li>智能体RL与常规RL的关键差异在于多轮环境交互导致<span class="term">rollout速度缓慢</span></li>
    <li>指出DAPO方法因<span class="term">sequential rollouts</span>加剧训练延迟</li>
    <li>提出DUPO优化方案：
      <ul>
        <li>训练前过滤简单案例（全正确样本）</li>
        <li>复制同批次内<span class="term">non-zero standard deviation</span>样本替代填充</li>
        <li>采用<span class="term">group-relative advantage</span>估计（GPRO）</li>
      </ul>
    </li>
    <li>设计混合奖励机制（格式+答案验证）防止<span class="term">reward hacking</span></li>
    <li>实验基于Qwen系列模型在BrowseComp-en等基准测试验证</li>
  </ol>
</section>

<!-- 内容翻译 -->
<section>
  <h2>内容翻译</h2>
  
  <div class="original">
    <p>its intrinsic potential to discover and internalize sophisticated problem-solving strategies beyond direct imitation.</p>
    <p>The main difference between RL for agents and conventional reasoning tasks is that rollout is a multi-turn process involving interaction with the environment (tool responses) (Sun et al., 2024). However, the interaction with the environment causes the rollout speed of agent RL to be much slower compared to standard RL. DAPO (Yu et al., 2025) employs dynamic sampling to filter out rollouts that are entirely correct or incorrect, subsequently filling the batch to its target size with new QAs. While this is effective for data curation, it may necessitate sequential rollouts for different cases within the same batch. This sequential processing further exacerbates the slow training speeds characteristic of agentic RL.</p>
  </div>
  <div class="translation">
    <p>其内在潜力在于能发现并内化超越直接模仿的复杂问题解决策略。</p>
    <p>智能体强化学习（<span class="term">RL for agents</span>）与传统推理任务的主要区别在于：推演（<span class="term">rollout</span>）是一个涉及与环境（工具响应）交互的多轮次过程（Sun et al., 2024）。然而，环境交互导致智能体RL的推演速度远慢于标准RL。DAPO（Yu et al., 2025）采用动态采样过滤完全正确或错误的推演，随后用新问答填充批次至目标大小。虽然这对数据管理有效，但可能需要对同一批次内不同案例进行顺序推演（<span class="term">sequential rollouts</span>），这种顺序处理进一步加剧了智能体RL训练速度慢的特性。</p>
  </div>

  <div class="original">
    <p>To solve this issue, we first filter out overly simple cases (those with all 8rollouts correct) before training. During training, instead of using padding to expand the batch, we duplicate samples from the same batch that have a non-zero standard deviation. Compared to DAPO’s dynamic sampling, this approach achieves approximately 2–3 times speedup. Similar to SFT, it is also necessary to mask observations when calculating the policy loss (Jin et al., 2025). We follow GPRO (Shao et al., 2024) to estimate the advantage in a group-relative manner. We also utilize the token-level policy gradient loss and higher clip techniques in DAPO. The training objective of DUPO is defined as follows:</p>
    <div class="formula-container">
      $$J(\theta) =E_{(q,y)\sim D,\{o_i\}_{i=1}^G \sim \pi_{\theta_{old}}(\cdot|\text{context} )}\left[\frac{1}{\sum_{i=1}^G |o_i|}\sum_{i=1}^G \sum_{t=1}^{|o_i|}\min\left( r_{i,t}(\theta)\hat{A}_{i,t}, \text{clip}\left( r_{i,t}(\theta), 1-\varepsilon_{\text{low}}, 1+\varepsilon_{\text{high}}\right)\hat{A}_{i,t}\right)\right]$$
      <div>(3) 约束条件: $0 < |\{o_i | \text{is_equivalent}(y,o_i)\}| < G$</div>
    </div>
    <p>where (q,y) is the question-answer pair, $r_{i,t}(\theta)$ is the importance sampling ratio, and $\hat{A}_{i,t}$ is an estimator of the advantage at time step t:</p>
    <div class="formula-container">
      $$r_{i,t}(\theta) =\frac{\pi_\theta(o_{i,t}|\text{context} )}{\pi_{\theta_{old}}(o_{i,t}|\text{context} )}, \quad \hat{A}_{i,t}=\frac{R_i - \text{mean}(\{R_i\}_{i=1}^G)}{\text{std}(\{R_i\}_{i=1}^G)}$$
      <div>(4)</div>
    </div>
    <p>Notably, in Eq. 4, $o_i$ represents the tokens generated by the model but not the whole trajectory. Meanwhile, context comprises the model generation and tool response. Cases with a standard deviation of 0 (i.e., all roll-out answers are either completely correct or completely incorrect) are removed. These slots in the batch were then filled by randomly duplicating other cases within the same batch whose standard deviation was not 0.</p>
  </div>
  <div class="translation">
    <p>为解决此问题，我们首先在训练前过滤过于简单的案例（所有8次推演均正确的样本）。训练期间，不使用填充（<span class="term">padding</span>）扩展批次，而是复制同批次内具有非零标准差（<span class="term">non-zero standard deviation</span>）的样本。相比DAPO的动态采样，该方法实现约2-3倍加速。类似于监督微调（<span class="term">SFT</span>），计算策略损失时需掩蔽观测值（Jin et al., 2025）。我们采用GPRO（Shao et al., 2024）以组相对方式估计优势（<span class="term">advantage</span>），并沿用DAPO的令牌级策略梯度损失（<span class="term">token-level policy gradient loss</span>）和更高裁剪技术。DUPO的训练目标定义为：</p>
    <div class="formula-container">
      $$J(\theta) =E_{(q,y)\sim D,\{o_i\}_{i=1}^G \sim \pi_{\theta_{old}}(\cdot|\text{上下文} )}\left[\frac{1}{\sum_{i=1}^G |o_i|}\sum_{i=1}^G \sum_{t=1}^{|o_i|}\min\left( r_{i,t}(\theta)\hat{A}_{i,t}, \text{裁剪}\left( r_{i,t}(\theta), 1-\varepsilon_{\text{低}}, 1+\varepsilon_{\text{高}}\right)\hat{A}_{i,t}\right)\right]$$
      <div>(3) 约束条件: $0 < |\{o_i | \text{等价}(y,o_i)\}| < G$</div>
    </div>
    <p>其中(q,y)为问答对，$r_{i,t}(\theta)$为重要性采样比率，$\hat{A}_{i,t}$是时间步t的优势估计量：</p>
    <div class="formula-container">
      $$r_{i,t}(\theta) =\frac{\pi_\theta(o_{i,t}|\text{上下文} )}{\pi_{\theta_{old}}(o_{i,t}|\text{上下文} )}, \quad \hat{A}_{i,t}=\frac{R_i - \text{平均值}(\{R_i\}_{i=1}^G)}{\text{标准差}(\{R_i\}_{i=1}^G)}$$
      <div>(4)</div>
    </div>
    <p>注意在公式4中，$o_i$表示模型生成的令牌而非完整轨迹；上下文包含模型生成内容和工具响应。标准差为0的案例（即所有推演答案全对或全错）被移除，这些批次空位通过随机复制同批次内标准差非0的其他案例填充。</p>
  </div>

  <div class="original">
    <p>To avoid reward hacking (Amodei et al., 2016; OpenAI, 2025c), we adopt a rule-based reward that combines both format validation and answer validation:</p>
    <div class="formula-container">
      $$R_i = 0.1 \times R^{\text{format}}_i + 0.9 \times R^{\text{answer}}_i$$
      <div>(5)</div>
    </div>
    <p>Specifically, the format score verifies whether the rollout trajectory follows the predefined format, such as whether different content segments are correctly wrapped with tags like <span class="term">\ltthought\gt</span> and <span class="term">\lttool_call\gt</span>, and whether the sequence complies with the ReAct framework. The answer score uses an LLM as a judge to determine whether the final prediction is correct.</p>
  </div>
  <div class="translation">
    <p>为避免奖励黑客（<span class="term">reward hacking</span>）（Amodei et al., 2016; OpenAI, 2025c），我们采用结合格式验证与答案验证的基于规则的奖励：</p>
    <div class="formula-container">
      $$R_i = 0.1 \times R^{\text{格式}}_i + 0.9 \times R^{\text{答案}}_i$$
      <div>(5)</div>
    </div>
    <p>具体而言，格式分验证推演轨迹是否符合预定格式（如不同内容段是否正确包裹<span class="term">\lt思考\gt</span>和<span class="term">\lt工具调用\gt</span>标签，序列是否遵循ReAct框架）；答案分使用LLM作为裁判判定最终预测正确性。</p>
  </div>

  <div class="original">
    <p>5 Experiments<br>5.1 Setup<br>Models and Benchmarks We perform RFT and RL training on Qwen-2.5-3B, Qwen-2.5-7B, Qwen-2.5-32B, Qwen-2.5-72B. We mainly evaluate our method on four challenging benchmarks:<br>• BrowseComp-en (Wei et al., 2025): one of the most challenging benchmarks introduced by OpenAI to evaluate the proficiency of AI agents in locating hard-to-find, often multi-faceted,</p>
  </div>
  <div class="translation">
    <p>5 实验<br>5.1 设置<br>模型与基准：我们在Qwen-2.5-3B/7B/32B/72B模型上进行RFT和RL训练，主要在四个挑战性基准上评估：<br>• BrowseComp-en（Wei et al., 2025）：OpenAI推出的最具挑战性的基准之一，用于评估AI智能体在定位难寻且多面性信息中的能力</p>
  </div>
</section>

<!-- 摘要总结 -->
<section>
  <h2>摘要总结</h2>
  <p>本文针对智能体强化学习（Agentic RL）训练速度慢的核心问题：</p>
  <ul>
    <li><strong>问题诊断</strong>：环境交互导致多轮推演（rollout）速度显著低于标准RL，现有DAPO方法因顺序处理加剧延迟</li>
    <li><strong>创新方案DUPO</strong>：
      <ol>
        <li>预过滤全正确样本，避免无效计算</li>
        <li>用同批次内非零标准差样本复制替代填充，提速2-3倍</li>
        <li>集成GPRO组相对优势估计与DAPO的令牌级梯度裁剪</li>
      </ol>
    </li>
    <li><strong>抗奖励黑客机制</strong>：混合奖励函数（10%格式验证 + 90%答案验证）</li>
    <li><strong>验证框架</strong>：基于Qwen系列模型在BrowseComp-en等复杂基准测试</li>
  </ul>
  <p>核心贡献：通过动态批次优化策略突破智能体RL训练瓶颈，公式化实现高效策略梯度更新。</p>
</section>

<!-- 术语识别 -->
<section>
  <h2>术语解释</h2>
  <dl>
    <dt><span class="term">Rollout (推演)</span></dt>
    <dd>智能体与环境交互的多轮次过程，通过执行动作观察状态转移，用于策略评估和数据收集</dd>
    
    <dt><span class="term">Agentic RL (智能体强化学习)</span></dt>
    <dd>专为具有自主决策能力的AI智能体设计的RL范式，需处理工具调用、多轮对话等复杂交互</dd>
    
    <dt><span class="term">DAPO (Dynamic Sampling)</span></dt>
    <dd>动态采样方法：过滤全对/全错推演后补充新样本，但需顺序处理导致延迟</dd>
    
    <dt><span class="term">DUPO (本文方法)</span></dt>
    <dd>通过复制同批次内非零标准差样本加速训练，避免顺序瓶颈</dd>
    
    <dt><span class="term">Group-relative Advantage (组相对优势)</span></dt>
    <dd>GPRO提出的优势估计方法：$\hat{A}_{i,t} = \frac{R_i - \text{mean}(\{R_i\})}{\text{std}(\{R_i\})}$，实现组内标准化</dd>
    
    <dt><span class="term">Reward Hacking (奖励黑客)</span></dt>
    <dd>智能体利用奖励函数漏洞获取高分但未实现真实目标的行为，需通过混合验证机制抑制</dd>
    
    <dt><span class="term">ReAct Framework</span></dt>
    <dd>推理-执行交互框架：交替进行<span class="term">\ltthought\gt</span>推理和<span class="term">\lttool_call\gt</span>动作执行</dd>
    
    <dt><span class="term">Token-level Policy Gradient (令牌级策略梯度)</span></dt>
    <dd>对生成序列中每个令牌计算策略梯度，实现细粒度优化</dd>
  </dl>
</section>

</body>
</html>
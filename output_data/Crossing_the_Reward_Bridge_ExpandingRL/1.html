<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>论文解析：跨领域可验证奖励的强化学习扩展</title>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <style>
    body { font-family: "Segoe UI", sans-serif; line-height: 1.6; }
    .original { 
      background-color: #f0f0f0; 
      border: 1px solid #cccccc; 
      padding: 15px; 
      margin-bottom: 5px;
    }
    .translation { 
      background-color: #e0ffe0; 
      border: 1px solid #4CAF50; 
      padding: 15px; 
      margin-bottom: 20px;
    }
    .term { 
      font-weight: bold; 
      color: #ff0000; 
    }
    .section-title { 
      font-weight: bold; 
      font-size: 1.2em; 
      margin: 25px 0 10px 0;
      color: #2c3e50;
    }
    .formula-container { 
      background-color: #fffde7; 
      padding: 15px; 
      text-align: center; 
      margin: 15px 0;
    }
    .formula-number { 
      display: block; 
      text-align: right; 
      font-style: italic;
    }
    .contribution { margin-left: 20px; }
  </style>
</head>
<body>
  <h1>论文解析：跨领域可验证奖励的强化学习扩展</h1>
  
  <!-- 翻译部分 -->
  <div class="section-title">📖 内容翻译（中英对照）</div>
  
  <div class="original">
    <p>Expanding RL with Verifiable Rewards Across Diverse Domains</p>
  </div>
  <div class="translation">
    <p>在多样化领域中扩展可验证奖励的强化学习</p>
  </div>
  
  <div class="original">
    <p>Reinforcement learning with verifiable rewards (RLVR) has recently emerged as an effective paradigm for improving the reasoning capabilities of large language models (LLMs), even in scenarios without supervised fine-tuning. RLVR typically leverages reference-based signals, assuming the availability of objective ground-truth answers to determine whether model responses align with reference outcomes. In prior studies, RLVR has mainly demonstrated success on tasks with precisely structured solutions, such as mathematical reasoning or code generation, where <span class="term">binary verification signals</span> (correct or incorrect) can be reliably computed with simple rule-based verifiers. Nonetheless, the extension of RLVR to broader, more nuanced domains remains largely unexplored, due primarily to the challenges associated with verifying complex, frequently unstructured reference answers.</p>
  </div>
  <div class="translation">
    <p>可验证奖励的强化学习（<span class="term">Reinforcement Learning with Verifiable Rewards, RLVR</span>）最近已成为提升大语言模型（LLMs）推理能力的有效范式，即使在无监督微调场景下也适用。RLVR通常利用基于参考的信号，假设可获得客观参考答案来确定模型响应是否与参考结果一致。在先前研究中，RLVR主要在具有精确结构化解决方案的任务（如数学推理或代码生成）中取得成功，这些任务可通过简单的基于规则验证器可靠计算<span class="term">二元验证信号（binary verification signals）</span>（正确/错误）。然而，由于验证复杂且通常非结构化的参考答案存在挑战，将RLVR扩展到更广泛、更细致的领域仍基本未被探索。</p>
  </div>
  
  <div class="original">
    <p>In this paper, we aim to extend the applicability of RLVR to domains beyond structured mathematics and coding, by investigating its performance in a diverse set of complex reasoning-intensive areas such as medicine, chemistry, psychology, economics, and education. Central to this exploration is the observation that <span class="term">binary correctness judgments</span>, even on broad-domain tasks, tend to exhibit remarkable agreement across varied large language models (LLMs), including both closed-source models and recently released powerful open-source solutions when provided high-quality objective references authored by domain experts. This finding indicates that reference-based evaluation of diverse domain answers is typically easier than reference-free verification, which is inherently as difficult as identifying the first mistake in a response. Consequently, this insight undermines the presumed necessity for extensive domain-specific annotation and motivates rethinking traditional practices in reward-model training for multi-domain scenarios.</p>
  </div>
  <div class="translation">
    <p>本文旨在通过研究RLVR在医学、化学、心理学、经济学和教育等多样化复杂推理密集型领域的表现，将其适用性扩展到结构化数学和编码之外的领域。本探索的核心发现是：即使面对广泛领域的任务，当提供领域专家编写的高质量客观参考时，<span class="term">二元正确性判断（binary correctness judgments）</span>在不同大语言模型（包括闭源模型和近期发布的强大开源解决方案）中表现出显著一致性。这表明基于参考的跨领域答案评估通常比无参考验证更容易，后者本质上等同于识别响应中的首个错误。因此，这一发现挑战了广泛领域特定标注的必要性，并促使重新思考多领域场景中奖励模型训练的传统实践。</p>
  </div>
  
  <div class="original">
    <p>While <span class="term">binary rewards</span> have been the prevalent standard across RLVR applications, they pose clear limitations—especially for unstructured tasks. Notably, our data analysis on real-world exam questions reveals that only 60.3% of mathematical problems possess single-term numerical answers verifiable by rule-based methods, with the ratio dropping further to 45.4% for complex multi-domain queries. This presents inherent challenges for binary reward schemes and demonstrates the need for richer and more granular verification mechanisms. To address these limitations, we propose incorporating <span class="term">soft scores</span> obtained from generative, model-based verifiers directly into RLVR. Specifically, we compute a <span class="term">soft reward</span> from the probability of a single indicative token produced by a generative verifier summarizing its assessment. Crucially, we demonstrate that it is feasible to distill effective multi-domain generative verifier models based on relatively compact models without conducting extensive domain-specific annotation. Instead, we employ data composed of response samples and their corresponding judgments collected during RL exploration under the supervision of a larger cross-domain generative teacher model. These noisy yet more realistic datasets promote robustness of the subsequently distilled model-based rewards.</p>
  </div>
  <div class="translation">
    <p>尽管<span class="term">二元奖励（binary rewards）</span>在RLVR应用中普遍存在，但其存在明显局限——尤其对非结构化任务。值得注意的是，我们对真实考试题目的数据分析显示：仅60.3%的数学问题具有可通过基于规则方法验证的单值数字答案，而复杂多领域查询中该比例降至45.4%。这对二元奖励方案构成固有挑战，表明需要更丰富、更细粒度的验证机制。为此，我们提出将<span class="term">生成式模型验证器（generative model-based verifiers）</span>产生的<span class="term">软分数（soft scores）</span>直接融入RLVR。具体而言，我们通过生成式验证器产生的单个指示性标记的概率计算<span class="term">软奖励（soft reward）</span>以总结其评估。关键的是，我们证明无需大量领域特定标注即可基于相对紧凑的模型（小至7B规模）蒸馏出有效的多领域生成式验证器模型。取而代之的是，我们利用在大型跨领域生成式教师模型监督下，于RL探索期间收集的响应样本及其对应判断数据。这些带噪声但更真实的数据集提升了后续蒸馏的基于模型的奖励的鲁棒性。</p>
  </div>
  
  <div class="original">
    <p>Our empirical results strongly validate the effectiveness of our extended RLVR framework across various domains. By fine-tuning modest-sized (7B) base models using various RL algorithms and our soft reward verifier, we obtain improved reasoning policies superior to state-of-the-art open-source alignment models, achieving performance boosts of up to 8.0% accuracy in diverse, free-form reasoning tasks. We particularly observe that our model-based soft rewards consistently scale better and produce more robust policies compared to conventional rule-based binary rewards, especially on unstructured answer scenarios and larger training data regimes.</p>
  </div>
  <div class="translation">
    <p>实证结果强有力验证了我们扩展的RLVR框架在多个领域的有效性。通过使用多种RL算法和软奖励验证器对中等规模（7B）基础模型进行微调，我们获得了优于最先进开源对齐模型的推理策略，在多样化自由形式推理任务中实现高达8.0%的准确率提升。我们特别观察到，相较于传统基于规则的二元奖励，我们基于模型的软奖励始终具有更好的扩展性并产生更鲁棒的策略，尤其在非结构化答案场景和更大训练数据规模下。</p>
  </div>
  
  <div class="original">
    <p>Contributions. Our key contributions can be summarized as follows:</p>
    <p>• We extend reinforcement learning with verifiable rewards (RLVR) to diverse domains, establishing its effectiveness beyond traditional structured answer scenarios.</p>
  </div>
  <div class="translation">
    <p>贡献。我们的核心贡献可总结如下：</p>
    <p>• 将可验证奖励的强化学习（RLVR）扩展到多样化领域，确立其在传统结构化答案场景之外的有效性。</p>
  </div>
  
  <!-- 内容理解 -->
  <div class="section-title">🧠 内容理解</div>
  <p>本文提出扩展<span class="term">可验证奖励的强化学习（RLVR）</span>到非结构化领域（如医学/心理学）的创新框架。核心发现是：当使用专家提供的参考答案时，不同LLM对答案的二元判断具有高度一致性，这降低了领域标注需求。针对传统<span class="term">二元奖励（binary rewards）</span>在复杂问题中的局限性（仅覆盖45-60%的题目），作者提出用<span class="term">生成式模型验证器</span>产生连续<span class="term">软奖励（soft rewards）</span>，并通过<span class="term">蒸馏（distillation）</span>技术构建轻量级多领域验证器（7B规模）。实验表明，该方法在自由形式推理任务中超越主流模型达8%准确率，尤其在非结构化场景下鲁棒性显著提升。</p>
  
  <!-- 摘要总结 -->
  <div class="section-title">📌 摘要总结</div>
  <p>本文突破性地将RLVR应用于医学/经济等非结构化领域，提出基于生成式模型的软奖励机制解决传统二元奖励的覆盖局限。关键技术包括：1）利用领域专家参考实现高效跨模型验证一致性；2）设计轻量级生成验证器（7B）通过概率标记输出软奖励；3）采用教师模型监督的蒸馏框架避免大量领域标注。在多样化推理任务中，该方法使7B模型超越72B级SOTA模型，最高提升8%准确率，为多领域RL提供新范式。</p>
  
  <!-- 术语识别 -->
  <div class="section-title">🔍 术语识别与解释</div>
  <ul>
    <li><span class="term">Reinforcement Learning with Verifiable Rewards (RLVR) 可验证奖励的强化学习</span>：通过可验证的参考信号（如参考答案）驱动LLM推理能力提升的强化学习范式，无需监督微调即可运作。</li>
    <li><span class="term">Binary Verification Signals 二元验证信号</span>：基于规则判断答案绝对正确/错误的离散奖励信号，适用于数学/编程等结构化任务但难以处理模糊答案。</li>
    <li><span class="term">Generative Model-based Verifiers 基于生成式模型的验证器</span>：替代传统规则验证器的神经网络模型，通过生成概率分布输出连续评估分数，可处理非结构化响应。</li>
    <li><span class="term">Soft Reward 软奖励</span>：由生成式验证器输出的连续值奖励（如[0,1]区间概率），通过单个指示性标记（如“正确”的概率）量化答案质量，提供比二元奖励更细粒度的训练信号。</li>
    <li><span class="term">Distillation 蒸馏</span>：使用大型教师模型（如跨领域生成模型）监督生成训练数据，使轻量级学生模型（7B）无需领域标注即可学习多领域验证能力的技术。</li>
  </ul>
</body>
</html>
<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>论文解析报告</title>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<style>
  body { font-family: Arial, sans-serif; line-height: 1.6; }
  .section { margin-bottom: 30px; }
  h2 { color: #2c3e50; border-bottom: 2px solid #3498db; padding-bottom: 5px; }
  .original { 
    background-color: #f5f5f5; 
    border: 1px solid #cccccc; 
    padding: 15px; 
    margin: 10px 0;
    border-radius: 5px;
  }
  .translation { 
    background-color: #e8f5e9; 
    border: 1px solid #4caf50; 
    padding: 15px; 
    margin: 10px 0;
    border-radius: 5px;
  }
  .term { color: #e74c3c; font-weight: bold; }
  .formula-container { 
    text-align: center; 
    margin: 20px 0; 
    background-color: #fffde7; 
    padding: 15px; 
    border-radius: 5px;
  }
  .formula-label { 
    display: block; 
    font-style: italic; 
    margin-top: 5px; 
    color: #7f8c8d;
  }
  .term-list { margin-left: 20px; }
  .term-item { margin-bottom: 10px; }
</style>
</head>
<body>

<h1>论文解析报告</h1>

<!-- 内容理解 -->
<div class="section">
  <h2>内容理解</h2>
  <p>该文本探讨了可验证奖励强化学习（RLVR）在不同领域的扩展。现有研究主要局限于数学解题、代码生成等结构化任务，依赖简洁的参考答案进行基于规则的验证（如GSM8K/MATH数据集）。这种依赖限制了RLVR在更广泛领域的应用。本文提出使用自由形式的参考答案（专家撰写/预训练语料/LLM生成），并设计了基于LLM的二元/软奖励函数。核心方法是在策略梯度算法（如REINFORCE）中使用可验证奖励，通过生成式LLM输出0/1判断响应正确性，同时处理了验证过程中的噪声问题。</p>
</div>

<!-- 内容翻译 -->
<div class="section">
  <h2>内容翻译</h2>
  
  <div class="original">
    <strong>Expanding RL with Verifiable Rewards Across Diverse Domains</strong>
    <p>Previous and on-going <span class="term">RLVR (Reinforcement Learning with Verifiable Rewards)</span> studies primarily focus on narrow tasks (Liu & Zhang, 2025; Xie et al., 2025) such as math word problem solving, code generation, and logic puzzles, where well-structured <span class="term">reference answers</span> allow for straightforward <span class="term">rule-based verification</span>. For example, <span class="term">SimpleRL</span> (Zeng et al., 2025) and <span class="term">Tulu</span> (Lambert et al., 2024) use math datasets <span class="term">GSM8K</span> (Cobbe et al., 2021b) and <span class="term">MATH</span> (Hendrycks et al., 2021b), in which each reference answer typically consists of fewer than two words. However, this reliance on well-structured data constrains the scale and diversity of resources that can be used for RLVR across broader domains.</p>
    <p>In this work, we explore RLVR using reasoning data spanning diverse domains, where reference answers are <span class="term">free-form</span>, either written by domain experts for unbiased evaluation (Yu et al., 2021), extracted from pre-training corpora (Yue et al., 2024), or generated by <span class="term">LLMs (Large Language Models)</span> (Yuan et al., 2025).</p>
  </div>
  <div class="translation">
    <strong>跨领域可验证奖励的强化学习扩展</strong>
    <p>先前和正在进行的<span class="term">RLVR（可验证奖励强化学习）</span>研究主要集中于狭窄任务（Liu & Zhang, 2025; Xie et al., 2025），如数学解题、代码生成和逻辑谜题。这些任务中结构良好的<span class="term">参考答案（reference answers）</span>允许直接的<span class="term">基于规则的验证（rule-based verification）</span>。例如，<span class="term">SimpleRL</span>（Zeng et al., 2025）和<span class="term">Tulu</span>（Lambert et al., 2024）使用数学数据集<span class="term">GSM8K</span>（Cobbe et al., 2021b）和<span class="term">MATH</span>（Hendrycks et al., 2021b），其中每个参考答案通常少于两个单词。然而，这种对结构化数据的依赖限制了可在更广泛领域中用于RLVR的资源的规模和多样性。</p>
    <p>本工作中，我们探索使用跨领域的推理数据进行RLVR，其中参考答案为<span class="term">自由形式（free-form）</span>，可由领域专家撰写用于无偏评估（Yu et al., 2021）、从预训练语料中提取（Yue et al., 2024）或由<span class="term">LLMs（大型语言模型）</span>生成（Yuan et al., 2025）。</p>
  </div>
  
  <div class="original">
    <strong>3 Method</strong>
    <p>We focus on a setting where each prompt \(x\) is accompanied by an expert-written reference answer \(a\). Reference answers have been shown to play a crucial role in providing accurate rewards for <span class="term">reinforcement learning</span> in reasoning-intensive tasks such as coding and mathematics (Shao et al., 2024). Ideally, in these domains, a response \(y\) can be objectively verified against the given reference answer \(a\). However, in practice, this verification process may be influenced by factors such as imperfect answer extraction and matching when pattern-based verifiers are used, as well as noise introduced by automated evaluation systems, such as a reward model \(r_ϕ(x,a,y)\).</p>
  </div>
  <div class="translation">
    <strong>3 方法</strong>
    <p>我们关注每个提示\(x\)配有专家撰写的参考答案\(a\)的场景。在代码和数学等推理密集型任务中，参考答案已被证明对为<span class="term">强化学习（reinforcement learning）</span>提供准确奖励至关重要（Shao et al., 2024）。理想情况下，在这些领域中，响应\(y\)可根据给定参考答案\(a\)进行客观验证。但实际上，当使用基于模式的验证器时，验证过程可能受答案提取和匹配不完善等因素影响，以及自动化评估系统（如奖励模型\(r_ϕ(x,a,y)\)）引入的噪声。</p>
  </div>
  
  <div class="original">
    <p>Nevertheless, we can still use this verifiable reward in a <span class="term">policy gradient</span> algorithm, with <span class="term">REINFORCE</span> (Williams, 1992) as an example, as follows:</p>
    <div class="formula-container">
      $$J(θ) =E_{(x,a)∼D}E_{y_i∼π_θ(·|x)}\\left[ r_ϕ(x,a,y_i)\\right]$$
      <span class="formula-label">公式(1): 目标函数</span>
    </div>
    <p>When the generation of an entire response is modeled as a single action (Ahmadian et al., 2024), the gradient becomes (see Section A.3 for details):</p>
    <div class="formula-container">
      $$∇_θJ(θ) =E_{(x,a)∼D}E_{y_i∼π_θ(·|x)}\\left[ r_ϕ(x,a,y_i)∇_θ\\log π_θ(y_i|x)\\right]$$
      <span class="formula-label">公式(2): 策略梯度</span>
    </div>
  </div>
  <div class="translation">
    <p>尽管如此，我们仍可在<span class="term">策略梯度（policy gradient）</span>算法中使用此可验证奖励，以<span class="term">REINFORCE</span>（Williams, 1992）为例：</p>
    <div class="formula-container">
      $$J(θ) =E_{(x,a)∼D}E_{y_i∼π_θ(·|x)}\\left[ r_ϕ(x,a,y_i)\\right]$$
      <span class="formula-label">公式(1): 目标函数</span>
    </div>
    <p>当整个响应的生成被建模为单一动作时（Ahmadian et al., 2024），梯度变为（详见章节A.3）：</p>
    <div class="formula-container">
      $$∇_θJ(θ) =E_{(x,a)∼D}E_{y_i∼π_θ(·|x)}\\left[ r_ϕ(x,a,y_i)∇_θ\\log π_θ(y_i|x)\\right]$$
      <span class="formula-label">公式(2): 策略梯度</span>
    </div>
  </div>
  
  <div class="original">
    <strong>3.1 Reward Estimation</strong>
    <p>To ensure a <span class="term">binary reward</span> signal, we instruct a generative <span class="term">LLM</span> \(π_ϕ\) to output only 0 or 1 (see system prompt in Table 4). For notational simplicity, we assume that each response consists of exactly \(T\) steps, where each step corresponds to a non-empty line. Let \(y_i^T\) denote the final step of response \(y_i\). The <span class="term">binary model-based reward</span> function is then defined as:</p>
    <div class="formula-container">
      $$r_ϕ(x,a,y_i) = \\mathbb{1}[c_i=1]$$
      <span class="formula-label">公式(3): 二元奖励函数</span>
    </div>
    <p>where \(c_i\) is sampled from \(π_ϕ(· |x,a,y_i^T)\), representing \(π_ϕ\)'s judgment on the correctness of \(y_i\).</p>
  </div>
  <div class="translation">
    <strong>3.1 奖励估计</strong>
    <p>为确保<span class="term">二元奖励（binary reward）</span>信号，我们指示生成式<span class="term">LLM（Large Language Model）</span> \(π_ϕ\) 仅输出0或1（见表4系统提示）。为简化表示，假设每个响应包含\(T\)步，每步对应非空行。令\(y_i^T\)表示响应\(y_i\)的最终步，则<span class="term">基于模型的二元奖励（binary model-based reward）</span>函数定义为：</p>
    <div class="formula-container">
      $$r_ϕ(x,a,y_i) = \\mathbb{1}[c_i=1]$$
      <span class="formula-label">公式(3): 二元奖励函数</span>
    </div>
    <p>其中\(c_i\)从\(π_ϕ(· |x,a,y_i^T)\)采样，代表\(π_ϕ\)对\(y_i\)正确性的判断。</p>
  </div>
  
  <div class="original">
    <p>Using \(π_ϕ\) as a <span class="term">verifier</span>, we can also define a <span class="term">soft reward</span> function using the probability of the judgment tokens (i.e., 0 or 1):</p>
    <div class="formula-container">
      $$r_ϕ(x,a,y_i) = \\begin{cases} 
        π_ϕ(1|x,a,y_i^T) & \\text{if } c_i=1 \\\\
        1-π_ϕ(0|x,a,y_i^T) & \\text{if } c_i=0 \\\\
        0 & \\text{otherwise}
      \\end{cases}$$
      <span class="formula-label">公式(4): 软奖励函数</span>
    </div>
    <p>As shown in Equations 3 and 4, \(r_ϕ(x,a,y_i)\) is bounded within [0, 1], ensuring consistency with the widely adopted binary reward scale.</p>
  </div>
  <div class="translation">
    <p>使用\(π_ϕ\)作为<span class="term">验证器（verifier）</span>，我们还可利用判断标记（即0或1）的概率定义<span class="term">软奖励（soft reward）</span>函数：</p>
    <div class="formula-container">
      $$r_ϕ(x,a,y_i) = \\begin{cases} 
        π_ϕ(1|x,a,y_i^T) & \\text{若 } c_i=1 \\\\
        1-π_ϕ(0|x,a,y_i^T) & \\text{若 } c_i=0 \\\\
        0 & \\text{其他情况}
      \\end{cases}$$
      <span class="formula-label">公式(4): 软奖励函数</span>
    </div>
    <p>如公式(3)和(4)所示，\(r_ϕ(x,a,y_i)\)有界于[0,1]，确保与广泛采用的二元奖励尺度一致。</p>
  </div>
</div>

<!-- 摘要总结 -->
<div class="section">
  <h2>摘要总结</h2>
  <p>本文提出扩展可验证奖励强化学习（RLVR）到多样化领域的方法。针对现有RLVR研究局限于结构化任务（如数学解题）的问题，作者引入自由形式参考答案（专家撰写/LLM生成），突破领域限制。核心贡献包括：1）在策略梯度算法（如REINFORCE）中集成可验证奖励；2）设计基于LLM的二元奖励函数（公式3）和软奖励函数（公式4）；3）通过生成式LLM输出0/1判断响应正确性，确保奖励一致性。该方法解决了传统基于规则验证的局限性，为RLVR在复杂领域的应用提供新范式。</p>
</div>

<!-- 术语识别 -->
<div class="section">
  <h2>术语识别</h2>
  <ul class="term-list">
    <li class="term-item"><span class="term">RLVR (Reinforcement Learning with Verifiable Rewards)</span>: 可验证奖励强化学习，一种使用可验证参考信号提供奖励的强化学习范式，适用于数学推理、代码生成等任务。</li>
    <li class="term-item"><span class="term">Reference Answers (参考答案)</span>: 用于验证模型输出的标准答案，在传统RLVR中多为结构化数据（如单个数值），本文扩展为自由形式文本。</li>
    <li class="term">Rule-based Verification (基于规则的验证)</span>: 通过预定义规则（如数学表达式匹配）判断响应正确性的方法，适用于结构化输出但缺乏领域灵活性。</li>
    <li class="term-item"><span class="term">SimpleRL/Tulu</span>: 现有RLVR系统，依赖GSM8K/MATH等结构化数学数据集进行训练和验证。</li>
    <li class="term-item"><span class="term">Free-form Answers (自由形式答案)</span>: 非结构化的参考答案形式（如段落文本），可由专家撰写、语料提取或LLM生成，支持跨领域应用。</li>
    <li class="term-item"><span class="term">Policy Gradient (策略梯度)</span>: 强化学习算法家族（如REINFORCE），通过梯度上升优化策略参数θ（公式1-2）。</li>
    <li class="term-item"><span class="term">Binary Reward (二元奖励)</span>: 离散奖励信号（0/1），由LLM验证器生成判断响应是否正确（公式3）。</li>
    <li class="term-item"><span class="term">Soft Reward (软奖励)</span>: 连续奖励信号（[0,1]区间），基于LLM验证器输出概率计算（公式4），提供更细粒度反馈。</li>
    <li class="term-item"><span class="term">Verifier (验证器)</span>: 本文指生成式LLM（πϕ），用于评估响应正确性并生成奖励信号。</li>
  </ul>
</div>

</body>
</html>
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <title>论文解析报告</title>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <style>
    body { font-family: Arial, sans-serif; line-height: 1.6; }
    .section { margin-bottom: 30px; }
    h2 { color: #2c3e50; border-bottom: 2px solid #3498db; padding-bottom: 5px; }
    .original { 
      background-color: #f0f0f0; 
      border: 1px solid #cccccc; 
      padding: 15px; 
      margin-bottom: 10px; 
      border-radius: 5px;
    }
    .translation { 
      background-color: #e0f7e0; 
      border: 1px solid #4CAF50; 
      padding: 15px; 
      margin-bottom: 20px; 
      border-radius: 5px;
    }
    .figure {
      background-color: #fffde7;
      padding: 15px;
      margin: 15px 0;
      border-radius: 5px;
      overflow-x: auto;
    }
    table {
      width: 100%;
      border-collapse: collapse;
      margin: 10px 0;
    }
    th, td {
      border: 1px solid #ddd;
      padding: 8px;
      text-align: left;
    }
    th {
      background-color: #f2f2f2;
    }
    .term {
      color: red;
      font-weight: bold;
    }
    .formula-container {
      text-align: center;
      margin: 15px 0;
    }
    .formula-label {
      display: block;
      font-style: italic;
      margin-top: 5px;
    }
  </style>
</head>
<body>

<div class="section">
  <h2>1. 内容理解与解释</h2>
  <p>本节研究<strong class="term">DeepSeek-GRM-27B</strong>模型的四个关键特性：</p>
  <ul>
    <li><strong>带参考的奖励生成</strong>：通过提供真实答案作为参考，模型在可验证任务上的准确率超过90%，有效缓解了标量/半标量奖励模型的领域偏差问题。</li>
    <li><strong>原则可迁移性</strong>：模型生成的原则可成功迁移到其他模型（如GPT-4o），性能甚至优于人工筛选原则，证明其原则具有强鲁棒性。</li>
    <li><strong>训练数据泛化</strong>：移除数学训练数据后，模型在Chat Hard子集性能下降7.9%，表明数学偏好数据能显著提升跨领域泛化能力。</li>
    <li><strong>响应长度分析</strong>：基于规则的强化学习后，推理任务响应长度显著增加，而安全任务响应缩短，反映模型自适应分配计算资源的特性。</li>
  </ul>
</div>

<div class="section">
  <h2>2. 内容翻译</h2>
  
  <div class="original">
    <h3>E.1.3 Generating Rewards with Reference</h3>
    <p>In Section 5.2, we show that scalar and semi-scalar <strong class="term">RMs (Reward Models)</strong> could have significant domain biases, and generally perform better on verifiable questions. To alleviate this issue, we test <strong class="term">DeepSeek-GRM-27B</strong> to generate rewards for these tasks with reference, where the reference is the ground truth for each query. The results are shown in Table 12. We find that <strong class="term">DeepSeek-GRM-27B</strong> could achieve a more than 90% accuracy with reference provided. This indicates that the pointwise <strong class="term">GRM (General Reward Model)</strong> could effectively judge responses with reference, mitigating performance on verifiable tasks.</p>
  </div>
  <div class="translation">
    <h3>E.1.3 基于参考的奖励生成</h3>
    <p>在第5.2节中，我们发现<strong class="term">标量和半标量奖励模型（RMs）</strong>存在显著的领域偏差，通常在可验证问题上表现更好。为缓解此问题，我们测试<strong class="term">DeepSeek-GRM-27B</strong>在提供参考（即每个查询的真实答案）时生成奖励的能力。结果如表12所示：当提供参考时，<strong class="term">DeepSeek-GRM-27B</strong>准确率超过90%。这表明逐点<strong class="term">通用奖励模型（GRM）</strong>能有效利用参考评估响应，提升可验证任务的性能。</p>
  </div>

  <div class="original">
    <h3>E.2 Transferability of Generated Principles</h3>
    <div class="figure">
      <table>
        <caption>Table 14: Experiments of the transferability of principles generated by different models.</caption>
        <thead>
          <tr><th>Method</th><th>Chat</th><th>Hard</th><th>IFEval</th></tr>
        </thead>
        <tbody>
          <tr><td>GPT-4o-2024-08-06</td><td>76.1</td><td>56.0</td><td>-</td></tr>
          <tr><td>+Self-Gen. Principles</td><td>75.9</td><td>55.6</td><td>-</td></tr>
          <tr><td>+Filtered Principles</td><td>77.8</td><td>57.5</td><td>-</td></tr>
          <tr><td>+DGRM-27B-Gen. Principles</td><td>78.1</td><td>58.3</td><td>-</td></tr>
          <tr><td>DeepSeek-GRM-27B</td><td>78.3</td><td>59.8</td><td>-</td></tr>
          <tr><td>+Filtered Principles</td><td>77.0</td><td>58.5</td><td>-</td></tr>
        </tbody>
      </table>
    </div>
    <p>We extend the preliminary experiment in Section 2.2 with <strong class="term">DeepSeek-GRM-27B</strong> generated principles. We test GPT-4o-2024-08-06 and <strong class="term">DeepSeek-GRM-27B</strong> with the filtered principles exactly the same as Table 1, and aforementioned <strong class="term">DeepSeek-GRM-27B</strong> generated ones. The results are shown in Table 14. We find that the principles generated by <strong class="term">DeepSeek-GRM-27B</strong> could be transferred to other models, and are even sightly better than manually filtered principles from GPT-4o. This indicates that the principles generated by <strong class="term">DeepSeek-GRM-27B</strong> are robust and transferable to other models.</p>
  </div>
  <div class="translation">
    <h3>E.2 生成原则的可迁移性</h3>
    <div class="figure">
      <table>
        <caption>表14：不同模型生成原则的可迁移性实验</caption>
        <thead>
          <tr><th>方法</th><th>Chat</th><th>Hard</th><th>IFEval</th></tr>
        </thead>
        <tbody>
          <tr><td>GPT-4o-2024-08-06</td><td>76.1</td><td>56.0</td><td>-</td></tr>
          <tr><td>+自生成原则</td><td>75.9</td><td>55.6</td><td>-</td></tr>
          <tr><td>+过滤原则</td><td>77.8</td><td>57.5</td><td>-</td></tr>
          <tr><td>+DGRM-27B生成原则</td><td>78.1</td><td>58.3</td><td>-</td></tr>
          <tr><td>DeepSeek-GRM-27B</td><td>78.3</td><td>59.8</td><td>-</td></tr>
          <tr><td>+过滤原则</td><td>77.0</td><td>58.5</td><td>-</td></tr>
        </tbody>
      </table>
    </div>
    <p>我们使用<strong class="term">DeepSeek-GRM-27B</strong>生成的原则扩展了2.2节的初步实验。在GPT-4o-2024-08-06和<strong class="term">DeepSeek-GRM-27B</strong>上测试了与表1完全相同的过滤原则及前述生成原则。结果（表14）表明：<strong class="term">DeepSeek-GRM-27B</strong>生成的原则可迁移至其他模型，且效果略优于GPT-4o人工过滤原则，证明其原则具有强鲁棒性和可迁移性。</p>
  </div>

  <div class="original">
    <h3>E.3 Generalization beyond Training Data</h3>
    <div class="figure">
      <table>
        <caption>Table 15: Results of training data generalization experiments on the Reward Bench benchmark. Bold numbers indicate the best performance.</caption>
        <thead>
          <tr><th>Model</th><th>Chat</th><th>Chat Hard</th><th>Safety</th><th>Reasoning</th><th>Reward Bench</th></tr>
        </thead>
        <tbody>
          <tr><td><strong>DeepSeek-GRM-27B</strong></td><td><strong>94.1</strong></td><td><strong>78.3</strong></td><td><strong>88.0</strong></td><td><strong>83.8</strong></td><td><strong>86.0</strong></td></tr>
          <tr><td>w/o MATH RM Data</td><td>96.1</td><td>70.4</td><td>85.3</td><td>82.5</td><td>83.0</td></tr>
          <tr><td>DeepSeek-GRM-16B</td><td>90.8</td><td>74.3</td><td>84.7</td><td>81.8</td><td>82.9</td></tr>
          <tr><td>w/o MATH RM Data</td><td>95.0</td><td>63.4</td><td>76.9</td><td>74.3</td><td>77.4</td></tr>
        </tbody>
      </table>
    </div>
    <p>We conduct <strong class="term">ablation study</strong> on the generalization of training data for <strong class="term">DeepSeek-GRM-27B</strong>. We remove the all data from MATH training set, and re-implement the training recipe. Results on the <strong class="term">Reward Bench</strong> benchmark are shown in Table 15. We found that merely adding math related preference data could also boost generalist <strong class="term">RM (Reward Model)</strong> performance on various domains, especially on the Chat Hard subset. The result reveals that <strong class="term">DeepSeek-GRM-27B</strong> could generalize to domains beyond the coverage of training data.</p>
  </div>
  <div class="translation">
    <h3>E.3 超越训练数据的泛化能力</h3>
    <div class="figure">
      <table>
        <caption>表15：Reward Bench基准上的训练数据泛化实验结果（粗体表示最佳性能）</caption>
        <thead>
          <tr><th>模型</th><th>Chat</th><th>Chat Hard</th><th>Safety</th><th>Reasoning</th><th>Reward Bench</th></tr>
        </thead>
        <tbody>
          <tr><td><strong>DeepSeek-GRM-27B</strong></td><td><strong>94.1</strong></td><td><strong>78.3</strong></td><td><strong>88.0</strong></td><td><strong>83.8</strong></td><td><strong>86.0</strong></td></tr>
          <tr><td>移除数学RM数据</td><td>96.1</td><td>70.4</td><td>85.3</td><td>82.5</td><td>83.0</td></tr>
          <tr><td>DeepSeek-GRM-16B</td><td>90.8</td><td>74.3</td><td>84.7</td><td>81.8</td><td>82.9</td></tr>
          <tr><td>移除数学RM数据</td><td>95.0</td><td>63.4</td><td>76.9</td><td>74.3</td><td>77.4</td></tr>
        </tbody>
      </table>
    </div>
    <p>我们对<strong class="term">DeepSeek-GRM-27B</strong>进行了训练数据泛化的<strong class="term">消融研究</strong>：移除MATH训练集所有数据后重新训练。在<strong class="term">Reward Bench</strong>基准上的结果（表15）显示，仅添加数学相关偏好数据即可提升通用<strong class="term">奖励模型（RM）</strong>在多个领域的性能（尤其在Chat Hard子集）。这表明<strong class="term">DeepSeek-GRM-27B</strong>能泛化至训练数据未覆盖的领域。</p>
  </div>

  <div class="original">
    <h3>E.4 Response Length Analysis for Rule-Based RL</h3>
    <p>We calculate the response lengths of <strong class="term">DeepSeek-GRM-27B</strong> before and after rule-based online <strong class="term">RL (Reinforcement Learning)</strong> on each subset of the <strong class="term">Reward Bench</strong> benchmark in Figure 7. The token count of <strong class="term">DeepSeek-GRM-27B</strong> is calculated based on the tokenizer of <strong class="term">Gemma-2-27B</strong>, while the result of <strong class="term">DeepSeek-R1</strong> uses its corresponding tokenizer. We found that the response length for the Chat subset barely increases in <strong class="term">RL</strong>, and the response length for the Safety subset even drops slightly. The largest increase of response lengths occurs in the Reasoning subset, where the performance of <strong class="term">DeepSeek-GRM-27B</strong> also improves greatly compared to <strong class="term">DeepSeek-GRM-27B-RFT</strong>, according to Table 8. This might indicate that <strong class="term">DeepSeek-GRM-27B</strong> learns to adaptively use more inference compute on reasoning extensive tasks, and the compute could be saved for some other domains, such as safety, after the model learns to generate principles accurately. However, <strong class="term">DeepSeek-R1</strong> uses way more tokens and achieves lower...</p>
  </div>
  <div class="translation">
    <h3>E.4 基于规则强化学习的响应长度分析</h3>
    <p>我们在图7中计算了<strong class="term">DeepSeek-GRM-27B</strong>在基于规则的在线<strong class="term">强化学习（RL）</strong>前后，于<strong class="term">Reward Bench</strong>各子集的响应长度。其中<strong class="term">DeepSeek-GRM-27B</strong>的token计数基于<strong class="term">Gemma-2-27B</strong>的分词器，而<strong class="term">DeepSeek-R1</strong>使用其专用分词器。结果显示：Chat子集的响应长度在<strong class="term">RL</strong>后几乎未增加，安全子集甚至略微下降；推理子集响应长度增幅最大（据表8，其性能提升显著）。这表明<strong class="term">DeepSeek-GRM-27B</strong>学会在推理任务上自适应分配更多计算资源，并在生成原则准确后节约其他领域（如安全）的计算成本。但<strong class="term">DeepSeek-R1</strong>消耗更多token却获得更低性能...</p>
  </div>
</div>

<div class="section">
  <h2>3. 摘要总结</h2>
  <p>本研究系统评估了<strong class="term">DeepSeek-GRM-27B</strong>奖励模型的四项核心能力：</p>
  <ol>
    <li><strong>带参考的奖励生成</strong>：通过引入真实答案作为参考，模型在可验证任务上准确率突破90%，显著缓解领域偏差问题。</li>
    <li><strong>原则可迁移性</strong>：模型生成的原则可迁移至GPT-4o等模型，性能超越人工筛选原则（表14），证明其跨模型适应性。</li>
    <li><strong>数据泛化能力</strong>：消融实验（表15）表明数学偏好数据能提升跨领域性能，尤其在Chat Hard子集（移除后下降7.9%）。</li>
    <li><strong>响应长度优化</strong>：基于规则的强化学习后，模型自适应调整响应长度——推理任务显著增长，安全任务缩短，反映计算资源的高效分配。</li>
  </ol>
  <p>综合证明<strong class="term">DeepSeek-GRM-27B</strong>具备解决奖励模型领域偏差、实现原则迁移、跨领域泛化和资源动态优化的先进能力。</p>
</div>

<div class="section">
  <h2>4. 术语解释</h2>
  <dl>
    <dt><strong class="term">标量/半标量奖励模型（Scalar/Semi-scalar RMs）</strong></dt>
    <dd>传统奖励模型架构：标量模型输出单一奖励值；半标量模型输出多维度评分但仍汇总为标量。易受领域特定偏差影响。</dd>
    
    <dt><strong class="term">逐点通用奖励模型（Pointwise GRM）</strong></dt>
    <dd>DeepSeek提出的奖励模型架构，能逐点评估响应质量，支持带参考的精细化奖励生成。</dd>
    
    <dt><strong class="term">可迁移性（Transferability）</strong></dt>
    <dd>指模型生成的原则可应用于其他模型的能力。表14证明生成原则在GPT-4o上的效果优于人工原则。</dd>
    
    <dt><strong class="term">消融研究（Ablation Study）</strong></dt>
    <dd>通过移除特定组件（如数学训练数据）分析其对模型性能的影响，用于验证组件必要性（表15）。</dd>
    
    <dt><strong class="term">基于规则的在线强化学习（Rule-based Online RL）</strong></dt>
    <dd>结合预定义规则与在线学习的强化学习方法，用于动态优化响应长度（E.4）。</dd>
    
    <dt><strong class="term">Reward Bench基准</strong></dt>
    <dd>评估奖励模型的多维度测试集，包含Chat、Safety、Reasoning等子任务（表15）。</dd>
  </dl>
</div>

</body>
</html>
<!DOCTYPE html>
<html lang="zh-CN">
<head>
<meta charset="UTF-8">
<title>论文解析报告</title>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<style>
  body { font-family: 'Segoe UI', Arial, sans-serif; line-height: 1.6; }
  .section { margin-bottom: 30px; }
  h2 { color: #2c3e50; border-bottom: 2px solid #3498db; padding-bottom: 5px; }
  .original { background-color: #f8f9fa; border: 1px solid #ced4da; padding: 15px; margin: 10px 0; border-radius: 5px; }
  .translation { background-color: #e8f5e9; border: 1px solid #c8e6c9; padding: 15px; margin: 10px 0; border-radius: 5px; }
  .figure { background-color: #fffde7; padding: 15px; margin: 15px 0; text-align: center; border: 1px dashed #fbc02d; }
  .term { color: #e53935; font-weight: bold; }
  .formula-container { text-align: center; margin: 20px 0; }
  .formula-label { font-style: italic; margin-top: 5px; }
  ul { list-style-type: none; padding-left: 0; }
  li { margin-bottom: 15px; }
</style>
</head>
<body>

<div class="section">
  <h2>内容理解</h2>
  <p>本文聚焦<strong class="term">奖励模型（Reward Model, RM）</strong>在<strong class="term">强化学习（Reinforcement Learning, RL）</strong>中的关键作用及其在通用领域的应用挑战。核心观点包括：</p>
  <ul>
    <li>• 高质量奖励信号对<strong class="term">大语言模型（Large Language Models, LLMs）</strong>性能至关重要，但现有方法局限于特定领域（如数学/编程）</li>
    <li>• 通用领域奖励建模面临两大挑战：输入灵活性不足（无法处理单/多响应）和推理时扩展性有限</li>
    <li>• 提出<strong class="term">自原则批判调优（Self-Principled Critique Tuning, SPCT）</strong>方法，通过规则化在线RL训练模型动态生成原则和批判</li>
    <li>• 开发<strong class="term">DeepSeek-GRM-27B</strong>模型，采用并行采样投票机制实现推理时扩展，提升奖励粒度和准确性</li>
  </ul>
</div>

<div class="section">
  <h2>内容翻译</h2>
  
  <div class="original">
    Preprint. Under review. et al., 2023), as a crucial component in RL, is essential for generating accurate reward signals for LLM responses. Current studies (Lightman et al., 2024; DeepSeek-AI, 2025) also show that, with high-quality and robust rewards in either training or inference time, LLMs can achieve strong performance in specific domains.
  </div>
  <div class="translation">
    预印本，正在评审中。奖励模型（Reward Model, RM）作为强化学习（RL）的关键组成部分，对于为大语言模型（LLM）响应生成准确奖励信号至关重要。当前研究（Lightman等，2024；DeepSeek-AI，2025）表明，无论在训练还是推理阶段，高质量且稳健的奖励都能使LLMs在特定领域取得强劲性能。
  </div>
  
  <div class="original">
    However, such high-quality rewards in specific domains are mainly obtained from human-designed environments with clear conditions (Yao et al., 2022; Xie et al., 2024) or from hand-crafted rules for verifiable questions, e.g., part of mathematical problems (Hendrycks et al., 2021; Veeraboina, 2023) and coding tasks (Jimenez et al., 2024; Zhuo et al., 2025). In general domains, reward generation is more challenging, as the criteria for rewards are more diverse and complex, and there are often no explicit reference or ground truth. Generalist reward modeling is thus crucial for improving the performance of LLMs in broader applications, either from post-training perspectives, e.g., RL at scale, or from inference perspectives, e.g., RM-guided search. Furthermore, RM performance should be improved by increasing both the training compute (Gao et al., 2023) and the inference compute.
  </div>
  <div class="translation">
    然而，特定领域的高质量奖励主要来源于具有明确条件的人工设计环境（Yao等，2022；Xie等，2024），或针对可验证问题（如部分数学问题（Hendrycks等，2021；Veeraboina，2023）和编程任务（Jimenez等，2024；Zhuo等，2025））的手工规则。在通用领域，奖励生成更具挑战性，因为奖励标准更加多样和复杂，且通常缺乏明确的参考或基本事实。因此，<strong class="term">通用奖励建模（Generalist Reward Modeling）</strong>对于提升LLMs在更广泛应用中的性能至关重要，无论是从训练后角度（如大规模RL）还是推理角度（如RM引导搜索）。此外，应通过增加训练计算量（Gao等，2023）和推理计算量来提升RM性能。
  </div>
  
  <div class="figure">
    ▲ 图示引用：Figure 2 - RM方法类型与特性关系示意图
  </div>
  
  <div class="original">
    In practice, challenges arise in making RMs both general and effectively scalable in inference time. Generalist RM demands (1) flexibility for different input types and (2) accurate reward generation in various domains. Moreover, effective inference-time scalability requires the RM (3) to generate higher-quality reward signals with increased inference compute, and (4) to learn scalable behaviors for better performance-compute scaling. Existing research on reward modeling demonstrates several paradigms for reward generation, including scalar (Cobbe et al., 2021; Wang et al., 2024e; Liu et al., 2024), semi-scalar (Ye et al., 2024; Yu et al., 2025b; Zhang et al., 2025a), and generative (Li et al., 2024a; Kim et al., 2024; Vu et al., 2024; Cao et al., 2024; Arabzadeh et al., 2024; Ye et al., 2025; Alexandru et al., 2025; Yu et al., 2025a) approaches, and various scoring patterns, such as pointwise (Kendall & Smith, 1940; Gao et al., 2023; Yuan et al., 2024; Winata et al., 2025; Guo et al., 2025) and pairwise (Park et al., 2024; Zheng et al., 2023; Jiang et al., 2023; Wang et al., 2024c; Liu et al., 2025). These approaches inherently determine the input flexibility and the inference-time scalability of RMs ( (1)&(3) ), as shown in Figure 2. For instance, pairwise RMs only consider the relative preference of paired responses, lacking flexibility to accept single or multiple responses as input; scalar RMs could hardly generate diverse reward signals for the same response, which obstructs getting better rewards through sampling-based inference-time scaling methods (Snell et al., 2025). Also, different learning methods (Wang et al., 2024a; Ankner et al., 2024; Wang et al., 2024c; Mahan et al., 2024) have been proposed to improve the quality of rewards, but few of them focus on inference-time scalability and study the interconnection between the learned reward generation behaviors and the effectiveness of inference-time scaling of RMs, resulting in marginal performance improvement ( (2)&(4) ).
  </div>
  <div class="translation">
    实践中，要使RM兼具通用性和有效的推理时扩展性面临挑战。通用RM需要：(1) 适应不同输入类型的灵活性；(2) 多领域的准确奖励生成。此外，有效的推理时扩展性要求RM能够：(3) 通过增加推理计算生成更高质量的奖励信号；(4) 学习可扩展行为以实现更好的性能-计算比例。现有奖励建模研究展示了多种奖励生成范式，包括<strong class="term">标量式（Scalar）</strong>、<strong class="term">半标量式（Semi-scalar）</strong>和<strong class="term">生成式（Generative）</strong>方法，以及<strong class="term">逐点评分（Pointwise Scoring）</strong>和<strong class="term">成对评分（Pairwise Scoring）</strong>等模式。这些方法本质上决定了RM的输入灵活性和推理时扩展性（(1)&(3)），如图2所示。例如，成对RM仅考虑配对响应的相对偏好，缺乏接受单/多响应输入的灵活性；标量RM难以对同一响应生成多样化奖励信号，阻碍了基于采样的推理时扩展方法（Snell等，2025）获取更好奖励。虽然已有不同学习方法（Wang等，2024a；Ankner等，2024）用于提升奖励质量，但少有研究关注推理时扩展性及奖励生成行为与扩展有效性的关联，导致性能提升有限（(2)&(4)）。
  </div>
  
  <div class="original">
    Current research (DeepSeek-AI, 2025) indicates that effective inference-time scalability could be enabled by proper learning methods, which raises the question: Can we design a learning method aiming to enable effective inference-time scaling for generalist reward modeling?
  </div>
  <div class="translation">
    当前研究（DeepSeek-AI，2025）表明，有效的推理时扩展性可通过恰当学习方法实现，这引出一个问题：能否设计一种学习方法，专门实现通用奖励建模的有效推理时扩展？
  </div>
  
  <div class="original">
    In this work, we investigate in different approaches for RM, and found that pointwise generative reward modeling (GRM) could unify the scoring of single, paired, and multiple responses within pure language representation, overcoming challenge (1). We explored that certain principles could guide reward generation within proper criteria for GRMs, improving the quality of rewards, which inspired us that inference-time scalability of RM might be achieved by scaling the generation of high-quality principles and accurate critiques. Based on this preliminary, we propose a novel learning method, Self-Principled Critique Tuning (SPCT), to foster effective inference-time scalable behaviors in GRMs. By leveraging rule-based online RL, SPCT enables GRMs to learn to adaptively posit principles and critiques based on the input query and responses, leading to better outcome rewards in general domains (challenge (2)). We then come up with DeepSeek-GRM-27B, which is post-trained with SPCT based on Gemma-2-27B (Team, 2024). For inference-time scaling, we expand compute usage by sampling multiple times. By sampling in parallel, DeepSeek-GRM could generate different sets of principles and according critiques, and then vote for the final reward. With larger-scale sampling, DeepSeek-GRM could judge more accurately upon principles with higher diversity, and output rewards with finer granularity, which resolves challenge (3)&(4). Furthermore, We train a meta RM besides voting for better scaling performance. Empirically, we show that SPCT significantly improves the quality and scalability of GRMs.
  </div>
  <div class="translation">
    本工作中，我们研究了不同RM方法，发现<strong class="term">逐点生成式奖励建模（Pointwise Generative Reward Modeling, GRM）</strong>能在纯语言表示中统一单响应、成对响应和多响应的评分，克服挑战(1)。我们发现特定原则可在适当标准下指导GRMs的奖励生成，提升奖励质量，这启发我们：通过扩展高质量原则和准确批判的生成，可能实现RM的推理时扩展性。基于此，我们提出新型学习方法——<strong class="term">自原则批判调优（Self-Principled Critique Tuning, SPCT）</strong>，以促进GRMs中的有效推理时扩展行为。SPCT利用基于规则的在线RL，使GRMs学会根据输入查询和响应自适应提出原则和批判，从而在通用领域产生更优结果奖励（挑战(2)）。随后我们基于Gemma-2-27B（Team, 2024）通过SPCT后训练得到DeepSeek-GRM-27B模型。对于推理时扩展，我们通过多次采样扩大计算使用：并行采样使DeepSeek-GRM生成多组原则及对应批判，并通过投票确定最终奖励。更大规模采样使模型能基于更高多样性的原则更准确地判断，输出更细粒度的奖励，解决挑战(3)&(4)。此外，除投票机制外，我们还训练元RM以提升扩展性能。实验证明SPCT显著提升了GRMs的质量和可扩展性。
  </div>
</div>

<div class="section">
  <h2>摘要总结</h2>
  <p>本文针对通用领域奖励建模的两大核心挑战——<strong class="term">输入灵活性</strong>和<strong class="term">推理时扩展性</strong>，提出创新解决方案：</p>
  <ol>
    <li><strong>方法创新</strong>：开发SPCT（自原则批判调优）框架，通过规则化在线RL训练模型动态生成评估原则和批判</li>
    <li><strong>架构创新</strong>：构建DeepSeek-GRM-27B模型，支持单/多响应统一评分，突破传统标量/成对RM的输入限制</li>
    <li><strong>扩展机制</strong>：采用并行采样投票系统，通过增加计算资源提升奖励粒度和准确性，实现性能-计算比例优化</li>
    <li><strong>实验验证</strong>：实证表明SPCT显著提升奖励质量和模型可扩展性，为通用LLM奖励建模提供新范式</li>
  </ol>
  <p>核心贡献在于首次系统解决通用RM的推理时扩展问题，建立原则生成-批判调优-投票决策的完整技术链条。</p>
</div>

<div class="section">
  <h2>术语识别</h2>
  <ul>
    <li><span class="term">奖励模型（Reward Model, RM）</span>：强化学习中生成奖励信号的组件，评估LLM响应质量的核心机制，需平衡准确性与计算效率</li>
    <li><span class="term">推理时扩展（Inference-time Scaling）</span>：通过增加推理阶段计算资源（如并行采样）提升模型性能的技术，区别于训练阶段扩展</li>
    <li><span class="term">生成式奖励建模（Generative Reward Modeling, GRM）</span>：直接生成奖励文本而非标量分数的RM范式，支持细粒度评估和原则解释</li>
    <li><span class="term">自原则批判调优（Self-Principled Critique Tuning, SPCT）</span>：本文提出的元学习框架，使RM动态生成评估原则和批判，通过规则化在线RL优化奖励质量</li>
    <li><span class="term">性能-计算比例（Performance-Compute Scaling）</span>：模型性能随计算资源增加的提升效率，关键评估RM扩展性的指标</li>
    <li><span class="term">逐点评分（Pointwise Scoring）</span>：直接为单个响应生成绝对分数的评估模式，区别于成对相对比较</li>
    <li><span class="term">元奖励模型（Meta Reward Model）</span>：用于优化基础RM投票过程的二层评估架构，提升扩展稳定性</li>
  </ul>
</div>

</body>
</html>